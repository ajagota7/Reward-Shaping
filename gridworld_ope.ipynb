{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajagota7/Reward-Shaping/blob/main/gridworld_ope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt3QR_86NGdz"
      },
      "source": [
        "# Creating Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-Ww-v9JI2Hhj"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, height, width, start, end, bad_regions, good_regions):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.bad_regions = bad_regions\n",
        "        self.good_regions = good_regions\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = self.start\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_position\n",
        "\n",
        "        if action == \"up\" and y < self.height - 1:\n",
        "            y += 1\n",
        "        elif action == \"down\" and y > 0:\n",
        "            y -= 1\n",
        "        elif action == \"left\" and x > 0:\n",
        "            x -= 1\n",
        "        elif action == \"right\" and x < self.width - 1:\n",
        "            x += 1\n",
        "\n",
        "        self.agent_position = (x, y)\n",
        "\n",
        "        if self.agent_position == self.end:\n",
        "            reward = 3\n",
        "            done = True\n",
        "        elif self.agent_position in self.bad_regions:\n",
        "            reward = -1\n",
        "            done = False\n",
        "        elif self.agent_position in self.good_regions:\n",
        "            reward = 0.5\n",
        "            done = False\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return (x, y), reward, done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HQACkJ1xWBoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, epsilon=0.0):\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def select_action(self, policy_func):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            # Choose a random action\n",
        "            action = np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "        else:\n",
        "            # Use the provided policy function to get the best action\n",
        "            action = policy_func()\n",
        "        return action\n",
        "\n",
        "# Define different policy functions outside the class\n",
        "\n",
        "def random_policy():\n",
        "    # Choose a random action\n",
        "    return np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "\n",
        "def behavior_policy():\n",
        "    action_probs = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def evaluation_policy():\n",
        "    action_probs = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def manhattan_distance(pos1, pos2):\n",
        "    # Compute the Manhattan distance between two positions\n",
        "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GipigR-ZNTo6"
      },
      "source": [
        "# Generating Policy data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gridworld environment\n",
        "height = 5\n",
        "width  = 5\n",
        "start = (0,0)\n",
        "end = (4,4)"
      ],
      "metadata": {
        "id": "5ActZ0YInJCE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IgfscYWMW0XS"
      },
      "outputs": [],
      "source": [
        "\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)])\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 200\n",
        "\n",
        "def create_policy_set(env, policy, num_episodes):\n",
        "  # Create a list to store policies as trajectories\n",
        "  policies = []\n",
        "\n",
        "  # Run multiple episodes\n",
        "  for episode in range(num_episodes):\n",
        "      # Create a new Agent for each episode to generate a different policy\n",
        "      agent = Agent(epsilon=0.0)\n",
        "\n",
        "      # Run an episode\n",
        "      env.reset()\n",
        "      done = False\n",
        "      trajectory = []  # Store the trajectory for the current episode\n",
        "      cumulative_reward = 0.0  # Initialize cumulative reward\n",
        "      while not done:\n",
        "          state = env.agent_position  # Get the current state\n",
        "          action = agent.select_action(policy)\n",
        "          next_state, reward, done = env.step(action)\n",
        "\n",
        "          # Compute cumulative reward\n",
        "          cumulative_reward += reward\n",
        "\n",
        "          # Compute feature function values (manhattan distances)\n",
        "          good_region_distances = [manhattan_distance(state, gr) for gr in env.good_regions]\n",
        "          bad_region_distances = [manhattan_distance(state, br) for br in env.bad_regions]\n",
        "\n",
        "          # print(\"Good Region Distances\",good_region_distances)\n",
        "          # print(\"Bad Region Distances: \",bad_region_distances)\n",
        "\n",
        "          # Store the (state, action, reward, next_state) tuple in the trajectory\n",
        "          trajectory.append((state, action, reward, next_state, cumulative_reward, good_region_distances, bad_region_distances))\n",
        "\n",
        "          # # Print the episode information\n",
        "          # print(\"Episode:\", episode + 1)\n",
        "          # print(\"State:\", state)\n",
        "          # print(\"Action:\", action)\n",
        "          # print(\"Reward:\", reward)\n",
        "          # print(\"Next State:\", next_state)\n",
        "          # print(\"Cumulative Reward:\", cumulative_reward)\n",
        "          # print(\"Done:\", done)\n",
        "          # print(\"-----\")\n",
        "\n",
        "      # Append the trajectory to the policies list\n",
        "      policies.append(trajectory)\n",
        "  return policies\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_policies = create_policy_set(env, behavior_policy, 200)"
      ],
      "metadata": {
        "id": "_ZzGc9yegxaW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZudK5NVl_vsm",
        "outputId": "75c85e7b-2f05-4f65-a714-7c11e88063be",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Episode: 166\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 167\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 169\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 170\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (2, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 172\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 173\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (2, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 175\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 176\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 177\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 178\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 180\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 182\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (3, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 183\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 184\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (4, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 185\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 186\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 187\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (2, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (1, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (0, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (0, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 189\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (2, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (1, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (0, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 190\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 191\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (2, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 192\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 194\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 195\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (3, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 196\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 197\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (3, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 198\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (2, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -3.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -4.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -4.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -5.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -5.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -5.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -5.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -5.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -5.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -6.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -6.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -6.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 199\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 200\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (2, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (4, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: True\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "\n",
        "bad_regions = [(1, 1), (2, 2)]\n",
        "good_regions = [(3, 3)]\n",
        "\n",
        "env = GridWorld(height, width, start, end, bad_regions, good_regions)\n",
        "\n",
        "# Create a list to store evaluation policies as trajectories\n",
        "evaluation_policies = []\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 200\n",
        "\n",
        "# Run multiple episodes\n",
        "for episode in range(num_episodes):\n",
        "    # Create a new Agent for each episode to generate a different behavior policy\n",
        "    agent = Agent(epsilon=0.0)\n",
        "\n",
        "    # Run an episode\n",
        "    env.reset()\n",
        "    done = False\n",
        "    trajectory = []  # Store the trajectory for the current episode\n",
        "    cumulative_reward = 0.0  # Initialize cumulative reward\n",
        "    while not done:\n",
        "        state = env.agent_position  # Get the current state\n",
        "        action = agent.select_action(evaluation_policy)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Compute cumulative reward\n",
        "        cumulative_reward += reward\n",
        "\n",
        "        # Store the (state, action, reward, next_state) tuple in the trajectory\n",
        "        trajectory.append((state, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "        # Print the episode information\n",
        "        # print(\"Episode:\", episode + 1)\n",
        "        # print(\"State:\", state)\n",
        "        # print(\"Action:\", action)\n",
        "        # print(\"Reward:\", reward)\n",
        "        # print(\"Next State:\", next_state)\n",
        "        # print(\"Cumulative Reward:\", cumulative_reward)\n",
        "        # print(\"Done:\", done)\n",
        "        # print(\"-----\")\n",
        "\n",
        "    # Append the trajectory to the behavior policies list\n",
        "    evaluation_policies.append(trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a variable to store the sum of all cumulative rewards\n",
        "total_cumulative_reward = 0.0\n",
        "\n",
        "# Iterate through the evaluation_policies list to sum up the rewards\n",
        "for episode_trajectory in evaluation_policies:\n",
        "    # Get the last tuple (state, action, reward, next_state, cumulative_reward)\n",
        "    # from the trajectory to get the cumulative reward of the episode\n",
        "    cumulative_reward_episode = episode_trajectory[-1][-1]\n",
        "\n",
        "    # Add the episode's cumulative reward to the total cumulative reward\n",
        "    total_cumulative_reward += cumulative_reward_episode\n",
        "\n",
        "# Print the total cumulative reward\n",
        "print(\"Mean Cumulative Reward of 200 evaluation policies:\", total_cumulative_reward/len(evaluation_policies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0rRjUcUf4Tv",
        "outputId": "24c4fda6-aab0-4d66-b705-923af30c0818"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Cumulative Reward of 200 evaluation policies: 2.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_policies[178][-1][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ac_1NgMhaEc",
        "outputId": "95a88cf4-66d5-4d17-b957-604d57dbeab1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_reward_episode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-NhNQNfhcbD",
        "outputId": "02d998a0-4dde-4864-f595-1de9ec8d16bf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkhpULigM6wI"
      },
      "source": [
        "# Training Reward Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyIPQCnIBAO"
      },
      "source": [
        "## State -> Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOWRWObYB7R"
      },
      "source": [
        "Training model to predict rewards based on state only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcB-vIWj2V8r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    rewards = [state_action_reward[2] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_rewards.extend(rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_rewards = np.array(all_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42rExRZ9uReQ",
        "outputId": "c2d6f96c-d803-4cfd-dc60-ba8297af8fa2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21499"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "ZPnb1zoYE3N3",
        "outputId": "ad44bcc7-37cb-4707-b6fa-28794ab13df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d4a356955079>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m ]\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m reward_predictor.fit(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mall_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m       _, _, filtered_flat_args = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictorStates(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorStates, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_states = RewardPredictorStates(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_states.fit(\n",
        "    all_next_states, all_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KaLPE-xb2e0",
        "outputId": "230b8ee6-0a6f-46a1-adcd-45f52ac7becf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Current State: -0.03829813\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Next State: -0.006269932\n"
          ]
        }
      ],
      "source": [
        "current_state = behavior_policies[0][5][0]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])\n",
        "\n",
        "new_state = behavior_policies[0][5][3]  # Replace ... with the new state for which you want to predict the reward\n",
        "new_state = np.array(new_state, dtype=np.float32)\n",
        "predicted_reward_next = reward_predictor_states.predict(np.expand_dims(new_state, axis=0))\n",
        "print(\"Predicted Reward Next State:\", predicted_reward_next[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7rtgAXVINap"
      },
      "source": [
        "## State -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f79jQoF-tQXR"
      },
      "source": [
        "Reward model based on State -> Cumulative Rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfr9e-LRtUEj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_cum_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    cum_rewards = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_cum_rewards.extend(cum_rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_cum_rewards = np.array(all_cum_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bioPIacuJIG",
        "outputId": "b20476bb-2697-4a4d-d516-b111e0becde0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21499"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_cum_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBE4eg3xtpYa",
        "outputId": "1c27e545-c834-4869-afc3-f37535a1e35e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "2691/2691 [==============================] - 14s 4ms/step - loss: 14287.8623 - val_loss: 9193.1582\n",
            "Epoch 2/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 12563.5850 - val_loss: 7138.1450\n",
            "Epoch 3/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 10212.1973 - val_loss: 5365.0083\n",
            "Epoch 4/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 8540.7607 - val_loss: 4638.1211\n",
            "Epoch 5/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7985.4175 - val_loss: 4635.2275\n",
            "Epoch 6/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7930.9414 - val_loss: 4674.1807\n",
            "Epoch 7/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7929.3970 - val_loss: 4667.2554\n",
            "Epoch 8/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7929.0581 - val_loss: 4669.0732\n",
            "Epoch 9/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7927.9824 - val_loss: 4675.1816\n",
            "Epoch 10/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7928.5508 - val_loss: 4663.8682\n",
            "Epoch 11/1000\n",
            "2691/2691 [==============================] - 12s 5ms/step - loss: 7928.4463 - val_loss: 4675.1260\n",
            "Epoch 12/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7926.7422 - val_loss: 4675.1904\n",
            "Epoch 13/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7926.7793 - val_loss: 4658.1753\n",
            "Epoch 14/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7927.3550 - val_loss: 4671.8379\n",
            "Epoch 15/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7927.1836 - val_loss: 4697.7061\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e28c5be80a0>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 2: Design the neural network\n",
        "# import tensorflow as tf\n",
        "\n",
        "class RewardPredictorCumulative(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorCumulative, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense3 = tf.keras.layers.Dense(1, kernel_initializer='he_normal')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_cumulative = RewardPredictorCumulative(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_cumulative.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_cumulative.fit(\n",
        "    all_next_states, all_cum_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdFBS44nIUn_"
      },
      "source": [
        "## State -> Rewards over past 3 timesteps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIkOPgG07ffz"
      },
      "source": [
        "Cumulative rewards over 3 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlBb0H0NSYDz"
      },
      "outputs": [],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_behavior_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in behavior_policies\n",
        "for trajectory in behavior_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox, bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_behavior_policies list\n",
        "    augmented_behavior_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "06vVUBLO23-Y",
        "outputId": "6f39260b-3788-45b0-e890-123da0f72cb1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-85b1615e7bfe>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Iterate through each trajectory in evaluation_policies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrajectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluation_policies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnew_trajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluation_policies' is not defined"
          ]
        }
      ],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_evaluation_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in evaluation_policies\n",
        "for trajectory in evaluation_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox. bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_evaluation_policies list\n",
        "    augmented_evaluation_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pngt7A8eLoOl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "def preprocess_nstep_data(policy_data):\n",
        "  # Initialize lists to store all 'next_state' and 'reward' values\n",
        "  all_next_states = []\n",
        "  all_past3_rewards = []\n",
        "\n",
        "  # Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "  for trajectory in policy_data:\n",
        "      # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "      next_states = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "      rewards = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "\n",
        "      # Append the values to the corresponding lists\n",
        "      all_next_states.extend(next_states)\n",
        "      all_past3_rewards.extend(rewards)\n",
        "\n",
        "  # Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "  all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "  all_past3_rewards = np.array(all_past3_rewards, dtype=np.float32)\n",
        "  return all_next_states, all_past3_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCAp1fg2LUfU"
      },
      "outputs": [],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictor3States(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictor3States, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frcPgL555yen",
        "outputId": "86b058ae-b14a-478b-f0c1-2ac4d03875b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "541/541 [==============================] - 2s 2ms/step - loss: 0.1163 - val_loss: 0.0807\n",
            "Epoch 2/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0498 - val_loss: 0.0302\n",
            "Epoch 3/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0184 - val_loss: 0.0124\n",
            "Epoch 4/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0085 - val_loss: 0.0050\n",
            "Epoch 5/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0036 - val_loss: 0.0026\n",
            "Epoch 6/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 7/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 8.8721e-04 - val_loss: 6.9921e-04\n",
            "Epoch 8/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.3470e-04 - val_loss: 2.7174e-04\n",
            "Epoch 9/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.5016e-04 - val_loss: 4.5576e-04\n",
            "Epoch 10/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8726e-04 - val_loss: 3.1336e-04\n",
            "Epoch 11/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.3167e-04 - val_loss: 2.8632e-04\n",
            "Epoch 12/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.0322e-04 - val_loss: 1.9239e-04\n",
            "Epoch 13/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.2741e-04 - val_loss: 1.2284e-04\n",
            "Epoch 14/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.3022e-04 - val_loss: 2.0752e-04\n",
            "Epoch 15/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.6412e-04 - val_loss: 2.0257e-05\n",
            "Epoch 16/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.0789e-06 - val_loss: 6.2865e-07\n",
            "Epoch 17/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.2175e-05 - val_loss: 6.4133e-05\n",
            "Epoch 18/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.5053e-04 - val_loss: 6.7341e-05\n",
            "Epoch 19/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.5021e-04 - val_loss: 4.7176e-05\n",
            "Epoch 20/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8386e-04 - val_loss: 9.6860e-05\n",
            "Epoch 21/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.8277e-04 - val_loss: 1.4178e-04\n",
            "Epoch 22/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.4954e-04 - val_loss: 4.4968e-05\n",
            "Epoch 23/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.9865e-05 - val_loss: 2.5360e-04\n",
            "Epoch 24/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.9564e-04 - val_loss: 1.2803e-04\n",
            "Epoch 25/1000\n",
            "541/541 [==============================] - 1s 3ms/step - loss: 6.9617e-05 - val_loss: 2.7309e-06\n",
            "Epoch 26/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 4.5949e-04 - val_loss: 5.9128e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bc0d40ae080>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Behavior policies training\n",
        "all_next_states_behav, all_past3_rewards_behav = preprocess_nstep_data(augmented_behavior_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_behav.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states.fit(\n",
        "    all_next_states_behav, all_past3_rewards_behav,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeNydEaSIz6v",
        "outputId": "848f8b86-c1f0-4100-d492-4fd8fe3e64b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted Reward Current State: 0.0006133178\n"
          ]
        }
      ],
      "source": [
        "current_state = [2,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr0W6G3l6btP",
        "outputId": "4b7fabd9-37e5-4e01-c4c0-aab4f4476b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "83/83 [==============================] - 1s 4ms/step - loss: 0.4849 - val_loss: 0.4438\n",
            "Epoch 2/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.4098 - val_loss: 0.4183\n",
            "Epoch 3/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.3815 - val_loss: 0.4099\n",
            "Epoch 4/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.3551 - val_loss: 0.3524\n",
            "Epoch 5/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.3321 - val_loss: 0.3293\n",
            "Epoch 6/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.3087 - val_loss: 0.3227\n",
            "Epoch 7/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.2799 - val_loss: 0.2717\n",
            "Epoch 8/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.2471 - val_loss: 0.2828\n",
            "Epoch 9/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.2288 - val_loss: 0.2699\n",
            "Epoch 10/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.2065 - val_loss: 0.1874\n",
            "Epoch 11/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 0.1470\n",
            "Epoch 12/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 0.1439\n",
            "Epoch 13/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1149 - val_loss: 0.1049\n",
            "Epoch 14/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1029 - val_loss: 0.1270\n",
            "Epoch 15/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0817 - val_loss: 0.0718\n",
            "Epoch 16/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0656 - val_loss: 0.0533\n",
            "Epoch 17/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.0458\n",
            "Epoch 18/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.0345\n",
            "Epoch 19/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0252\n",
            "Epoch 20/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0217\n",
            "Epoch 21/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0163\n",
            "Epoch 22/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0117\n",
            "Epoch 23/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0116\n",
            "Epoch 24/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0071\n",
            "Epoch 25/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0065 - val_loss: 0.0076\n",
            "Epoch 26/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0048\n",
            "Epoch 27/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0034\n",
            "Epoch 28/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 29/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 30/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0029\n",
            "Epoch 31/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0027\n",
            "Epoch 32/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0019\n",
            "Epoch 33/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 34/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 35/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 36/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 37/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 38/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 9.5918e-04 - val_loss: 7.8082e-04\n",
            "Epoch 39/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 8.0509e-04 - val_loss: 6.2314e-04\n",
            "Epoch 40/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 7.8073e-04 - val_loss: 9.3644e-04\n",
            "Epoch 41/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.2917e-04 - val_loss: 7.1878e-04\n",
            "Epoch 42/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.1702e-04 - val_loss: 4.6636e-04\n",
            "Epoch 43/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 4.5697e-04 - val_loss: 3.6084e-04\n",
            "Epoch 44/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.0917e-04 - val_loss: 3.4176e-04\n",
            "Epoch 45/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.7846e-04 - val_loss: 2.6852e-04\n",
            "Epoch 46/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 2.6775e-04 - val_loss: 2.5256e-04\n",
            "Epoch 47/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 2.3555e-04 - val_loss: 2.5202e-04\n",
            "Epoch 48/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.5380e-04 - val_loss: 4.1054e-04\n",
            "Epoch 49/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 1.7520e-04\n",
            "Epoch 50/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 2.5071e-04 - val_loss: 1.2413e-04\n",
            "Epoch 51/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.2073e-04 - val_loss: 1.2467e-04\n",
            "Epoch 52/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.5401e-04 - val_loss: 1.4558e-04\n",
            "Epoch 53/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.9663e-04 - val_loss: 7.4738e-05\n",
            "Epoch 54/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.3939e-05 - val_loss: 6.0070e-05\n",
            "Epoch 55/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.0002e-04 - val_loss: 1.3382e-04\n",
            "Epoch 56/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.8231e-05 - val_loss: 4.1035e-05\n",
            "Epoch 57/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.4122e-05 - val_loss: 3.3879e-05\n",
            "Epoch 58/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 6.4845e-05 - val_loss: 3.5132e-04\n",
            "Epoch 59/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.8452e-04 - val_loss: 5.2070e-05\n",
            "Epoch 60/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.5917e-05 - val_loss: 1.6958e-04\n",
            "Epoch 61/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.8848e-04 - val_loss: 3.6782e-05\n",
            "Epoch 62/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 4.3945e-05 - val_loss: 4.2240e-05\n",
            "Epoch 63/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.6298e-04 - val_loss: 1.0759e-04\n",
            "Epoch 64/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.3274e-05 - val_loss: 1.0252e-05\n",
            "Epoch 65/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 1.9539e-05 - val_loss: 6.6084e-06\n",
            "Epoch 66/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 8.9223e-06 - val_loss: 4.6469e-06\n",
            "Epoch 67/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.2781e-06 - val_loss: 3.7274e-06\n",
            "Epoch 68/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 4.9305e-06 - val_loss: 1.5393e-06\n",
            "Epoch 69/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 4.0442e-06 - val_loss: 2.5122e-06\n",
            "Epoch 70/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.8186e-04 - val_loss: 2.5483e-04\n",
            "Epoch 71/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 9.9513e-04 - val_loss: 3.9814e-04\n",
            "Epoch 72/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 4.8291e-04 - val_loss: 3.7384e-04\n",
            "Epoch 73/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 74/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 5.3833e-04\n",
            "Epoch 75/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 1.1340e-04 - val_loss: 2.2208e-05\n",
            "Epoch 76/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.1680e-05 - val_loss: 3.0175e-06\n",
            "Epoch 77/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 2.8516e-06 - val_loss: 1.7239e-06\n",
            "Epoch 78/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.7179e-06 - val_loss: 2.9370e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bc0cd77ffa0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation policies training\n",
        "all_next_states_eval, all_past3_rewards_eval = preprocess_nstep_data(augmented_evaluation_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_eval.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states_eval = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states_eval.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states_eval.fit(\n",
        "    all_next_states_eval, all_past3_rewards_eval,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeEQ3Isp7KxB",
        "outputId": "2404e9dd-3673-47db-8793-eea611bce14b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "Predicted Reward Current State: 0.00042444468\n"
          ]
        }
      ],
      "source": [
        "current_state = [4,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states_eval.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyxbNoxzs_ah"
      },
      "source": [
        "State, action, Next State -> Reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm0AQ1adtC3-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# # Initialize lists to store all 'current_state', 'action', 'next_state', and 'reward' values\n",
        "# all_current_states = []\n",
        "# all_actions = []\n",
        "# all_next_states = []\n",
        "# all_rewards = []\n",
        "\n",
        "# # Extract the 'current_state', 'action', 'next_state', and 'reward' from the 'behavior_policies' list\n",
        "# for trajectory in behavior_policies:\n",
        "#     for state, action, reward, next_state in trajectory:\n",
        "#         all_current_states.append(state)\n",
        "#         all_actions.append(action)\n",
        "#         all_next_states.append(next_state)\n",
        "#         all_rewards.append(reward)\n",
        "\n",
        "# # Convert 'current_states', 'actions', 'next_states', and 'rewards' into appropriate formats for training\n",
        "# all_current_states = np.array(all_current_states, dtype=np.float32)\n",
        "# all_actions = np.array(all_actions, dtype=np.float32)\n",
        "# all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "# all_rewards = np.array(all_rewards, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx2TXRLpJdVV"
      },
      "source": [
        "## State + proximity to good/bad regions -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhffPOSBJpJR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oszAKQ0PJpM-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPE Calculations"
      ],
      "metadata": {
        "id": "6nNbcCLTOMxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3ykvUcj6YAJM"
      },
      "outputs": [],
      "source": [
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "def calculate_importance_weights(eval_policy, behav_policy, behavior_policies):\n",
        "  all_weights = []\n",
        "  for trajectory in behavior_policies:\n",
        "    cum_ratio = 1\n",
        "    cumul_weights = []\n",
        "    for step in trajectory:\n",
        "        ratio = eval_policy[step[1]]/behav_policy[step[1]]\n",
        "        # print(\"Ratio:\",ratio)\n",
        "        cum_ratio *= ratio\n",
        "        cumul_weights.append(cum_ratio)\n",
        "        # print(\"Cumul:\",cum_ratio)\n",
        "    all_weights.append(cumul_weights)\n",
        "\n",
        "  return all_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IS"
      ],
      "metadata": {
        "id": "VgadQNBwlT2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "0yCj9q-qGxvf"
      },
      "outputs": [],
      "source": [
        "def per_step_IS(behavior_policies, num_trajectories = 0.3):\n",
        "  all_timesteps = []\n",
        "  gamma = 0.9\n",
        "  policy_for_scope,_ = subset_policies(behavior_policies, num_trajectories)\n",
        "  scope_weights = calculate_importance_weights(eval_policy, behav_policy, policy_for_scope)\n",
        "  for j in range(len(scope_weights)):\n",
        "    Timestep_values = []\n",
        "    for i in range(len(scope_weights[j])-1):\n",
        "      timestep = gamma**(i)*scope_weights[j][i]*policy_for_scope[j][i][2]\n",
        "      # print(\"Timestep: \",timestep)\n",
        "      Timestep_values.append(timestep)\n",
        "\n",
        "    all_timesteps.append(Timestep_values)\n",
        "\n",
        "  V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "  seed_value = 42\n",
        "  np.random.seed(seed_value)\n",
        "\n",
        "  num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "  bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "                        for _ in range(100)]\n",
        "\n",
        "  V_per_sample = [sum(sample)/len(bootstrap_samples) for sample in bootstrap_samples]\n",
        "  V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "  std_deviation = np.std(V_per_sample)\n",
        "  quartiles = np.percentile(V_per_sample, [25, 50, 75])\n",
        "  max_value = np.max(V_per_sample)\n",
        "  min_value = np.min(V_per_sample)\n",
        "\n",
        "  return V_per_sample, std_deviation, quartiles, max_value, min_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_per_traj, is_std, is_quartiles, is_max_value, is_min_value = per_step_IS(behavior_policies,0.3)\n",
        "print(is_std)\n",
        "print(is_quartiles)\n",
        "print(is_max_value)\n",
        "print(is_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQM-eboLqbcY",
        "outputId": "7325eda8-2e47-497d-91be-2fadb43ad13e"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1891794197227112\n",
            "[-1.22241614 -1.11114855 -0.97093305]\n",
            "-0.7227057605032811\n",
            "-1.7273244546915811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SCOPE"
      ],
      "metadata": {
        "id": "DzDCmT7zm4IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def SCOPE(behavior_policies, beta, num_trajectories = 0.3):\n",
        "    all_timesteps = []\n",
        "    gamma = 0.9\n",
        "    policy_for_scope,_ = subset_policies(behavior_policies, num_trajectories)\n",
        "    scope_weights = calculate_importance_weights(eval_policy, behav_policy, policy_for_scope)\n",
        "    for j in range(len(scope_weights)):\n",
        "        Timestep_values = []\n",
        "        for i in range(len(scope_weights[j]) - 1):\n",
        "            features = policy_for_scope[j][i][5] + policy_for_scope[j][i][6]\n",
        "            features_next = policy_for_scope[j][i + 1][5] + policy_for_scope[j][i + 1][6]\n",
        "            timestep = gamma ** (i) * scope_weights[j][i] * (policy_for_scope[j][i][2] + gamma * phi(features_next, beta) - phi(features, beta))\n",
        "            Timestep_values.append(timestep)\n",
        "\n",
        "        all_timesteps.append(Timestep_values)\n",
        "\n",
        "    V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "\n",
        "\n",
        "    seed_value = 42\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "    bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(100)]\n",
        "\n",
        "    V_per_sample = [sum(sample)/len(bootstrap_samples) for sample in bootstrap_samples]\n",
        "    V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "    std_deviation = np.std(V_per_sample)\n",
        "    quartiles = np.percentile(V_per_sample, [25, 50, 75])\n",
        "    max_value = np.max(V_per_sample)\n",
        "    min_value = np.min(V_per_sample)\n",
        "\n",
        "    return V_per_sample, std_deviation, quartiles, max_value, min_value\n",
        "    # return bootstrap_samples\n"
      ],
      "metadata": {
        "id": "BgqnBM4abWL9"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = SCOPE(behavior_policies, beta, 0.3)"
      ],
      "metadata": {
        "id": "SaTaVi4Q4eGV"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(samples[99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TaEsJ5_4lcX",
        "outputId": "e14f79e7-d9f2-4dfc-99dc-fc89949fa2b1"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-147.58609119391252"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_weights = calculate_importance_weights(eval_policy, behav_policy, behavior_policies)\n",
        "beta =  [ 0.2609209,   0.47456879, -0.52815694]\n",
        "# beta = [0,0,0]\n",
        "V_per_sample, scope_std, scope_quartiles, scope_max_value, scope_min_value = SCOPE(behavior_policies,beta,0.3)\n",
        "print(scope_std)\n",
        "print(scope_quartiles)\n",
        "print(scope_max_value)\n",
        "print(scope_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEk4vgdcbarZ",
        "outputId": "c9afbb06-493e-4950-dd02-96ec8f3c817b"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.15306293298484386\n",
            "[-1.2602556  -1.17031182 -1.07600677]\n",
            "-0.8297692802152171\n",
            "-1.662222105105361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V_per_traj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lxwm5Asb_8f",
        "outputId": "6dc372c4-d8dd-499d-b2f5-0cfd68c46d2f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-7.56562059e-01, -4.41777539e-02, -1.09117301e+00, -2.41810257e+00,\n",
              "       -9.45358157e-01, -7.78874657e-02, -7.62366957e-02, -1.34624032e+00,\n",
              "       -1.87134195e-01,  1.03075188e-01, -3.45502767e-01, -1.85788253e+00,\n",
              "       -5.98407132e-01, -1.20016595e+00, -4.23127358e-01, -1.09955063e-01,\n",
              "       -7.42024759e-02, -5.64272195e-01, -1.60282221e-02, -5.53633931e-01,\n",
              "       -2.78762477e-01, -3.22443198e-01, -6.64136707e-02, -4.22590246e-01,\n",
              "       -3.71587773e-01, -4.65464551e-02, -5.85888067e-01, -6.86821322e-01,\n",
              "       -4.50056457e-01, -3.28122819e-01, -1.30894259e-01, -8.10605919e-02,\n",
              "       -2.46292724e-01, -6.75891741e-01, -3.57964173e-01, -1.52345762e-01,\n",
              "       -3.22955939e-01, -3.80209745e-01, -1.88372634e-01, -7.33269820e-02,\n",
              "       -6.09426216e-02, -3.15248066e-02,  2.94639266e-01, -4.58678143e-02,\n",
              "       -5.06345949e-02, -5.19557000e+00,  3.16498303e-02,  1.94812430e-01,\n",
              "       -5.42871917e-02, -2.89305859e+00, -3.62613343e+00, -1.59187260e+00,\n",
              "       -4.01409921e-01, -1.16875823e-01, -1.72022206e+00, -2.44507435e+00,\n",
              "       -1.81959052e-01, -3.79789083e-01, -1.74952078e+00, -8.19111641e-01,\n",
              "       -1.27580306e-01, -6.17540493e-02,  1.51431919e-02, -1.30595161e+00,\n",
              "       -1.49394948e-02, -3.76874638e-01, -3.58639354e-02, -1.36784506e+00,\n",
              "       -4.43686839e-01, -3.69693947e-01, -3.86115410e-01, -6.62763884e-02,\n",
              "       -3.33671396e+00, -2.53015279e+00, -8.45817824e-01, -3.51906338e-01,\n",
              "       -9.84130498e-02, -3.19076994e-01, -2.89286615e-01, -1.30860668e+00,\n",
              "       -6.52948049e-01, -3.59751979e-01, -4.80922606e-01, -7.46480815e-02,\n",
              "        1.24971842e-01, -4.24273958e-01, -5.45102604e+00, -6.26047466e-02,\n",
              "       -4.07989293e-01, -7.47241392e-02, -6.11944778e-01, -4.07513683e-02,\n",
              "       -2.53727651e-02,  4.77902139e-01, -6.49935749e-02, -7.19768339e-01,\n",
              "       -3.05308403e-02, -3.58959788e-01, -3.27584624e-02, -2.70467570e-02,\n",
              "       -2.36965536e-01, -1.39763949e-01, -2.86973321e+00, -7.23747710e-01,\n",
              "       -3.00745330e-01, -4.01503835e+00, -2.20621638e-01, -4.95885036e-03,\n",
              "        8.94739054e-03,  7.84695605e-02, -1.78507903e+00, -1.96468038e-01,\n",
              "       -1.04147827e-01, -7.12009273e-02, -4.43362467e+00, -9.05031699e-01,\n",
              "       -9.91203996e-01, -2.98102299e+00, -1.07025299e+00, -9.39231332e-02,\n",
              "       -3.56002867e-01, -7.37042294e-02, -1.19637292e+00, -2.87934753e-01,\n",
              "       -2.25314530e+00, -1.99179007e+00, -7.13721636e+00, -5.33049733e-01,\n",
              "       -4.78239291e-02, -3.11345021e-02, -2.52413531e+00, -1.96944612e-02,\n",
              "       -2.60237459e+00, -3.18715590e+00,  2.37706763e-02, -6.94029433e+00,\n",
              "       -2.94288845e+00,  3.72305881e-02, -1.20093388e+00, -6.54616743e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance Preparation and Calculation"
      ],
      "metadata": {
        "id": "WL9BcVXloGpd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "CfXiEdgqDE4D"
      },
      "outputs": [],
      "source": [
        "def phi(features, beta):\n",
        "  features = np.array(features)\n",
        "  beta = np.array(beta)\n",
        "  phi_s = np.dot(beta,features)\n",
        "  return phi_s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "9NdwpKSkhxZR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# gamma = 0.9\n",
        "# beta = [random.random() for _ in range(3)]\n",
        "def variance_terms(policy_set,gamma, beta):\n",
        "  all_weights = calculate_importance_weights(eval_policy, behav_policy, policy_set)\n",
        "  y_w_r_all = 0\n",
        "  r_all = 0\n",
        "  f_a = 0\n",
        "  for j in range(len(policy_set)):\n",
        "    y_w_r = 0\n",
        "    r = 0\n",
        "    for i in range(len(policy_set[j])):\n",
        "      features = policy_set[j][i][5]+policy_set[j][i][6]\n",
        "      y_w_r += gamma**(i)*all_weights[j][i]*policy_set[j][i][2]\n",
        "      if i>0 & i<len(policy_set):\n",
        "        r += phi(features, beta)*(all_weights[j][i-1]-all_weights[j][i])\n",
        "    y_w_r_all += y_w_r\n",
        "    f_a +=  gamma**(len(policy_set[j]))*all_weights[j][-1]*phi(features,beta) - phi(features, beta) # fix the features part\n",
        "    r_all += r\n",
        "\n",
        "  IS = y_w_r_all/len(policy_set)\n",
        "  R = r_all/len(policy_set)\n",
        "  F = f_a/len(policy_set)\n",
        "  return IS, R, F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "6BpxUSqy7K86"
      },
      "outputs": [],
      "source": [
        "def subset_policies(policy, percent_to_estimate_phi):\n",
        "    seed_value = 42\n",
        "    np.random.seed(seed_value)\n",
        "    num_policies = len(policy)\n",
        "    num_policies_to_estimate_phi = int(num_policies * percent_to_estimate_phi)\n",
        "\n",
        "    policy_for_scope = policy[num_policies_to_estimate_phi:]\n",
        "    policy_for_phi = policy[:num_policies_to_estimate_phi]\n",
        "\n",
        "    return policy_for_scope, policy_for_phi\n",
        "\n",
        "def calc_variance(behavior_policies, gamma, beta, num_bootstrap_samples = 100,num_trajectories = 0.3):\n",
        "  # Set the seed value (you can use any integer value)\n",
        "  seed_value = 42\n",
        "  np.random.seed(seed_value)\n",
        "  num_trajectories_to_sample = max(1, int(len(behavior_policies) * num_trajectories))\n",
        "\n",
        "  policy_for_scope, policy_for_phi = subset_policies(behavior_policies, percent_to_estimate_phi=num_trajectories)\n",
        "  num_trajectories_to_sample = max(1, len(policy_for_phi))\n",
        "\n",
        "  bootstrap_samples = [np.random.choice(policy_for_phi, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(num_bootstrap_samples)]\n",
        "  IS_all = []\n",
        "  R_all = []\n",
        "  F_all = []\n",
        "\n",
        "  for pol in bootstrap_samples:\n",
        "    IS, R, F = variance_terms(pol,0.9,beta)\n",
        "    IS_all.append(IS)\n",
        "    R_all.append(R)\n",
        "    F_all.append(F)\n",
        "  IS_sq = np.mean([num**2 for num in IS_all])\n",
        "  IS_R_F = 2*np.mean([IS_all[i]*(R_all[i]+F_all[i]) for i in range(len(IS_all))])\n",
        "  R_sq = np.mean([num**2 for num in R_all])\n",
        "  IS_sq_all = (np.mean(IS_all))**2\n",
        "  IS_r_t_f = 2*np.mean(IS_all)*np.mean([R_all[i]+F_all[i] for i in range(len(R_all))])\n",
        "  R_sq_all = (np.mean(R_all))**2\n",
        "\n",
        "  variance_scope = IS_sq + IS_R_F + R_sq - IS_sq_all - IS_r_t_f - R_sq_all\n",
        "  variance_is = IS_sq - IS_sq_all\n",
        "  return variance_scope, variance_is"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scope_set, phi_set = subset_policies(behavior_policies, 0.3)\n",
        "variance_scope, variance_is = calc_variance(phi_set,0.9,[-0.1,.1,.1], 100, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74yBmg6_edJv",
        "outputId": "ecdeba0c-6bfd-4a52-88b7-7540a64accd1"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-100-f5a21e0b065d>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(policy_for_phi, size=num_trajectories_to_sample, replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.9321278090837419\n",
            "Var IS:  0.18251798747629672\n",
            "Percent change in variance:  410.70462806017713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UefNOC1GC8Wm"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function to minimize variance_scope\n",
        "def objective_function(beta):\n",
        "    scope_set, phi_set = subset_policies(behavior_policies, 0.3)\n",
        "    variance_scope, variance_is = calc_variance(phi_set, 0.9, beta, 100, 0.3)\n",
        "    return variance_scope\n",
        "\n",
        "# Set the initial values of beta\n",
        "initial_beta = np.array([ 0.2610704,   0.30396575, -0.43850237])\n",
        "\n",
        "# Lists to store beta and variance_scope values at each iteration\n",
        "all_betas = []\n",
        "all_variance_scopes = []\n",
        "\n",
        "# Callback function to record beta and variance_scope values at each iteration\n",
        "def callback_function(beta):\n",
        "    all_betas.append(beta.copy())\n",
        "    variance_scope = objective_function(beta)\n",
        "    all_variance_scopes.append(variance_scope)\n",
        "    print(\"Iteration:\", len(all_betas))\n",
        "    print(\"Beta:\", beta)\n",
        "    print(\"Variance Scope:\", variance_scope)\n",
        "    print(\"----------\")\n",
        "\n",
        "# Run the optimization with the callback\n",
        "result = minimize(objective_function, initial_beta, method='L-BFGS-B', callback=callback_function)\n",
        "\n",
        "# Extract the optimal beta values\n",
        "optimal_beta = result.x\n",
        "\n",
        "# Print the result\n",
        "print(\"Optimal Beta Values:\", optimal_beta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws00tRw1KZnN",
        "outputId": "7dcb8edc-36ee-4ce7-8d68-8875cfdc9c5a"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-100-f5a21e0b065d>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(policy_for_phi, size=num_trajectories_to_sample, replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [ 0.20643808  0.37225496 -0.40707805]\n",
            "Variance Scope: 0.07884312743887278\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.20172417  0.37330396 -0.41680254]\n",
            "Variance Scope: 0.07761681675957388\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.18827351  0.38791832 -0.46826011]\n",
            "Variance Scope: 0.07467007370176332\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.19546902  0.39854131 -0.47835576]\n",
            "Variance Scope: 0.07430152131553047\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.25227677  0.4674727  -0.52758759]\n",
            "Variance Scope: 0.07301274073304509\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.2609776   0.47457761 -0.52808368]\n",
            "Variance Scope: 0.0729717459656698\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.2609209   0.47456879 -0.52815694]\n",
            "Variance Scope: 0.07297173623011044\n",
            "----------\n",
            "Optimal Beta Values: [ 0.2609209   0.47456879 -0.52815694]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function to minimize variance_scope\n",
        "def objective_function(beta):\n",
        "    IS_all, R_all, F_all, variance_scope, variance_is = calc_variance(behavior_policies, 0.9, beta)\n",
        "    return variance_scope\n",
        "\n",
        "# Set the initial values of beta\n",
        "initial_beta = np.array([1, -0.1, -0.1])\n",
        "\n",
        "# Lists to store beta and variance_scope values at each iteration\n",
        "all_betas = []\n",
        "all_variance_scopes = []\n",
        "\n",
        "# Callback function to record beta and variance_scope values at each iteration\n",
        "def callback_function(beta):\n",
        "    all_betas.append(beta.copy())\n",
        "    variance_scope = objective_function(beta)\n",
        "    all_variance_scopes.append(variance_scope)\n",
        "    print(\"Iteration:\", len(all_betas))\n",
        "    print(\"Beta:\", beta)\n",
        "    print(\"Variance Scope:\", variance_scope)\n",
        "    print(\"----------\")\n",
        "\n",
        "# Run the optimization with the callback\n",
        "result = minimize(objective_function, initial_beta, method='L-BFGS-B', callback=callback_function)\n",
        "\n",
        "# Extract the optimal beta values\n",
        "optimal_beta = result.x\n",
        "\n",
        "# Print the result\n",
        "print(\"Optimal Beta Values:\", optimal_beta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u23aNe_r1yat",
        "outputId": "a9887235-6587-421e-bb4d-4280a298bdab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-b436dd9eb8ff>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [ 0.74558738 -0.0727051  -0.13429517]\n",
            "Variance Scope: 0.17205980293372852\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.51422013  0.01444041 -0.13118779]\n",
            "Variance Scope: 0.10132221718839504\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.21974179  0.12674824 -0.12945906]\n",
            "Variance Scope: 0.06890789685172624\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.22100604  0.15532669 -0.17596816]\n",
            "Variance Scope: 0.06790719356206737\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.2578635   0.30333719 -0.43552043]\n",
            "Variance Scope: 0.06528770052678368\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.26104719  0.30397559 -0.43850381]\n",
            "Variance Scope: 0.0652844236998165\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.2610704   0.30396575 -0.43850237]\n",
            "Variance Scope: 0.06528442349549637\n",
            "----------\n",
            "Optimal Beta Values: [ 0.2610704   0.30396575 -0.43850237]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function to minimize variance_scope\n",
        "def objective_function(beta):\n",
        "    IS_all, R_all, F_all, variance_scope, variance_is = calc_variance(behavior_policies, 0.9, beta)\n",
        "    return variance_scope\n",
        "\n",
        "# Set the initial values of beta\n",
        "initial_beta = np.array([1, -0.1, -0.1])\n",
        "\n",
        "# Lists to store beta and variance_scope values at each iteration\n",
        "all_betas = []\n",
        "all_variance_scopes = []\n",
        "\n",
        "# Callback function to record beta and variance_scope values at each iteration\n",
        "def callback_function(beta):\n",
        "    all_betas.append(beta.copy())\n",
        "    variance_scope = objective_function(beta)\n",
        "    all_variance_scopes.append(variance_scope)\n",
        "    print(\"Iteration:\", len(all_betas))\n",
        "    print(\"Beta:\", beta)\n",
        "    print(\"Variance Scope:\", variance_scope)\n",
        "    print(\"----------\")\n",
        "\n",
        "# Run the optimization with the callback\n",
        "result = minimize(objective_function, initial_beta, method='L-BFGS-B', callback=callback_function)\n",
        "\n",
        "# Extract the optimal beta values\n",
        "optimal_beta = result.x\n",
        "\n",
        "# Print the result\n",
        "print(\"Optimal Beta Values:\", optimal_beta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUHzR5PDLOcS",
        "outputId": "33bf3173-2ae3-413c-e084-6795bbf4b4ba"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-80-b436dd9eb8ff>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [ 0.95461424 -0.18038518 -0.14714184]\n",
            "Variance Scope: 0.11818026023735667\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.9016154  -0.156789   -0.15068515]\n",
            "Variance Scope: 0.10796090614265882\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.46725172  0.10210465 -0.29978589]\n",
            "Variance Scope: 0.06275082447768895\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.44485279  0.171732   -0.410621  ]\n",
            "Variance Scope: 0.05877067174584359\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.46031673  0.3296736  -0.71168708]\n",
            "Variance Scope: 0.05390185593164576\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.46135818  0.32966832 -0.7124576 ]\n",
            "Variance Scope: 0.05390157377166176\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.46138779  0.32965123 -0.71244844]\n",
            "Variance Scope: 0.05390157358436144\n",
            "----------\n",
            "Optimal Beta Values: [ 0.46138779  0.32965123 -0.71244844]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bb6VtUv4loLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value estimates of IS and SCOPE estimators"
      ],
      "metadata": {
        "id": "LUVgi1GMloz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_weights = calculate_importance_weights(eval_policy, behav_policy, behavior_policies)"
      ],
      "metadata": {
        "id": "1XT65vAXLtaZ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta = [-1,1,1]\n",
        "# beta =  [ 0.24443418,  0.30068975, -0.40465262]\n",
        "V_SCOPE = SCOPE(all_weights, behavior_policies,beta)\n",
        "print(\"SCOPE values estimate: %f and variance: %f\" % (V_SCOPE,variance_scope))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Eje3uw7L1wk",
        "outputId": "b86685e2-e955-4ad2-94c7-235490306164"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCOPE values estimate: 1.024355 and variance: 0.015076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V = per_step_IS(all_weights, behavior_policies)\n",
        "print(\"IS values estimate: %f and variance: %f\" % (V,variance_is))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKHQjsGNMI8C",
        "outputId": "08798130-3bc6-4d2a-c9a7-417165258c24"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IS values estimate: -0.735629 and variance: 0.029827\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN2hDhSkEJcFRSYsC1F1o7O",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}