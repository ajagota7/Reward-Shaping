{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajagota7/Reward-Shaping/blob/main/gridworld_ope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt3QR_86NGdz"
      },
      "source": [
        "# Creating Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "-Ww-v9JI2Hhj"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, height, width, start, end, bad_regions, good_regions, good_region_rewards, bad_region_rewards, final_reward):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.bad_regions = bad_regions\n",
        "        self.good_regions = good_regions\n",
        "        self.good_region_rewards = good_region_rewards\n",
        "        self.bad_region_rewards = bad_region_rewards\n",
        "        self.final_reward = final_reward\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = self.start\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_position\n",
        "\n",
        "        if action == \"up\" and y < self.height - 1:\n",
        "            y += 1\n",
        "        elif action == \"down\" and y > 0:\n",
        "            y -= 1\n",
        "        elif action == \"left\" and x > 0:\n",
        "            x -= 1\n",
        "        elif action == \"right\" and x < self.width - 1:\n",
        "            x += 1\n",
        "\n",
        "        self.agent_position = (x, y)\n",
        "\n",
        "        if self.agent_position == self.end:\n",
        "            reward = self.final_reward\n",
        "            done = True\n",
        "        elif self.agent_position in self.bad_regions:\n",
        "            reward = self.bad_region_rewards\n",
        "            done = False\n",
        "        elif self.agent_position in self.good_regions:\n",
        "            reward = self.good_region_rewards\n",
        "            done = False\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return (x, y), reward, done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "HQACkJ1xWBoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, epsilon=0.0):\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def select_action(self, policy_func):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            # Choose a random action\n",
        "            action = np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "        else:\n",
        "            # Use the provided policy function to get the best action\n",
        "            action = policy_func()\n",
        "        return action\n",
        "\n",
        "# Define different policy functions outside the class\n",
        "\n",
        "def random_policy():\n",
        "    # Choose a random action\n",
        "    return np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "\n",
        "# def behavior_policy(behav_policy):\n",
        "#     action_probs = behav_policy\n",
        "#     return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "# def evaluation_policy(eval_policy):\n",
        "#     action_probs = eval_policy\n",
        "#     return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def run_policy(policy):\n",
        "    action_probs = policy\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "\n",
        "def manhattan_distance(pos1, pos2):\n",
        "    # Compute the Manhattan distance between two positions\n",
        "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
        "\n",
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_policy(behav_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YRY_4aXuaLEJ",
        "outputId": "5e63a9f1-5849-41e1-893a-7c195871d118"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'right'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_probs = behav_policy\n",
        "np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dw9_S47dZlz2",
        "outputId": "19fa056f-927d-4c3e-a0a0-241baf7b33c6"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'up'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filename(env, behav_policy, eval_policy, num_episodes, train_split):\n",
        "    good_regions_str = \"_\".join([f\"gr_{pos[0]}_{pos[1]}\" for pos in env.good_regions])\n",
        "    bad_regions_str = \"_\".join([f\"br_{pos[0]}_{pos[1]}\" for pos in env.bad_regions])\n",
        "\n",
        "    behav_probs_str = \"_\".join([f\"{prob:.2f}\" for prob in behav_policy.values()])\n",
        "    eval_probs_str = \"_\".join([f\"{prob:.2f}\" for prob in eval_policy.values()])\n",
        "\n",
        "    file = f\"behav_{behav_probs_str}_eval_{eval_probs_str}_{good_regions_str}_{env.good_region_rewards}_{bad_regions_str}_{env.bad_region_rewards}_trajectories_{num_episodes}_train_split_{train_split}.txt\"\n",
        "    return file"
      ],
      "metadata": {
        "id": "xyuwU_kjU8xp"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "eval_policy = {\"up\": 0.36, \"down\": 0.14, \"left\": 0.14, \"right\": 0.36}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "num_episodes = 200\n",
        "train_split = 0.8\n",
        "file = filename(env, behav_policy, eval_policy, num_episodes, train_split)\n",
        "print(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8AwyGmhU-uf",
        "outputId": "6dedf93a-0653-4ea1-80ee-99f1903101f8"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "behav_0.25_0.25_0.25_0.25_eval_0.36_0.14_0.14_0.36_gr_3_3_0.5_br_1_1_br_2_2_-1_trajectories_200_train_split_0.8.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GipigR-ZNTo6"
      },
      "source": [
        "# Generating Policy data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gridworld environment\n",
        "height = 5\n",
        "width  = 5\n",
        "start = (0,0)\n",
        "end = (4,4)"
      ],
      "metadata": {
        "id": "5ActZ0YInJCE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "IgfscYWMW0XS"
      },
      "outputs": [],
      "source": [
        "\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)], 0.5, -1, 3)\n",
        "\n",
        "# # Number of episodes\n",
        "# num_episodes = 200\n",
        "\n",
        "def create_policy_set(env, policy_func, policy, num_episodes):\n",
        "  # Create a list to store policies as trajectories\n",
        "  policies = []\n",
        "\n",
        "  # Run multiple episodes\n",
        "  for episode in range(num_episodes):\n",
        "      # Create a new Agent for each episode to generate a different policy\n",
        "      agent = Agent(epsilon=0.0)\n",
        "\n",
        "      # Run an episode\n",
        "      env.reset()\n",
        "      done = False\n",
        "      trajectory = []  # Store the trajectory for the current episode\n",
        "      cumulative_reward = 0.0  # Initialize cumulative reward\n",
        "      while not done:\n",
        "          state = env.agent_position  # Get the current state\n",
        "          action = agent.select_action(lambda: policy_func(policy))\n",
        "          next_state, reward, done = env.step(action)\n",
        "\n",
        "          # Compute cumulative reward\n",
        "          cumulative_reward += reward\n",
        "\n",
        "          # Compute feature function values (manhattan distances)\n",
        "          good_region_distances = [manhattan_distance(state, gr) for gr in env.good_regions]\n",
        "          bad_region_distances = [manhattan_distance(state, br) for br in env.bad_regions]\n",
        "\n",
        "          # Store the (state, action, reward, next_state) tuple in the trajectory\n",
        "          trajectory.append((state, action, reward, next_state, cumulative_reward, good_region_distances, bad_region_distances))\n",
        "\n",
        "      # Append the trajectory to the policies list\n",
        "      policies.append(trajectory)\n",
        "\n",
        "  # good_regions_str = \"_\".join([f\"gr_{pos[0]}_{pos[1]}\" for pos in env.good_regions])\n",
        "  # bad_regions_str = \"_\".join([f\"br_{pos[0]}_{pos[1]}\" for pos in env.bad_regions])\n",
        "  # filename = f\"{env.__class__.__name__}_policy_{policy.__name__}_{good_regions_str}_{bad_regions_str}.txt\"\n",
        "\n",
        "  return policies\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_policy_set(env, behavior_policy(behav_policy),100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "B4b4G7d1a-ZO",
        "outputId": "7187fefc-94cb-4e68-f3de-b175947827e8"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-e42a4cc28d6e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_policy_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehav_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-dc59bb93689c>\u001b[0m in \u001b[0;36mcreate_policy_set\u001b[0;34m(env, policy, num_episodes)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_position\u001b[0m  \u001b[0;31m# Get the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m           \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-136-305fb38f7659>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, policy_func)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# Use the provided policy function to get the best action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.str_' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_policy(behav_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nOhzaperbMLU",
        "outputId": "5b69fee4-b41a-4183-9d38-b8b16befdb3d"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'right'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_V_pi_e(evaluation_policies):\n",
        "\n",
        "  total_cumulative_reward = 0.0\n",
        "\n",
        "  for episode_trajectory in evaluation_policies:\n",
        "      total_cumulative_reward += episode_trajectory[-1][4]\n",
        "\n",
        "  return total_cumulative_reward/len(evaluation_policies)"
      ],
      "metadata": {
        "id": "v0rRjUcUf4Tv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and Loading Data"
      ],
      "metadata": {
        "id": "3UdUjHYmPsZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def filename(env, behav_policy, eval_policy, num_episodes, train_split):\n",
        "#   good_regions_str = \"_\".join([f\"gr_{pos[0]}_{pos[1]}\" for pos in env.good_regions])\n",
        "#   bad_regions_str = \"_\".join([f\"br_{pos[0]}_{pos[1]}\" for pos in env.bad_regions])\n",
        "#   file = f\"{policy.__name__}_{good_regions_str}_{env.good_region_rewards}_{bad_regions_str}_{env.bad_region_rewards}_trajectories_{num_episodes}_train_split_{train_split}.txt\"\n",
        "#   return file"
      ],
      "metadata": {
        "id": "6nUi8aT9aHwN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filename(env, behavior_policy, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4QGAMsoqaZpX",
        "outputId": "8bf5fd34-40b5-4864-e660-591c9e215fb7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'behavior_policy_gr_3_3_0.5_br_1_1_br_2_2_-1_trajectories_1000.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in"
      ],
      "metadata": {
        "id": "-8zpy0jdUiid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def save_data_to_file(data, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "def load_data_from_file(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "CfI_8J51Pwer"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkhpULigM6wI"
      },
      "source": [
        "# Training Reward Models (Ignore this section for now)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyIPQCnIBAO"
      },
      "source": [
        "## State -> Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOWRWObYB7R"
      },
      "source": [
        "Training model to predict rewards based on state only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcB-vIWj2V8r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    rewards = [state_action_reward[2] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_rewards.extend(rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_rewards = np.array(all_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPnb1zoYE3N3"
      },
      "outputs": [],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictorStates(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorStates, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_states = RewardPredictorStates(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_states.fit(\n",
        "    all_next_states, all_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KaLPE-xb2e0",
        "outputId": "230b8ee6-0a6f-46a1-adcd-45f52ac7becf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Current State: -0.03829813\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Next State: -0.006269932\n"
          ]
        }
      ],
      "source": [
        "current_state = behavior_policies[0][5][0]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])\n",
        "\n",
        "new_state = behavior_policies[0][5][3]  # Replace ... with the new state for which you want to predict the reward\n",
        "new_state = np.array(new_state, dtype=np.float32)\n",
        "predicted_reward_next = reward_predictor_states.predict(np.expand_dims(new_state, axis=0))\n",
        "print(\"Predicted Reward Next State:\", predicted_reward_next[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7rtgAXVINap"
      },
      "source": [
        "## State -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f79jQoF-tQXR"
      },
      "source": [
        "Reward model based on State -> Cumulative Rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfr9e-LRtUEj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_cum_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    cum_rewards = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_cum_rewards.extend(cum_rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_cum_rewards = np.array(all_cum_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBE4eg3xtpYa"
      },
      "outputs": [],
      "source": [
        "# Step 2: Design the neural network\n",
        "# import tensorflow as tf\n",
        "\n",
        "class RewardPredictorCumulative(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorCumulative, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense3 = tf.keras.layers.Dense(1, kernel_initializer='he_normal')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_cumulative = RewardPredictorCumulative(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_cumulative.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_cumulative.fit(\n",
        "    all_next_states, all_cum_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdFBS44nIUn_"
      },
      "source": [
        "## State -> Rewards over past 3 timesteps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIkOPgG07ffz"
      },
      "source": [
        "Cumulative rewards over 3 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlBb0H0NSYDz"
      },
      "outputs": [],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_behavior_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in behavior_policies\n",
        "for trajectory in behavior_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox, bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_behavior_policies list\n",
        "    augmented_behavior_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06vVUBLO23-Y"
      },
      "outputs": [],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_evaluation_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in evaluation_policies\n",
        "for trajectory in evaluation_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox. bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_evaluation_policies list\n",
        "    augmented_evaluation_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pngt7A8eLoOl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "def preprocess_nstep_data(policy_data):\n",
        "  # Initialize lists to store all 'next_state' and 'reward' values\n",
        "  all_next_states = []\n",
        "  all_past3_rewards = []\n",
        "\n",
        "  # Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "  for trajectory in policy_data:\n",
        "      # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "      next_states = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "      rewards = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "\n",
        "      # Append the values to the corresponding lists\n",
        "      all_next_states.extend(next_states)\n",
        "      all_past3_rewards.extend(rewards)\n",
        "\n",
        "  # Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "  all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "  all_past3_rewards = np.array(all_past3_rewards, dtype=np.float32)\n",
        "  return all_next_states, all_past3_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCAp1fg2LUfU"
      },
      "outputs": [],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictor3States(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictor3States, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frcPgL555yen",
        "outputId": "86b058ae-b14a-478b-f0c1-2ac4d03875b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "541/541 [==============================] - 2s 2ms/step - loss: 0.1163 - val_loss: 0.0807\n",
            "Epoch 2/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0498 - val_loss: 0.0302\n",
            "Epoch 3/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0184 - val_loss: 0.0124\n",
            "Epoch 4/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0085 - val_loss: 0.0050\n",
            "Epoch 5/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0036 - val_loss: 0.0026\n",
            "Epoch 6/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 7/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 8.8721e-04 - val_loss: 6.9921e-04\n",
            "Epoch 8/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.3470e-04 - val_loss: 2.7174e-04\n",
            "Epoch 9/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.5016e-04 - val_loss: 4.5576e-04\n",
            "Epoch 10/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8726e-04 - val_loss: 3.1336e-04\n",
            "Epoch 11/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.3167e-04 - val_loss: 2.8632e-04\n",
            "Epoch 12/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.0322e-04 - val_loss: 1.9239e-04\n",
            "Epoch 13/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.2741e-04 - val_loss: 1.2284e-04\n",
            "Epoch 14/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.3022e-04 - val_loss: 2.0752e-04\n",
            "Epoch 15/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.6412e-04 - val_loss: 2.0257e-05\n",
            "Epoch 16/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.0789e-06 - val_loss: 6.2865e-07\n",
            "Epoch 17/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.2175e-05 - val_loss: 6.4133e-05\n",
            "Epoch 18/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.5053e-04 - val_loss: 6.7341e-05\n",
            "Epoch 19/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.5021e-04 - val_loss: 4.7176e-05\n",
            "Epoch 20/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8386e-04 - val_loss: 9.6860e-05\n",
            "Epoch 21/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.8277e-04 - val_loss: 1.4178e-04\n",
            "Epoch 22/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.4954e-04 - val_loss: 4.4968e-05\n",
            "Epoch 23/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.9865e-05 - val_loss: 2.5360e-04\n",
            "Epoch 24/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.9564e-04 - val_loss: 1.2803e-04\n",
            "Epoch 25/1000\n",
            "541/541 [==============================] - 1s 3ms/step - loss: 6.9617e-05 - val_loss: 2.7309e-06\n",
            "Epoch 26/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 4.5949e-04 - val_loss: 5.9128e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bc0d40ae080>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Behavior policies training\n",
        "all_next_states_behav, all_past3_rewards_behav = preprocess_nstep_data(augmented_behavior_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_behav.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states.fit(\n",
        "    all_next_states_behav, all_past3_rewards_behav,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeNydEaSIz6v",
        "outputId": "848f8b86-c1f0-4100-d492-4fd8fe3e64b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted Reward Current State: 0.0006133178\n"
          ]
        }
      ],
      "source": [
        "current_state = [2,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr0W6G3l6btP"
      },
      "outputs": [],
      "source": [
        "# Evaluation policies training\n",
        "all_next_states_eval, all_past3_rewards_eval = preprocess_nstep_data(augmented_evaluation_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_eval.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states_eval = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states_eval.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states_eval.fit(\n",
        "    all_next_states_eval, all_past3_rewards_eval,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeEQ3Isp7KxB"
      },
      "outputs": [],
      "source": [
        "current_state = [4,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states_eval.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyxbNoxzs_ah"
      },
      "source": [
        "State, action, Next State -> Reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm0AQ1adtC3-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# # Initialize lists to store all 'current_state', 'action', 'next_state', and 'reward' values\n",
        "# all_current_states = []\n",
        "# all_actions = []\n",
        "# all_next_states = []\n",
        "# all_rewards = []\n",
        "\n",
        "# # Extract the 'current_state', 'action', 'next_state', and 'reward' from the 'behavior_policies' list\n",
        "# for trajectory in behavior_policies:\n",
        "#     for state, action, reward, next_state in trajectory:\n",
        "#         all_current_states.append(state)\n",
        "#         all_actions.append(action)\n",
        "#         all_next_states.append(next_state)\n",
        "#         all_rewards.append(reward)\n",
        "\n",
        "# # Convert 'current_states', 'actions', 'next_states', and 'rewards' into appropriate formats for training\n",
        "# all_current_states = np.array(all_current_states, dtype=np.float32)\n",
        "# all_actions = np.array(all_actions, dtype=np.float32)\n",
        "# all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "# all_rewards = np.array(all_rewards, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx2TXRLpJdVV"
      },
      "source": [
        "## State + proximity to good/bad regions -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPE Calculations"
      ],
      "metadata": {
        "id": "6nNbcCLTOMxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance Weights"
      ],
      "metadata": {
        "id": "608qtLdqhbOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3ykvUcj6YAJM"
      },
      "outputs": [],
      "source": [
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "def calculate_importance_weights(eval_policy, behav_policy, behavior_policies):\n",
        "  all_weights = []\n",
        "  for trajectory in behavior_policies:\n",
        "    cum_ratio = 1\n",
        "    cumul_weights = []\n",
        "    for step in trajectory:\n",
        "        ratio = eval_policy[step[1]]/behav_policy[step[1]]\n",
        "        # print(\"Ratio:\",ratio)\n",
        "        cum_ratio *= ratio\n",
        "        cumul_weights.append(cum_ratio)\n",
        "        # print(\"Cumul:\",cum_ratio)\n",
        "    all_weights.append(cumul_weights)\n",
        "\n",
        "  return all_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IS"
      ],
      "metadata": {
        "id": "VgadQNBwlT2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def per_step_IS(scope_set, num_bootstraps):\n",
        "    all_timesteps = []\n",
        "    gamma = 0.9\n",
        "    # scope_set,_ = subset_policies(scope_set, phi_trajectories)\n",
        "    scope_weights = calculate_importance_weights(eval_policy, behav_policy, scope_set)\n",
        "    for j in range(len(scope_weights)):\n",
        "        Timestep_values = []\n",
        "        for i in range(len(scope_weights[j]) - 1):\n",
        "            timestep = gamma ** (i) * scope_weights[j][i] * scope_set[j][i][2]\n",
        "            Timestep_values.append(timestep)\n",
        "\n",
        "        all_timesteps.append(Timestep_values)\n",
        "\n",
        "    V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "    seed_value = 42\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "    bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(num_bootstraps)]\n",
        "\n",
        "    V_per_sample = [sum(sample) / len(scope_set) for sample in bootstrap_samples]\n",
        "    V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "    std_deviation = np.std(V_per_sample)\n",
        "    quartiles = np.percentile(V_per_sample, [25, 50, 75])\n",
        "    max_value = np.max(V_per_sample)\n",
        "    min_value = np.min(V_per_sample)\n",
        "\n",
        "    return {\n",
        "        'std_deviation': std_deviation,\n",
        "        'quartiles': quartiles,\n",
        "        'max_value': max_value,\n",
        "        'min_value': min_value\n",
        "    }\n"
      ],
      "metadata": {
        "id": "QBr9A3d00YfY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SCOPE"
      ],
      "metadata": {
        "id": "DzDCmT7zm4IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def SCOPE(scope_policies, beta, num_bootstraps):\n",
        "    all_timesteps = []\n",
        "    gamma = 0.9\n",
        "    # scope_policies,_ = subset_policies(scope_policies, phi_trajectories)\n",
        "    scope_weights = calculate_importance_weights(eval_policy, behav_policy, scope_policies)\n",
        "    for j in range(len(scope_weights)):\n",
        "        Timestep_values = []\n",
        "        for i in range(len(scope_weights[j]) - 1):\n",
        "            features = scope_policies[j][i][5] + scope_policies[j][i][6]\n",
        "            features_next = scope_policies[j][i + 1][5] + scope_policies[j][i + 1][6]\n",
        "            timestep = gamma ** (i) * scope_weights[j][i] * (scope_policies[j][i][2] + gamma * phi(features_next, beta) - phi(features, beta))\n",
        "            Timestep_values.append(timestep)\n",
        "\n",
        "        all_timesteps.append(Timestep_values)\n",
        "\n",
        "    V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "    seed_value = 42\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "    bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(num_bootstraps)]\n",
        "\n",
        "    V_per_sample = [sum(sample)/len(scope_policies) for sample in bootstrap_samples]\n",
        "    V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "    std_deviation = np.std(V_per_sample)\n",
        "    quartiles = np.percentile(V_per_sample, [25, 50, 75])\n",
        "    max_value = np.max(V_per_sample)\n",
        "    min_value = np.min(V_per_sample)\n",
        "\n",
        "    return {\n",
        "        'std_deviation': std_deviation,\n",
        "        'quartiles': quartiles,\n",
        "        'max_value': max_value,\n",
        "        'min_value': min_value\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Zo6h5pcQ0Qc9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance Preparation and Calculation"
      ],
      "metadata": {
        "id": "WL9BcVXloGpd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CfXiEdgqDE4D"
      },
      "outputs": [],
      "source": [
        "def phi(features, beta):\n",
        "  features = np.array(features)\n",
        "  beta = np.array(beta)\n",
        "  phi_s = np.dot(beta,features)\n",
        "  return phi_s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9NdwpKSkhxZR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# gamma = 0.9\n",
        "# beta = [random.random() for _ in range(3)]\n",
        "def variance_terms(policy_set,gamma, beta):\n",
        "  all_weights = calculate_importance_weights(eval_policy, behav_policy, policy_set)\n",
        "  y_w_r_all = 0\n",
        "  r_all = 0\n",
        "  f_a = 0\n",
        "  for j in range(len(policy_set)):\n",
        "    y_w_r = 0\n",
        "    r = 0\n",
        "    for i in range(len(policy_set[j])):\n",
        "      features = policy_set[j][i][5]+policy_set[j][i][6]\n",
        "      y_w_r += gamma**(i)*all_weights[j][i]*policy_set[j][i][2]\n",
        "      if i>0 & i<len(policy_set):\n",
        "        r += phi(features, beta)*(all_weights[j][i-1]-all_weights[j][i])\n",
        "    y_w_r_all += y_w_r\n",
        "    f_a +=  gamma**(len(policy_set[j]))*all_weights[j][-1]*phi(features,beta) - phi(features, beta) # fix the features part\n",
        "    r_all += r\n",
        "\n",
        "  IS = y_w_r_all/len(policy_set)\n",
        "  R = r_all/len(policy_set)\n",
        "  F = f_a/len(policy_set)\n",
        "  return IS, R, F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6BpxUSqy7K86"
      },
      "outputs": [],
      "source": [
        "\n",
        "def subset_policies(policies, percent_to_estimate_phi):\n",
        "    seed_value = 42\n",
        "    np.random.seed(seed_value)\n",
        "    num_policies = len(policies)\n",
        "    num_policies_to_estimate_phi = int(num_policies * percent_to_estimate_phi)\n",
        "\n",
        "    policies_for_scope = policies[num_policies_to_estimate_phi:]\n",
        "    policies_for_phi = policies[:num_policies_to_estimate_phi]\n",
        "\n",
        "    return policies_for_scope, policies_for_phi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calc_variance(phi_policies, gamma, beta, num_bootstrap_samples):\n",
        "  # Set the seed value (you can use any integer value)\n",
        "  seed_value = 42\n",
        "  np.random.seed(seed_value)\n",
        "  # _, phi_policies = subset_policies(phi_policies, percent_to_estimate_phi=phi_trajectories)\n",
        "  num_trajectories_to_sample = max(1, len(phi_policies))\n",
        "\n",
        "  bootstrap_samples = [np.random.choice(phi_policies, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(num_bootstrap_samples)]\n",
        "  IS_all = []\n",
        "  R_all = []\n",
        "  F_all = []\n",
        "\n",
        "  for pol in bootstrap_samples:\n",
        "    IS, R, F = variance_terms(pol,0.9,beta)\n",
        "    IS_all.append(IS)\n",
        "    R_all.append(R)\n",
        "    F_all.append(F)\n",
        "  IS_sq = np.mean([num**2 for num in IS_all])\n",
        "  IS_R_F = 2*np.mean([IS_all[i]*(R_all[i]+F_all[i]) for i in range(len(IS_all))])\n",
        "  R_sq = np.mean([num**2 for num in R_all])\n",
        "  IS_sq_all = (np.mean(IS_all))**2\n",
        "  IS_r_t_f = 2*np.mean(IS_all)*np.mean([R_all[i]+F_all[i] for i in range(len(R_all))])\n",
        "  R_sq_all = (np.mean(R_all))**2\n",
        "\n",
        "  variance_scope = IS_sq + IS_R_F + R_sq - IS_sq_all - IS_r_t_f - R_sq_all\n",
        "  variance_is = IS_sq - IS_sq_all\n",
        "  return variance_scope, variance_is"
      ],
      "metadata": {
        "id": "dTxzFiNTwV30"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of an initial guess of phi can be seen below, as you can see the SCOPE variance is not ideal."
      ],
      "metadata": {
        "id": "36vQxkjcnrEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scope_set, phi_set = subset_policies(behavior_policies, 0.3)\n",
        "variance_scope, variance_is = calc_variance(phi_set,0.9,[-0.1,.1,.1], 100, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74yBmg6_edJv",
        "outputId": "92e0acaf-5fc1-4131-81e9-273bba973508"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.7292656998003821\n",
            "Var IS:  0.3796743263659492\n",
            "Percent change in variance:  92.07664283770379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UefNOC1GC8Wm"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we aim to optimize beta to minimize SCOPE variance."
      ],
      "metadata": {
        "id": "RWx9IbJon2IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function to minimize variance_scope\n",
        "def objective_function(beta, phi_set):\n",
        "    # scope_set, phi_set = subset_policies(phi_set, phi_trajectories)\n",
        "    variance_scope, variance_is = calc_variance(phi_set, 0.9, beta, 100)\n",
        "    return variance_scope\n",
        "\n",
        "# Set the initial values of beta\n",
        "initial_beta = np.array([ 0.2610704,   0.30396575, -0.43850237])\n",
        "\n",
        "\n",
        "def optimize_variance_scope(initial_beta, phi_set, phi_trajectories):\n",
        "    # Lists to store beta and variance_scope values at each iteration\n",
        "    all_betas = []\n",
        "    all_variance_scopes = []\n",
        "\n",
        "    # Callback function to record beta and variance_scope values at each iteration\n",
        "    def callback_function(beta):\n",
        "        all_betas.append(beta.copy())\n",
        "        variance_scope = objective_function(beta, phi_set)\n",
        "        all_variance_scopes.append(variance_scope)\n",
        "        print(\"Iteration:\", len(all_betas))\n",
        "        print(\"Beta:\", beta)\n",
        "        print(\"Variance Scope:\", variance_scope)\n",
        "        print(\"----------\")\n",
        "\n",
        "    # Run the optimization with the callback\n",
        "    result = minimize(\n",
        "        objective_function,\n",
        "        initial_beta,\n",
        "        args=(phi_set),\n",
        "        method='L-BFGS-B',\n",
        "        callback=callback_function\n",
        "    )\n",
        "\n",
        "    # Extract the optimal beta values\n",
        "    optimal_beta = result.x\n",
        "\n",
        "    return optimal_beta\n"
      ],
      "metadata": {
        "id": "Ws00tRw1KZnN"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_beta = optimize_variance_scope(initial_beta, behavior_policies, 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZFyTciRkhpd",
        "outputId": "661864ee-246d-4fbf-af01-58cde24a3543"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [ 0.16821737  0.35271354 -0.42156545]\n",
            "Variance Scope: 0.11317799047435423\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.15853249  0.34137881 -0.42710298]\n",
            "Variance Scope: 0.11040088606596254\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.15073224  0.32968859 -0.42471605]\n",
            "Variance Scope: 0.1093933569252897\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.14615259  0.3202478  -0.4164246 ]\n",
            "Variance Scope: 0.10883472336459371\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.14331703  0.31081431 -0.40169853]\n",
            "Variance Scope: 0.108436847289928\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.14352606  0.31073858 -0.40072254]\n",
            "Variance Scope: 0.10843321214844745\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.14356653  0.31079481 -0.40072313]\n",
            "Variance Scope: 0.10843319408861363\n",
            "----------\n",
            "Iteration: 8\n",
            "Beta: [ 0.14356994  0.31080162 -0.40072872]\n",
            "Variance Scope: 0.10843319398401882\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value estimates of IS and SCOPE estimators"
      ],
      "metadata": {
        "id": "LUVgi1GMloz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all_weights = calculate_importance_weights(eval_policy, behav_policy, behavior_policies)\n",
        "beta =  [ 0.2609209,   0.47456879, -0.52815694]\n",
        "# beta = [0,0,0]\n",
        "V_per_sample, scope_std, scope_quartiles, scope_max_value, scope_min_value = SCOPE(behavior_policies,optimal_beta,0.3,300)\n",
        "print(\"SCOPE Std Dev: \", scope_std)\n",
        "print(\"SCOPE quartiles: \",scope_quartiles)\n",
        "print(\"SCOPE max: \",scope_max_value)\n",
        "print(\"SCOPE min\",scope_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHQ-x-S-hLnV",
        "outputId": "332add1b-f692-49f4-f837-d456cb61a0a9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCOPE Std Dev:  0.10569617855732055\n",
            "SCOPE quartiles:  [-0.59477096 -0.52791441 -0.44771446]\n",
            "SCOPE max:  -0.2758543575915438\n",
            "SCOPE min -0.8459066085675662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_per_traj, is_std, is_quartiles, is_max_value, is_min_value = per_step_IS(behavior_policies,0.3,300)\n",
        "print(\"IS std dev: \",is_std)\n",
        "print(\"IS quartiles: \",is_quartiles)\n",
        "print(\"IS max: \",is_max_value)\n",
        "print(\"IS min: \", is_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBVmYwOIhuTo",
        "outputId": "89ef9958-1310-4e73-d831-aaf399f2cbd4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IS std dev:  0.10933610817754\n",
            "IS quartiles:  [-0.84036769 -0.77909955 -0.69050267]\n",
            "IS max:  -0.5479659789843923\n",
            "IS min:  -1.0898425977186181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xTexgYskreS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assimilation"
      ],
      "metadata": {
        "id": "PsY0dioZrpEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)])\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 1000\n"
      ],
      "metadata": {
        "id": "IflYRufTrqQe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_policies_1000, filename = create_policy_set(env, behavior_policy, num_episodes)"
      ],
      "metadata": {
        "id": "J-xiQHlgrrJh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scope_set, phi_set = subset_policies(behavior_policies_1000, 0.3)\n",
        "variance_scope, variance_is = calc_variance(phi_set,0.9,[-0.1,.1,.1], 100, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COh19GO3sKYb",
        "outputId": "37f09c6d-7fd2-4d5d-ec2f-409ab64fd6af"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.032392092704986115\n",
            "Var IS:  0.0184793990245945\n",
            "Percent change in variance:  75.28758733914998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_beta = [-0.1,.1,.1]\n",
        "optimal_beta = optimize_variance_scope(initial_beta, behavior_policies_1000, 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRS-QsNHsWgw",
        "outputId": "7b15fbc5-34f2-4360-89a2-4923a395637e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [-0.06523181  0.02304533  0.05253761]\n",
            "Variance Scope: 0.02084052183274824\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [-0.02091298  0.02012022  0.04768606]\n",
            "Variance Scope: 0.01692647175961828\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [0.04789626 0.02635576 0.02143015]\n",
            "Variance Scope: 0.014199863342526705\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.0614707   0.03860064 -0.00288591]\n",
            "Variance Scope: 0.013542628515201995\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.12437417  0.14081556 -0.19456702]\n",
            "Variance Scope: 0.010246852619742673\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.12737027  0.19524445 -0.28980009]\n",
            "Variance Scope: 0.00932164732044462\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.12140381  0.20291535 -0.30178977]\n",
            "Variance Scope: 0.00926777196071786\n",
            "----------\n",
            "Iteration: 8\n",
            "Beta: [ 0.12023918  0.20215579 -0.3002091 ]\n",
            "Variance Scope: 0.009267233371063562\n",
            "----------\n",
            "Iteration: 9\n",
            "Beta: [ 0.12012512  0.20199007 -0.29989563]\n",
            "Variance Scope: 0.009267227627684869\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "variance_scope, variance_is = calc_variance(phi_set,0.9,optimal_beta, 300, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2OPoXh9wdQ",
        "outputId": "d507d820-f9aa-4091-865c-ed61db019105"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.00878962726137877\n",
            "Var IS:  0.015733865317291262\n",
            "Percent change in variance:  -44.13561395038057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V_per_traj, scope_std, scope_quartiles, scope_max_value, scope_min_value = SCOPE(behavior_policies_1000,optimal_beta,0.3,10000)\n",
        "print(\"SCOPE Std Dev: \", scope_std)\n",
        "print(\"SCOPE quartiles: \",scope_quartiles)\n",
        "print(\"SCOPE max: \",scope_max_value)\n",
        "print(\"SCOPE min\",scope_min_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAIgTfMKskLi",
        "outputId": "27d2be95-70cd-40ed-aa22-1afda90dec5b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCOPE Std Dev:  0.0847273731246662\n",
            "SCOPE quartiles:  [-0.73940798 -0.67815197 -0.6243099 ]\n",
            "SCOPE max:  -0.4319544216644854\n",
            "SCOPE min -1.0604403679851322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "at, IS_per_traj, is_std, is_quartiles, is_max_value, is_min_value = per_step_IS(behavior_policies_1000,0.3,1000)\n",
        "print(\"IS std dev: \",is_std)\n",
        "print(\"IS quartiles: \",is_quartiles)\n",
        "print(\"IS max: \",is_max_value)\n",
        "print(\"IS min: \", is_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXrb1n237q0R",
        "outputId": "4a67d882-0995-4d7e-e4ab-c74fd41e41cb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IS std dev:  0.055378959467358886\n",
            "IS quartiles:  [-0.8130771  -0.77558457 -0.74007423]\n",
            "IS max:  -0.6024242140435747\n",
            "IS min:  -0.93931194204315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d_L6pGLhm8NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_bad = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [])\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 1000\n"
      ],
      "metadata": {
        "id": "7QK8GxihmjeA"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_policies_bad_1000, filename = create_policy_set(env_bad, behavior_policy, num_episodes)"
      ],
      "metadata": {
        "id": "avT5FfwTmpTh"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scope_set, phi_set = subset_policies(behavior_policies_bad_1000, 0.3)\n",
        "variance_scope, variance_is = calc_variance(phi_set,0.9,[-0.1,.1], 100, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47JhNMSInAoa",
        "outputId": "973e94d7-1606-42c7-d241-57ed0a63bad9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.4941626782271849\n",
            "Var IS:  0.39076410364829994\n",
            "Percent change in variance:  26.460612326854605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_beta = [-0.1,.1]\n",
        "optimal_beta = optimize_variance_scope(initial_beta, behavior_policies_bad_1000, 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owQtb6mdnDwX",
        "outputId": "97688da1-70af-4e42-c408-90788277459e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [0.06354653 0.16516067]\n",
            "Variance Scope: 0.3081369785991253\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [0.09808462 0.09657364]\n",
            "Variance Scope: 0.29495541572903466\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.31395925 -0.33211788]\n",
            "Variance Scope: 0.25681217412820095\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "variance_scope, variance_is = calc_variance(behavior_policies_bad_1000,0.9,optimal_beta, 300, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifwEccyroGf5",
        "outputId": "67d62332-ac09-43ef-81c7-8502bdf346bc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.03902754903340479\n",
            "Var IS:  0.03555696106170689\n",
            "Percent change in variance:  9.760642833550683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V_per_traj, scope_std, scope_quartiles, scope_max_value, scope_min_value = SCOPE(behavior_policies_bad_1000,optimal_beta,0.3,10000)\n",
        "print(\"SCOPE Std Dev: \", scope_std)\n",
        "print(\"SCOPE quartiles: \",scope_quartiles)\n",
        "print(\"SCOPE max: \",scope_max_value)\n",
        "print(\"SCOPE min\",scope_min_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_psH1qU7ofjd",
        "outputId": "46a74337-2617-455b-d567-c7a193a06c0b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCOPE Std Dev:  0.03854389393810322\n",
            "SCOPE quartiles:  [-0.00872616  0.01719359  0.04284018]\n",
            "SCOPE max:  0.17204384949003806\n",
            "SCOPE min -0.11922521511670789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "at, IS_per_traj, is_std, is_quartiles, is_max_value, is_min_value = per_step_IS(behavior_policies_1000,0.3,1000)\n",
        "print(\"IS std dev: \",is_std)\n",
        "print(\"IS quartiles: \",is_quartiles)\n",
        "print(\"IS max: \",is_max_value)\n",
        "print(\"IS min: \", is_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuluyPbUon4o",
        "outputId": "9e98f5b4-fc3e-4425-86ef-67c84589ece3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IS std dev:  0.055378959467358886\n",
            "IS quartiles:  [-0.8130771  -0.77558457 -0.74007423]\n",
            "IS max:  -0.6024242140435747\n",
            "IS min:  -0.93931194204315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "W6UDEPbgEG7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "def run_experiment(env, num_episodes, behav_policy, eval_policy, phi_traj):\n",
        "  file = filename(env, behav_policy, eval_policy, num_episodes, phi_traj)\n",
        "  # Check if the file already exists\n",
        "  if os.path.exists(file):\n",
        "    loaded_data = load_data_from_file(file)\n",
        "    return loaded_data\n",
        "  behavior_policies = create_policy_set(env, run_policy,behav_policy, num_episodes)\n",
        "  initial_beta = [random.uniform(-0.5, 0.5) for _ in range(len(env.good_regions + env.bad_regions))]\n",
        "  scope_set, phi_set = subset_policies(behavior_policies, phi_traj)\n",
        "  optimal_beta = optimize_variance_scope(initial_beta, phi_set, phi_traj)\n",
        "  variance_scope, variance_is = calc_variance(phi_set,0.9,optimal_beta, 500)\n",
        "  print(\"Var SCOPE_phi: \",variance_scope)\n",
        "  print(\"Var IS_phi: \",variance_is)\n",
        "  print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)\n",
        "  scope_results = SCOPE(scope_set,optimal_beta,500)\n",
        "  IS_results = per_step_IS(scope_set,500)\n",
        "  print(\"SCOPE results: \", scope_results)\n",
        "  print(\"IS results: \", IS_results)\n",
        "  evaluation_policies = create_policy_set(env, run_policy,eval_policy, 1000)\n",
        "  true_evaluation = calc_V_pi_e(evaluation_policies)\n",
        "  print(\"true eval: \", true_evaluation)\n",
        "  data_to_save = {\n",
        "    'policy_set': behavior_policies,\n",
        "    'optimal_beta': optimal_beta,\n",
        "    'variance_scope_train': variance_scope,\n",
        "    'variance_IS_train': variance_is,\n",
        "    'scope_results': scope_results,\n",
        "    'IS_results': IS_results,\n",
        "    'True Evaluation': true_evaluation\n",
        "  }\n",
        "  save_data_to_file(data_to_save, file)\n",
        "\n",
        "  return data_to_save\n",
        "\n"
      ],
      "metadata": {
        "id": "F2qemIPUr_KL"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_bad = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [], 0.5, -2, 3 )"
      ],
      "metadata": {
        "id": "RUB7QtVtd5Ja"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_experiment(env_bad,200 ,behav_policy, eval_policy, 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70V9ZZOmd_ll",
        "outputId": "b7a449b1-80e7-4b6b-c694-c8b1c4e20de6"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [-0.23775529 -0.31538279]\n",
            "Variance Scope: 0.1952989531616176\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.00855792 -0.08425921]\n",
            "Variance Scope: 0.06434114007870334\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.0352773  -0.11086204]\n",
            "Variance Scope: 0.06380017194509344\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.08619689 -0.16507904]\n",
            "Variance Scope: 0.06337315182993845\n",
            "----------\n",
            "Var SCOPE_phi:  0.05957555228925791\n",
            "Var IS_phi:  0.0683541967788337\n",
            "Percent change in variance:  -12.842875643729526\n",
            "SCOPE results:  {'std_deviation': 0.24342653846751341, 'quartiles': array([-1.54442223, -1.3919995 , -1.21875629]), 'max_value': -0.704109558487164, 'min_value': -2.107781336742554}\n",
            "IS results:  {'std_deviation': 0.23874003778219374, 'quartiles': array([-1.9645772 , -1.82428064, -1.65781159]), 'max_value': -1.1542280596283199, 'min_value': -2.5997582966046138}\n",
            "true eval:  0.404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_trajectories = [200, 400, 600, 800, 1000]"
      ],
      "metadata": {
        "id": "22V2zzp0lstx"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in num_trajectories:\n",
        "  run_experiment(env_bad,i ,behav_policy, eval_policy, 0.3)\n",
        "  print(i,\" trajectories done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hggvFZ3mkl7",
        "outputId": "1bddddee-9aa9-47da-dfe7-b3bfc2ad12d2"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200  trajectories done\n",
            "Iteration: 1\n",
            "Beta: [-0.04182442  0.28337535]\n",
            "Variance Scope: 0.1449970355058024\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [0.03501534 0.15854378]\n",
            "Variance Scope: 0.12902630782358546\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.42167859 -0.46962023]\n",
            "Variance Scope: 0.0924749963173842\n",
            "----------\n",
            "Var SCOPE_phi:  0.10620962200244352\n",
            "Var IS_phi:  0.15946274760524215\n",
            "Percent change in variance:  -33.395339289292416\n",
            "SCOPE results:  {'std_deviation': 0.15043182531039923, 'quartiles': array([-0.5105846 , -0.41700359, -0.30639972]), 'max_value': 0.0391103159666555, 'min_value': -0.8527535898790396}\n",
            "IS results:  {'std_deviation': 0.1866995279574805, 'quartiles': array([-1.84457456, -1.71930555, -1.60012277]), 'max_value': -1.191328549806053, 'min_value': -2.341247034329893}\n",
            "true eval:  0.446\n",
            "400  trajectories done\n",
            "Iteration: 1\n",
            "Beta: [0.01539058 0.03108489]\n",
            "Variance Scope: 0.03554862988138942\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.06022739 -0.03786455]\n",
            "Variance Scope: 0.03314401745022946\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.35183104 -0.48629403]\n",
            "Variance Scope: 0.02588278335736985\n",
            "----------\n",
            "Var SCOPE_phi:  0.027471785016006892\n",
            "Var IS_phi:  0.03704370672129009\n",
            "Percent change in variance:  -25.83953538262395\n",
            "SCOPE results:  {'std_deviation': 0.12435128812983919, 'quartiles': array([-0.48807759, -0.40809986, -0.32952994]), 'max_value': -0.06760723634283511, 'min_value': -0.7695355641535668}\n",
            "IS results:  {'std_deviation': 0.16075135875442842, 'quartiles': array([-1.89569458, -1.79014709, -1.68774635]), 'max_value': -1.3040143548067793, 'min_value': -2.3542119839188453}\n",
            "true eval:  0.314\n",
            "600  trajectories done\n",
            "Iteration: 1\n",
            "Beta: [ 0.24635885 -0.19126773]\n",
            "Variance Scope: 0.14846975301297533\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.25263335 -0.20188272]\n",
            "Variance Scope: 0.14839161152771657\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.29553392 -0.27446192]\n",
            "Variance Scope: 0.14814265644647756\n",
            "----------\n",
            "Var SCOPE_phi:  0.19378481608452228\n",
            "Var IS_phi:  0.31295162222123163\n",
            "Percent change in variance:  -38.07834747457164\n",
            "SCOPE results:  {'std_deviation': 0.13019269439193687, 'quartiles': array([-1.56389129, -1.47532495, -1.37621889]), 'max_value': -1.134391634140759, 'min_value': -1.82413895983285}\n",
            "IS results:  {'std_deviation': 0.1529813518974417, 'quartiles': array([-2.26432161, -2.15506226, -2.05219274]), 'max_value': -1.7197789907384426, 'min_value': -2.5568071965530033}\n",
            "true eval:  0.318\n",
            "800  trajectories done\n",
            "Iteration: 1\n",
            "Beta: [0.42135308 0.27669758]\n",
            "Variance Scope: 0.18339776089543547\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [0.10378293 0.08211188]\n",
            "Variance Scope: 0.06748613310504817\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [0.14373605 0.00626419]\n",
            "Variance Scope: 0.06526678031951574\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.36242065 -0.36061234]\n",
            "Variance Scope: 0.06018831034609447\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.36382278 -0.36124102]\n",
            "Variance Scope: 0.06018760517949856\n",
            "----------\n",
            "Var SCOPE_phi:  0.05086023748515621\n",
            "Var IS_phi:  0.06628335038524746\n",
            "Percent change in variance:  -23.26845702646308\n",
            "SCOPE results:  {'std_deviation': 0.09277738542966339, 'quartiles': array([-0.76851919, -0.709626  , -0.64619076]), 'max_value': -0.3702517689174196, 'min_value': -1.0009232263040553}\n",
            "IS results:  {'std_deviation': 0.1062077578119959, 'quartiles': array([-1.68174335, -1.6088458 , -1.53504132]), 'max_value': -1.3021016723451344, 'min_value': -1.934495140978267}\n",
            "true eval:  0.402\n",
            "1000  trajectories done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7I78rynTvHAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scope_results = SCOPE(scope_set,optimal_beta,500)\n",
        "# IS_results = per_step_IS(scope_set,500)"
      ],
      "metadata": {
        "id": "BKgT8S79kQJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scope_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ioEd1K_gya-",
        "outputId": "7c63fc86-d634-48c8-f172-28c6284d533f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'std_deviation': 0.11261486039911385,\n",
              " 'quartiles': array([-1.07860049, -0.99737878, -0.92685939]),\n",
              " 'max_value': -0.6313860538166753,\n",
              " 'min_value': -1.422541698239632}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iivabjn1goP4",
        "outputId": "35583fd3-e33c-4c9b-9dba-4a89433a0061"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'std_deviation': 0.13408309598657783,\n",
              " 'quartiles': array([-1.67340765, -1.58713057, -1.4952746 ]),\n",
              " 'max_value': -1.1657089191395902,\n",
              " 'min_value': -2.0559831549212264}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ygM6yarDXcnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bC0trgvoXb1V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-mwQsHXgdzHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_policies, filename, optimal_beta, variance_scope, variance_is, scope_results, IS_results = run_experiment(env_bad,1000 ,behavior_policy, 0.7)\n",
        "print(scope_results)\n",
        "print(IS_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL6zmjSG1YqP",
        "outputId": "1578b7be-2729-4382-d6e9-60b223bb5e76"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [-0.12358608  0.43439658]\n",
            "Variance Scope: 0.0529948625719362\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [-0.02379625  0.26730518]\n",
            "Variance Scope: 0.04915401571774246\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.23031038 -0.15818471]\n",
            "Variance Scope: 0.04506639694166817\n",
            "----------\n",
            "Var SCOPE_phi:  0.04319949352023597\n",
            "Var IS_phi:  0.06137102558687213\n",
            "Percent change in variance:  -29.609301609134636\n",
            "{'std_deviation': 0.1816364652895848, 'quartiles': array([-1.63864017, -1.5189338 , -1.39122464]), 'max_value': -0.8817429883100243, 'min_value': -2.104600495146245}\n",
            "{'std_deviation': 0.1996321229174356, 'quartiles': array([-1.93377534, -1.79870724, -1.65558761]), 'max_value': -1.1563952708896945, 'min_value': -2.4648609292563046}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scope_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrl3RtIRY0iN",
        "outputId": "6af3a924-cbba-4fba-d08d-9ea06ec05fb2"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'std_deviation': 0.1339393292951269, 'quartiles': array([-1.72733479, -1.63439695, -1.54314097]), 'max_value': -1.2074829515578098, 'min_value': -2.0653949404468106}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyGBMU-6Y30d",
        "outputId": "478b114f-d7f0-4ec9-f525-e662f809d2f1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'std_deviation': 0.13650308099263406,\n",
              " 'quartiles': array([-1.67442867, -1.58017901, -1.4833015 ]),\n",
              " 'max_value': -1.1432981130479694,\n",
              " 'min_value': -2.0136122287610227}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scope_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RAbERuvGDef",
        "outputId": "72d7f7da-9742-4924-f086-e5ab02896e1c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'std_deviation': 0.11226431456528738,\n",
              " 'quartiles': array([-1.85448587, -1.77393822, -1.70213649]),\n",
              " 'max_value': -1.38917102086352,\n",
              " 'min_value': -2.2141242521410565}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_YK8K74I2IL",
        "outputId": "3836e457-7184-46ec-a098-e70feddf5432"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'std_deviation': 0.11294778498352055,\n",
              " 'quartiles': array([-1.76041381, -1.68035073, -1.60816667]),\n",
              " 'max_value': -1.2945023523807457,\n",
              " 'min_value': -2.1000001572694758}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "variance_scope"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcdTtzgdDrI1",
        "outputId": "5b4e3241-e447-45dd-af67-3dcc8da54ae5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02882402526179912"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "variance_is"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvAO3Gf0Ds6-",
        "outputId": "ca0e5017-5955-45f6-93fa-95d70838ea09"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03555696106170689"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scope_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNJ6JqYW3IZf",
        "outputId": "38609f6d-d577-484d-e403-dab5b5153f85"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'V_per_sample': array([-0.56437323, -0.5210324 , -0.53276965, -0.49357804, -0.52836147,\n",
              "        -0.48525697, -0.65673471, -0.50829075, -0.53725812, -0.50479599,\n",
              "        -0.51741058, -0.51025228, -0.55211292, -0.52298763, -0.5526282 ,\n",
              "        -0.44639213, -0.58234988, -0.46607688, -0.51961046, -0.51236296,\n",
              "        -0.5235279 , -0.49464656, -0.59479118, -0.49624564, -0.533716  ,\n",
              "        -0.53653165, -0.54664179, -0.52655398, -0.586763  , -0.57858333,\n",
              "        -0.54751258, -0.48122891, -0.53986367, -0.5672449 , -0.61747564,\n",
              "        -0.52192736, -0.56236636, -0.56381097, -0.55579657, -0.5027783 ,\n",
              "        -0.509657  , -0.56380593, -0.50112068, -0.51452382, -0.50140193,\n",
              "        -0.59108855, -0.5157604 , -0.57602769, -0.50275665, -0.61382958,\n",
              "        -0.58167523, -0.60457347, -0.48998986, -0.61905011, -0.51425333,\n",
              "        -0.53718976, -0.50730614, -0.48356443, -0.5153413 , -0.49916141,\n",
              "        -0.53786134, -0.52375351, -0.50299795, -0.56013226, -0.48882184,\n",
              "        -0.60247892, -0.54069533, -0.56992593, -0.54780032, -0.56014665,\n",
              "        -0.58543526, -0.51215982, -0.5319011 , -0.49130683, -0.53238727,\n",
              "        -0.50541529, -0.4889843 , -0.5223187 , -0.55197731, -0.54656156,\n",
              "        -0.54749938, -0.54336326, -0.53523207, -0.58760849, -0.55892862,\n",
              "        -0.49053067, -0.52107455, -0.54109059, -0.51831232, -0.49227456,\n",
              "        -0.45921089, -0.52346037, -0.6162177 , -0.53884352, -0.55380581,\n",
              "        -0.51066329, -0.55278879, -0.49569402, -0.6096075 , -0.55104762,\n",
              "        -0.52417267, -0.52592459, -0.53707632, -0.53276339, -0.50315098,\n",
              "        -0.56193352, -0.49858694, -0.57419457, -0.5156591 , -0.59216426,\n",
              "        -0.52664493, -0.5051072 , -0.52581264, -0.53416763, -0.54580921,\n",
              "        -0.53426723, -0.52373564, -0.66390524, -0.48492338, -0.6208616 ,\n",
              "        -0.52269742, -0.6015418 , -0.57674825, -0.60719035, -0.52576747,\n",
              "        -0.57647763, -0.65786605, -0.59322657, -0.52735107, -0.52427261,\n",
              "        -0.5380505 , -0.58856838, -0.50812422, -0.53042492, -0.5522481 ,\n",
              "        -0.5554106 , -0.55449005, -0.59682514, -0.50961235, -0.53528128,\n",
              "        -0.53614374, -0.55314675, -0.47276681, -0.55699588, -0.57545589,\n",
              "        -0.5243141 , -0.57954849, -0.5178449 , -0.5491711 , -0.54563852,\n",
              "        -0.47491233, -0.58054937, -0.50557088, -0.61126778, -0.59152009,\n",
              "        -0.57712723, -0.51820885, -0.61032723, -0.5275226 , -0.65237379,\n",
              "        -0.5704474 , -0.53392932, -0.48584467, -0.53055595, -0.5132609 ,\n",
              "        -0.57628436, -0.53308085, -0.52336689, -0.57281951, -0.59240967,\n",
              "        -0.52325642, -0.53039925, -0.50347066, -0.57123965, -0.61329231,\n",
              "        -0.6493547 , -0.48046235, -0.58320412, -0.47740754, -0.59039361,\n",
              "        -0.59594471, -0.46669515, -0.6643637 , -0.51034365, -0.55632343,\n",
              "        -0.50073235, -0.55318376, -0.57687578, -0.64238282, -0.48337605,\n",
              "        -0.43828526, -0.55637406, -0.55914356, -0.572873  , -0.57496841,\n",
              "        -0.52980385, -0.57667157, -0.59019156, -0.51614253, -0.59180063,\n",
              "        -0.57540261, -0.54412641, -0.62630563, -0.60885423, -0.61595112,\n",
              "        -0.55021655, -0.53473103, -0.51715244, -0.47401977, -0.52304869,\n",
              "        -0.50248056, -0.5522727 , -0.59957268, -0.53677173, -0.4940971 ,\n",
              "        -0.56788448, -0.62540042, -0.60972593, -0.48402808, -0.55800895,\n",
              "        -0.53150341, -0.51373908, -0.56934406, -0.52937032, -0.53776574,\n",
              "        -0.58015905, -0.57797236, -0.52691128, -0.58996192, -0.5588612 ,\n",
              "        -0.52659484, -0.51379281, -0.62992737, -0.54395546, -0.5438183 ,\n",
              "        -0.54408162, -0.57408944, -0.54140037, -0.54788069, -0.51275876,\n",
              "        -0.60184755, -0.60843274, -0.62335885, -0.51841623, -0.6087832 ,\n",
              "        -0.56076055, -0.54675687, -0.58612492, -0.46261087, -0.58397874,\n",
              "        -0.50424867, -0.4936329 , -0.49537929, -0.55388706, -0.48487594,\n",
              "        -0.56193756, -0.54429817, -0.54520243, -0.54696511, -0.56649523,\n",
              "        -0.57575749, -0.48955688, -0.52170753, -0.58154581, -0.62686981,\n",
              "        -0.57491777, -0.49659956, -0.47888381, -0.52169341, -0.51053809,\n",
              "        -0.60109373, -0.50936301, -0.61165552, -0.49622898, -0.63318911,\n",
              "        -0.55733546, -0.607239  , -0.56503136, -0.62104181, -0.57780914,\n",
              "        -0.60195912, -0.59664904, -0.54922648, -0.51382113, -0.62340241,\n",
              "        -0.53278523, -0.59471579, -0.46708117, -0.55360627, -0.51896651,\n",
              "        -0.59891849, -0.56386836, -0.48992687, -0.52488528, -0.53609312,\n",
              "        -0.48377176, -0.57518224, -0.48804033, -0.53898166, -0.45925322,\n",
              "        -0.5654269 , -0.48739658, -0.57811101, -0.49776735, -0.45902188,\n",
              "        -0.55898525, -0.50023643, -0.5813348 , -0.52488836, -0.48641697,\n",
              "        -0.48965503, -0.49414892, -0.56445056, -0.60792772, -0.53627872,\n",
              "        -0.47174732, -0.57754884, -0.47741669, -0.60708913, -0.50809866,\n",
              "        -0.60450663, -0.58777061, -0.51633451, -0.55541722, -0.52864839,\n",
              "        -0.57613726, -0.4799229 , -0.51523255, -0.56183696, -0.57168346,\n",
              "        -0.47869983, -0.61470428, -0.61143289, -0.54380788, -0.55796367,\n",
              "        -0.54993547, -0.56436735, -0.49698791, -0.50055171, -0.54380102,\n",
              "        -0.51052922, -0.57005795, -0.51846477, -0.64001277, -0.54835529,\n",
              "        -0.56331424, -0.60063737, -0.55709642, -0.55935114, -0.55285937,\n",
              "        -0.50921704, -0.61952819, -0.49832478, -0.56890417, -0.40137885,\n",
              "        -0.59758619, -0.56339285, -0.56138796, -0.49500286, -0.57697501,\n",
              "        -0.53122998, -0.54759872, -0.55368042, -0.58538347, -0.62657589,\n",
              "        -0.51084797, -0.48861937, -0.48050296, -0.57659006, -0.60015911,\n",
              "        -0.52719055, -0.54829993, -0.58126352, -0.55717048, -0.5321695 ,\n",
              "        -0.62552271, -0.57980828, -0.55312368, -0.54426699, -0.49573982,\n",
              "        -0.61838659, -0.64669372, -0.53943747, -0.55764948, -0.62537639,\n",
              "        -0.57175672, -0.55296941, -0.52539565, -0.61135477, -0.53625503,\n",
              "        -0.54268227, -0.51061622, -0.4806498 , -0.54980088, -0.61360253,\n",
              "        -0.60309438, -0.51862189, -0.56036151, -0.57697366, -0.54406028,\n",
              "        -0.53936319, -0.57738909, -0.51333762, -0.53086207, -0.54958016,\n",
              "        -0.60756631, -0.47892211, -0.57523979, -0.56377705, -0.5014104 ,\n",
              "        -0.54136383, -0.50239236, -0.60068898, -0.50210672, -0.57041383,\n",
              "        -0.5272763 , -0.60149055, -0.59293175, -0.58669952, -0.54242006,\n",
              "        -0.51959552, -0.58352174, -0.52270143, -0.5430444 , -0.47436112,\n",
              "        -0.49753769, -0.58397036, -0.47908429, -0.46307569, -0.58131555,\n",
              "        -0.55662328, -0.58561592, -0.59918986, -0.53198005, -0.56827249,\n",
              "        -0.46580546, -0.51883647, -0.54373134, -0.52355389, -0.50937999,\n",
              "        -0.48750812, -0.56370599, -0.46315333, -0.55209967, -0.541338  ,\n",
              "        -0.53087225, -0.6255034 , -0.45254825, -0.52780529, -0.54046922,\n",
              "        -0.50635113, -0.57087579, -0.55330733, -0.57312657, -0.51524801,\n",
              "        -0.49559238, -0.57164906, -0.52298398, -0.55855755, -0.47484408,\n",
              "        -0.53820357, -0.61157608, -0.56321941, -0.53510401, -0.58756428,\n",
              "        -0.54799887, -0.53126847, -0.53298918, -0.58848658, -0.63217641,\n",
              "        -0.4901729 , -0.59511285, -0.52308631, -0.47676757, -0.47802342,\n",
              "        -0.50670941, -0.49360349, -0.55169622, -0.60632576, -0.55038791,\n",
              "        -0.59082737, -0.53161187, -0.56839112, -0.54815168, -0.513612  ,\n",
              "        -0.57705682, -0.50225547, -0.55804904, -0.5879483 , -0.51424005,\n",
              "        -0.52785865, -0.52003828, -0.47539913, -0.60986625, -0.54783442,\n",
              "        -0.5283767 , -0.49852824, -0.54332448, -0.51654349, -0.54089182,\n",
              "        -0.57194421, -0.52357835, -0.60183094, -0.58473391, -0.57692687,\n",
              "        -0.50812417, -0.62536923, -0.53251298, -0.52980679, -0.59476922,\n",
              "        -0.52084093, -0.55717269, -0.49151031, -0.53778446, -0.45028316,\n",
              "        -0.56865816, -0.51267241, -0.45912076, -0.58165179, -0.54430853,\n",
              "        -0.49898989, -0.57279586, -0.50834137, -0.48363214, -0.55895444,\n",
              "        -0.61476756, -0.47556223, -0.57836407, -0.59683125, -0.53701517,\n",
              "        -0.58790831, -0.57937135, -0.58390448, -0.60747286, -0.56598995,\n",
              "        -0.51768619, -0.50509132, -0.46863475, -0.53521896, -0.51529097,\n",
              "        -0.60961735, -0.57755673, -0.54158943, -0.53433448, -0.56439341,\n",
              "        -0.5144815 , -0.65913536, -0.56571092, -0.56381647, -0.48541093,\n",
              "        -0.46028261, -0.62422514, -0.6473471 , -0.5431253 , -0.49603473,\n",
              "        -0.51055155, -0.52807033, -0.46447464, -0.55397792, -0.51220117,\n",
              "        -0.56286521, -0.5541387 , -0.5719758 , -0.58691874, -0.54292548,\n",
              "        -0.57824736, -0.55639404, -0.60749672, -0.51956572, -0.54836914,\n",
              "        -0.57020339, -0.57123398, -0.65574848, -0.61379952, -0.58260341,\n",
              "        -0.56219457, -0.56935572, -0.55102362, -0.5122826 , -0.61983777,\n",
              "        -0.48460347, -0.49469993, -0.64111548, -0.55107274, -0.54905034,\n",
              "        -0.58948274, -0.56006979, -0.51306577, -0.51417937, -0.47589867,\n",
              "        -0.57486577, -0.56509218, -0.47173976, -0.58065943, -0.54075938,\n",
              "        -0.56312489, -0.58080053, -0.52761452, -0.50066809, -0.56651903,\n",
              "        -0.48872855, -0.54764014, -0.5778718 , -0.52842022, -0.58755075,\n",
              "        -0.54073462, -0.52473395, -0.58811314, -0.50277986, -0.58189366,\n",
              "        -0.58122169, -0.53062975, -0.56834548, -0.56720051, -0.51476397,\n",
              "        -0.47790045, -0.58361288, -0.51933804, -0.51340609, -0.60699472,\n",
              "        -0.54745497, -0.52643445, -0.54177342, -0.55433809, -0.54486439,\n",
              "        -0.47461396, -0.54427902, -0.47670375, -0.4935474 , -0.51492612,\n",
              "        -0.58404526, -0.61004663, -0.49855902, -0.54607957, -0.57197744,\n",
              "        -0.54600339, -0.62468985, -0.58146791, -0.61075276, -0.57337796,\n",
              "        -0.53090755, -0.5356999 , -0.51942561, -0.53148203, -0.54918732,\n",
              "        -0.48752409, -0.52587275, -0.52661763, -0.56835233, -0.49868485,\n",
              "        -0.58977384, -0.6085358 , -0.55838201, -0.59327604, -0.46872775,\n",
              "        -0.58831387, -0.51793538, -0.56267853, -0.61880579, -0.5940479 ,\n",
              "        -0.56727673, -0.51844082, -0.56030995, -0.55058694, -0.57578233,\n",
              "        -0.53171045, -0.49950945, -0.52373366, -0.49745597, -0.50466715,\n",
              "        -0.51770146, -0.58806787, -0.55268563, -0.51431148, -0.50767221,\n",
              "        -0.57729499, -0.4991246 , -0.5226312 , -0.52214364, -0.59062129,\n",
              "        -0.47004758, -0.52882131, -0.59343799, -0.59327907, -0.58082398,\n",
              "        -0.5351526 , -0.47752039, -0.45044279, -0.56790517, -0.5774404 ,\n",
              "        -0.49528212, -0.51258703, -0.58848495, -0.47804093, -0.51880228,\n",
              "        -0.5511704 , -0.54129139, -0.50851355, -0.53259283, -0.57321275,\n",
              "        -0.50309416, -0.52738013, -0.51823749, -0.53830672, -0.51049458,\n",
              "        -0.461147  , -0.50507436, -0.51539397, -0.51161823, -0.5722364 ,\n",
              "        -0.47117223, -0.50855659, -0.56552326, -0.59098977, -0.55297639,\n",
              "        -0.49432211, -0.57129677, -0.46646762, -0.48474638, -0.58473124,\n",
              "        -0.48457373, -0.60408499, -0.51467478, -0.4953398 , -0.47676676,\n",
              "        -0.52947736, -0.5932028 , -0.54935577, -0.55585448, -0.51446306,\n",
              "        -0.54314453, -0.52385637, -0.52296975, -0.52011494, -0.47672356,\n",
              "        -0.5060382 , -0.47151733, -0.48284244, -0.59296184, -0.61659038,\n",
              "        -0.54852302, -0.5670191 , -0.54158032, -0.50377016, -0.54412318,\n",
              "        -0.53859031, -0.54214961, -0.54695083, -0.5544521 , -0.45932033,\n",
              "        -0.57687422, -0.50491791, -0.52365992, -0.55369359, -0.49169695,\n",
              "        -0.58250293, -0.53804285, -0.48654046, -0.55552307, -0.52271144,\n",
              "        -0.63865766, -0.55994428, -0.50894201, -0.55191371, -0.57821892,\n",
              "        -0.51622951, -0.49514837, -0.51159079, -0.57219373, -0.5776318 ,\n",
              "        -0.51988332, -0.58947599, -0.51108146, -0.52932126, -0.51658397,\n",
              "        -0.44233498, -0.51806357, -0.48240106, -0.4875991 , -0.60486129,\n",
              "        -0.53912098, -0.53414538, -0.55683423, -0.52573518, -0.55489424,\n",
              "        -0.56664508, -0.57047624, -0.58522071, -0.60084175, -0.6589749 ,\n",
              "        -0.51236375, -0.514233  , -0.60559238, -0.53799153, -0.50004865,\n",
              "        -0.61348289, -0.45157367, -0.59306263, -0.53153598, -0.59932077,\n",
              "        -0.44207963, -0.58863939, -0.52414651, -0.54780977, -0.53500651,\n",
              "        -0.54853224, -0.53903969, -0.56052316, -0.52735239, -0.6055033 ,\n",
              "        -0.55875756, -0.45715422, -0.55684436, -0.52451654, -0.57581032,\n",
              "        -0.62755294, -0.56131352, -0.48685362, -0.50754906, -0.52508464,\n",
              "        -0.50858803, -0.53440961, -0.58207885, -0.55008682, -0.54315801,\n",
              "        -0.55698105, -0.46440964, -0.52734954, -0.56132962, -0.58019602,\n",
              "        -0.55775334, -0.48714281, -0.56369484, -0.58608261, -0.69799551,\n",
              "        -0.56454759, -0.52724936, -0.5401442 , -0.58111452, -0.54384784,\n",
              "        -0.47740705, -0.55487658, -0.57604417, -0.54792759, -0.44872184,\n",
              "        -0.58357642, -0.55249905, -0.55278217, -0.57355599, -0.47010305,\n",
              "        -0.55595278, -0.54929495, -0.58088891, -0.49332027, -0.59413042,\n",
              "        -0.55352987, -0.60787871, -0.51469155, -0.49447557, -0.49699488,\n",
              "        -0.52138427, -0.56165162, -0.54891979, -0.62831494, -0.51874972,\n",
              "        -0.56308591, -0.54707087, -0.50518872, -0.49840916, -0.55455253,\n",
              "        -0.61738459, -0.53222416, -0.49135682, -0.56147412, -0.49227315,\n",
              "        -0.54210597, -0.55876409, -0.54283366, -0.52812905, -0.59648158,\n",
              "        -0.47634558, -0.56139879, -0.60032308, -0.58418823, -0.47316546,\n",
              "        -0.64399107, -0.52013251, -0.61253789, -0.53904385, -0.51156733,\n",
              "        -0.58777199, -0.51216175, -0.57414062, -0.51773617, -0.63016729,\n",
              "        -0.62589575, -0.55322888, -0.55999944, -0.52649678, -0.54603143,\n",
              "        -0.55231914, -0.56022297, -0.56968584, -0.57834499, -0.56255972,\n",
              "        -0.58813852, -0.51101643, -0.56114057, -0.52151552, -0.62653981,\n",
              "        -0.52684071, -0.59981232, -0.53750358, -0.56841887, -0.49405264,\n",
              "        -0.55216948, -0.48947914, -0.54878258, -0.58389953, -0.55536312,\n",
              "        -0.51746614, -0.59602539, -0.56645266, -0.5037549 , -0.54794928,\n",
              "        -0.48914298, -0.52655647, -0.46713457, -0.53614319, -0.59090327,\n",
              "        -0.55241113, -0.49200369, -0.44100875, -0.53717622, -0.60648508,\n",
              "        -0.51654234, -0.51701954, -0.59969531, -0.56645855, -0.54417896,\n",
              "        -0.58145179, -0.64259522, -0.52054435, -0.61547925, -0.50862842,\n",
              "        -0.58038837, -0.52762191, -0.61233421, -0.54876373, -0.5047539 ,\n",
              "        -0.52742497, -0.49315453, -0.51362659, -0.55485428, -0.62203616,\n",
              "        -0.51134618, -0.53756448, -0.48622473, -0.54809587, -0.56084107,\n",
              "        -0.62640167, -0.48145026, -0.61169006, -0.60924386, -0.57741311,\n",
              "        -0.56970271, -0.59673423, -0.6127291 , -0.56457597, -0.4507584 ,\n",
              "        -0.52871266, -0.48381246, -0.54779366, -0.51838588, -0.59381153,\n",
              "        -0.53658592, -0.58027405, -0.582087  , -0.63133022, -0.52543212,\n",
              "        -0.56425211, -0.43633533, -0.52100079, -0.49915399, -0.50104303,\n",
              "        -0.5488572 , -0.51925185, -0.47129899, -0.54437373, -0.59049624,\n",
              "        -0.60960773, -0.49797951, -0.47758944, -0.58028447, -0.49809917,\n",
              "        -0.54975519, -0.53933729, -0.50853486, -0.47510372, -0.55429658]),\n",
              " 'std_deviation': 0.04372596456945925,\n",
              " 'quartiles': array([-0.57633268, -0.5441248 , -0.51381405]),\n",
              " 'max_value': -0.401378850749117,\n",
              " 'min_value': -0.6979955123486603}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzoBw0TZ3L6u",
        "outputId": "b0ebb04f-9efc-4e38-a49c-ba44a52f0a3e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'V_per_sample': array([-0.80635585, -0.75962151, -0.77404319, -0.75063382, -0.78921466,\n",
              "        -0.70901995, -0.91690172, -0.74059884, -0.77565571, -0.73872761,\n",
              "        -0.76039001, -0.76384259, -0.78081748, -0.76341132, -0.79888797,\n",
              "        -0.67438609, -0.82316216, -0.7058464 , -0.75662919, -0.75524291,\n",
              "        -0.76704005, -0.72611658, -0.83798647, -0.7354355 , -0.77420199,\n",
              "        -0.79142962, -0.7876153 , -0.76480896, -0.86244563, -0.84710444,\n",
              "        -0.8076942 , -0.74988262, -0.77494965, -0.82438352, -0.87213392,\n",
              "        -0.76803174, -0.8031787 , -0.79272544, -0.79058759, -0.75362274,\n",
              "        -0.76540027, -0.80281143, -0.74207932, -0.75591317, -0.75086944,\n",
              "        -0.85047168, -0.76030188, -0.81211215, -0.74341216, -0.84473291,\n",
              "        -0.84508997, -0.8692639 , -0.70776828, -0.86327176, -0.76208407,\n",
              "        -0.76971978, -0.72928233, -0.71224534, -0.76311836, -0.7425491 ,\n",
              "        -0.78983302, -0.76709464, -0.74886825, -0.81634316, -0.71775411,\n",
              "        -0.86844368, -0.79423608, -0.81165372, -0.80388172, -0.81808446,\n",
              "        -0.84789102, -0.76872915, -0.77494245, -0.71638034, -0.77454538,\n",
              "        -0.73853546, -0.72174308, -0.77076925, -0.81228813, -0.79515243,\n",
              "        -0.79267526, -0.78034178, -0.78529847, -0.82502141, -0.80355918,\n",
              "        -0.75607711, -0.77092215, -0.77004036, -0.75916903, -0.70994732,\n",
              "        -0.68541759, -0.76244919, -0.88163767, -0.79076372, -0.78894643,\n",
              "        -0.72289476, -0.8173986 , -0.73348426, -0.86351252, -0.83465698,\n",
              "        -0.76787177, -0.76424896, -0.77078171, -0.76617556, -0.75220622,\n",
              "        -0.8172796 , -0.75251896, -0.81590185, -0.76514209, -0.83200088,\n",
              "        -0.75826007, -0.72922251, -0.75585781, -0.77285829, -0.81199214,\n",
              "        -0.76743498, -0.7538693 , -0.92355007, -0.72874919, -0.90109701,\n",
              "        -0.75954132, -0.85810897, -0.82074492, -0.87223232, -0.76950121,\n",
              "        -0.82615932, -0.924523  , -0.83735935, -0.77416424, -0.77368777,\n",
              "        -0.76846001, -0.8275858 , -0.73043975, -0.77947123, -0.79232535,\n",
              "        -0.81453455, -0.80058248, -0.85904126, -0.75330772, -0.79316254,\n",
              "        -0.76284614, -0.78484077, -0.70416491, -0.80591288, -0.80212452,\n",
              "        -0.75432445, -0.83163818, -0.77716595, -0.80038448, -0.78903282,\n",
              "        -0.69356749, -0.84659388, -0.73312677, -0.8598394 , -0.86324717,\n",
              "        -0.82689012, -0.75056416, -0.84736118, -0.79908453, -0.91074614,\n",
              "        -0.8255713 , -0.78729614, -0.72410086, -0.74001506, -0.77305786,\n",
              "        -0.81139689, -0.77084262, -0.77592829, -0.80033777, -0.84385282,\n",
              "        -0.76894161, -0.78173715, -0.72776157, -0.82657581, -0.86906889,\n",
              "        -0.9152526 , -0.73845863, -0.85095505, -0.70179642, -0.83522086,\n",
              "        -0.86078668, -0.69176813, -0.9273494 , -0.74110276, -0.79740762,\n",
              "        -0.7266955 , -0.79084584, -0.81106096, -0.90363012, -0.72192984,\n",
              "        -0.66932394, -0.78940475, -0.78819139, -0.83498091, -0.82959864,\n",
              "        -0.77989551, -0.82212402, -0.84546457, -0.76929425, -0.83904204,\n",
              "        -0.83604947, -0.79008754, -0.88782011, -0.85793634, -0.8458382 ,\n",
              "        -0.79726824, -0.79522932, -0.75256252, -0.69854314, -0.76810121,\n",
              "        -0.75384645, -0.81961601, -0.84354734, -0.80257623, -0.69927788,\n",
              "        -0.82047694, -0.87693864, -0.86895168, -0.71712363, -0.82105631,\n",
              "        -0.78693345, -0.75218607, -0.79979586, -0.77280698, -0.81117619,\n",
              "        -0.82062979, -0.83805376, -0.74694318, -0.84095346, -0.79322269,\n",
              "        -0.7799547 , -0.76069686, -0.87588165, -0.78739282, -0.7769117 ,\n",
              "        -0.78723128, -0.83917838, -0.79463777, -0.80817692, -0.74986444,\n",
              "        -0.85269815, -0.86872883, -0.86416091, -0.75226073, -0.89699267,\n",
              "        -0.81706868, -0.80248272, -0.83457542, -0.68157782, -0.82005112,\n",
              "        -0.72529596, -0.74693858, -0.7456662 , -0.81197274, -0.6878132 ,\n",
              "        -0.795433  , -0.79792246, -0.78404203, -0.78628642, -0.81402763,\n",
              "        -0.83824494, -0.71281919, -0.77613995, -0.82872806, -0.8748274 ,\n",
              "        -0.83519921, -0.73078058, -0.72393567, -0.76290648, -0.74464269,\n",
              "        -0.83126553, -0.72839981, -0.8620733 , -0.73354248, -0.89255685,\n",
              "        -0.80062111, -0.86174151, -0.79484758, -0.87566545, -0.83440566,\n",
              "        -0.86452527, -0.85737867, -0.80903194, -0.74828362, -0.86177052,\n",
              "        -0.77523674, -0.84315076, -0.68625955, -0.80807839, -0.75443639,\n",
              "        -0.85658225, -0.82205774, -0.72059297, -0.78447913, -0.77845559,\n",
              "        -0.73364568, -0.83567226, -0.73646494, -0.78244877, -0.69872886,\n",
              "        -0.81543988, -0.73115   , -0.82366438, -0.68914915, -0.70880592,\n",
              "        -0.78200135, -0.74088576, -0.81339887, -0.75945346, -0.73777122,\n",
              "        -0.73213251, -0.72807181, -0.8111376 , -0.84295594, -0.76629798,\n",
              "        -0.71823773, -0.81079882, -0.70101091, -0.8582839 , -0.76615731,\n",
              "        -0.86625195, -0.85763894, -0.76310939, -0.79379892, -0.76756863,\n",
              "        -0.80914758, -0.68633383, -0.74565084, -0.77370562, -0.82485153,\n",
              "        -0.71836607, -0.87668283, -0.84171304, -0.80401375, -0.79149638,\n",
              "        -0.80736927, -0.81915979, -0.73165324, -0.74338137, -0.76868277,\n",
              "        -0.74955121, -0.82315844, -0.7575972 , -0.91206222, -0.77448632,\n",
              "        -0.8037957 , -0.85307726, -0.81065038, -0.80416834, -0.79830277,\n",
              "        -0.74057544, -0.87814006, -0.72782364, -0.79441117, -0.61962318,\n",
              "        -0.85754276, -0.79593152, -0.81406082, -0.73156423, -0.83305727,\n",
              "        -0.77084932, -0.79629072, -0.8032708 , -0.81302806, -0.9129231 ,\n",
              "        -0.75331605, -0.72617376, -0.71497129, -0.8284043 , -0.88591918,\n",
              "        -0.76879725, -0.80176588, -0.82975062, -0.81469665, -0.76742566,\n",
              "        -0.88523472, -0.86134096, -0.79493333, -0.77606888, -0.74093093,\n",
              "        -0.88903612, -0.88722849, -0.81614111, -0.81574328, -0.86185395,\n",
              "        -0.82053719, -0.80578941, -0.75894138, -0.88914053, -0.77961269,\n",
              "        -0.79857371, -0.74967955, -0.71544653, -0.79614081, -0.84911778,\n",
              "        -0.84179401, -0.75270758, -0.81770883, -0.83192241, -0.76321851,\n",
              "        -0.78767041, -0.81680582, -0.7558253 , -0.78083146, -0.79692398,\n",
              "        -0.87301861, -0.70997608, -0.81640996, -0.82776483, -0.72087765,\n",
              "        -0.80011434, -0.74877648, -0.85367564, -0.74971146, -0.82251459,\n",
              "        -0.7843518 , -0.85040635, -0.82923021, -0.85203552, -0.78674254,\n",
              "        -0.75028855, -0.834918  , -0.76379138, -0.79902421, -0.7156759 ,\n",
              "        -0.74032616, -0.8375168 , -0.73807509, -0.70868715, -0.84499142,\n",
              "        -0.7887397 , -0.845906  , -0.871758  , -0.7975035 , -0.80087379,\n",
              "        -0.6998655 , -0.74749922, -0.78147068, -0.77801292, -0.7492758 ,\n",
              "        -0.71103498, -0.80357319, -0.69685001, -0.8246252 , -0.79411863,\n",
              "        -0.78300915, -0.85533522, -0.67000811, -0.75429261, -0.77728251,\n",
              "        -0.75106294, -0.81497189, -0.80672119, -0.81130901, -0.75963144,\n",
              "        -0.71758278, -0.79978861, -0.75873842, -0.80043028, -0.71325812,\n",
              "        -0.75794953, -0.84960813, -0.83506425, -0.76151242, -0.85212953,\n",
              "        -0.79215492, -0.77815293, -0.80445747, -0.82865728, -0.8848997 ,\n",
              "        -0.73154448, -0.83083511, -0.751448  , -0.70652414, -0.72047362,\n",
              "        -0.73674425, -0.72673403, -0.791701  , -0.86175456, -0.78690463,\n",
              "        -0.86344145, -0.77226087, -0.81996743, -0.78811532, -0.75512751,\n",
              "        -0.81738488, -0.74669192, -0.8088759 , -0.83888856, -0.74048476,\n",
              "        -0.77094563, -0.77228082, -0.69884883, -0.86957492, -0.77397966,\n",
              "        -0.775255  , -0.75832763, -0.78147575, -0.76329522, -0.76922738,\n",
              "        -0.80089456, -0.79023488, -0.85649093, -0.83590791, -0.82530333,\n",
              "        -0.7595728 , -0.86352301, -0.77203308, -0.77858127, -0.85253575,\n",
              "        -0.75533061, -0.79908574, -0.71853487, -0.76995987, -0.68396836,\n",
              "        -0.80222333, -0.76130182, -0.6924794 , -0.81331329, -0.79748675,\n",
              "        -0.72545501, -0.83218166, -0.76592983, -0.70450213, -0.78851256,\n",
              "        -0.88779069, -0.70152317, -0.83195494, -0.85648267, -0.77576301,\n",
              "        -0.83208579, -0.82300225, -0.84755173, -0.85486914, -0.83332075,\n",
              "        -0.75663294, -0.72864873, -0.7149308 , -0.78819362, -0.74950395,\n",
              "        -0.88001928, -0.81706753, -0.7575617 , -0.77128737, -0.80354148,\n",
              "        -0.76624684, -0.92354805, -0.81391548, -0.8086893 , -0.7292022 ,\n",
              "        -0.71672319, -0.87069816, -0.89198519, -0.78985354, -0.73413553,\n",
              "        -0.73479118, -0.75930673, -0.67631146, -0.78870746, -0.74460014,\n",
              "        -0.80987428, -0.80331933, -0.82963204, -0.83193301, -0.79622571,\n",
              "        -0.8218704 , -0.82520365, -0.85281537, -0.7499924 , -0.79974822,\n",
              "        -0.80564952, -0.79420425, -0.9276999 , -0.85384752, -0.84826508,\n",
              "        -0.81921495, -0.79507347, -0.82045432, -0.73003298, -0.88444061,\n",
              "        -0.73512027, -0.74503121, -0.90730238, -0.7903724 , -0.80371506,\n",
              "        -0.86130001, -0.83936232, -0.73924992, -0.7654984 , -0.72699571,\n",
              "        -0.81970134, -0.82225314, -0.67529057, -0.82432096, -0.79836576,\n",
              "        -0.8009539 , -0.82905847, -0.78499278, -0.72704281, -0.82699116,\n",
              "        -0.72140044, -0.78379925, -0.81949201, -0.76067014, -0.8343838 ,\n",
              "        -0.79497454, -0.74573278, -0.84844308, -0.75784031, -0.83850393,\n",
              "        -0.84183759, -0.79788238, -0.82036672, -0.82322014, -0.76228535,\n",
              "        -0.71912492, -0.84689069, -0.76966395, -0.748492  , -0.85836419,\n",
              "        -0.79401115, -0.76960982, -0.77664335, -0.78816559, -0.78475143,\n",
              "        -0.70242228, -0.77685739, -0.70051364, -0.72772161, -0.74799701,\n",
              "        -0.84118821, -0.86040476, -0.71626209, -0.80070557, -0.8159865 ,\n",
              "        -0.81348887, -0.88706199, -0.84718711, -0.8993413 , -0.84292654,\n",
              "        -0.7758699 , -0.79447729, -0.74931349, -0.75743079, -0.80982056,\n",
              "        -0.71986202, -0.77768706, -0.74237969, -0.80236597, -0.7210424 ,\n",
              "        -0.85617082, -0.85227396, -0.78860627, -0.85436668, -0.70252587,\n",
              "        -0.82406861, -0.74570102, -0.79950516, -0.86677105, -0.8381488 ,\n",
              "        -0.81746366, -0.76141008, -0.80507409, -0.80053097, -0.83722179,\n",
              "        -0.77484405, -0.73295329, -0.7552431 , -0.73305307, -0.75301594,\n",
              "        -0.75691965, -0.82043253, -0.80071722, -0.75080073, -0.74206024,\n",
              "        -0.82625577, -0.7131307 , -0.75682208, -0.76421333, -0.8436094 ,\n",
              "        -0.71601668, -0.76872505, -0.84431994, -0.8610089 , -0.82435838,\n",
              "        -0.77757918, -0.7058759 , -0.69959997, -0.81608664, -0.81064818,\n",
              "        -0.75904649, -0.76462115, -0.82168689, -0.70032988, -0.74851885,\n",
              "        -0.8093097 , -0.78445185, -0.74344102, -0.77693949, -0.83881345,\n",
              "        -0.74173781, -0.76866277, -0.78604879, -0.79197304, -0.74329999,\n",
              "        -0.69270986, -0.75114508, -0.75575148, -0.7364603 , -0.82258566,\n",
              "        -0.69732426, -0.74021589, -0.8133591 , -0.83000918, -0.80588463,\n",
              "        -0.74075412, -0.84328694, -0.69044705, -0.71328364, -0.8391358 ,\n",
              "        -0.73985021, -0.85435611, -0.73872747, -0.72410299, -0.70263653,\n",
              "        -0.78276486, -0.84823397, -0.80560916, -0.8071107 , -0.75969025,\n",
              "        -0.77196925, -0.78290448, -0.76492279, -0.75166459, -0.74238725,\n",
              "        -0.71894187, -0.70131559, -0.70178976, -0.87177575, -0.86294069,\n",
              "        -0.79878415, -0.81536675, -0.79092071, -0.75768705, -0.79002008,\n",
              "        -0.76536852, -0.78474761, -0.80846763, -0.76999493, -0.67921092,\n",
              "        -0.84545752, -0.74515446, -0.78386894, -0.81371725, -0.73377202,\n",
              "        -0.83110229, -0.78351175, -0.73503168, -0.81116854, -0.78973869,\n",
              "        -0.88514612, -0.81365825, -0.74407622, -0.78195033, -0.82342067,\n",
              "        -0.76212265, -0.73418978, -0.74806423, -0.80686352, -0.83683096,\n",
              "        -0.75106041, -0.80872312, -0.75414807, -0.75998604, -0.75073872,\n",
              "        -0.66841343, -0.72482791, -0.72286156, -0.69110813, -0.85689872,\n",
              "        -0.7823201 , -0.76445813, -0.80252116, -0.76275434, -0.80824223,\n",
              "        -0.81641208, -0.81088056, -0.82023881, -0.85042075, -0.92193581,\n",
              "        -0.75858361, -0.7429864 , -0.87734491, -0.78133699, -0.73243128,\n",
              "        -0.87466862, -0.66066036, -0.8419908 , -0.78548415, -0.84126795,\n",
              "        -0.66812072, -0.84009387, -0.77015345, -0.78068187, -0.79681782,\n",
              "        -0.78499687, -0.78556763, -0.80946233, -0.77508489, -0.86248394,\n",
              "        -0.80052561, -0.70435909, -0.81256174, -0.75546415, -0.81867048,\n",
              "        -0.87698029, -0.82819184, -0.7247182 , -0.74771754, -0.79051799,\n",
              "        -0.76449418, -0.7733612 , -0.83941921, -0.7957861 , -0.80278099,\n",
              "        -0.80044137, -0.70078329, -0.77207365, -0.82952488, -0.84571182,\n",
              "        -0.80932389, -0.75190196, -0.80609963, -0.842602  , -0.95886096,\n",
              "        -0.80690475, -0.7711602 , -0.77521983, -0.8431232 , -0.78370637,\n",
              "        -0.71732917, -0.77977284, -0.86224952, -0.79165536, -0.65155122,\n",
              "        -0.84031522, -0.79593575, -0.78447293, -0.81091895, -0.70139346,\n",
              "        -0.79854633, -0.79651559, -0.82702202, -0.7317172 , -0.83186156,\n",
              "        -0.7868527 , -0.86040808, -0.74400941, -0.74641223, -0.73883578,\n",
              "        -0.77229088, -0.82134164, -0.80871711, -0.89099039, -0.75952076,\n",
              "        -0.80694898, -0.7783495 , -0.74432747, -0.74615383, -0.8313688 ,\n",
              "        -0.88002016, -0.77701616, -0.7189024 , -0.80285927, -0.73327961,\n",
              "        -0.79152079, -0.82368949, -0.78848879, -0.75871816, -0.84490906,\n",
              "        -0.71421844, -0.80983964, -0.84910446, -0.83390349, -0.72398593,\n",
              "        -0.90423469, -0.76900188, -0.88125562, -0.77506914, -0.75096242,\n",
              "        -0.84967187, -0.74448216, -0.81122837, -0.76408133, -0.88387748,\n",
              "        -0.89759743, -0.80761453, -0.84044421, -0.75987323, -0.79546177,\n",
              "        -0.80299431, -0.83773332, -0.80568736, -0.82095039, -0.82404915,\n",
              "        -0.85877874, -0.75979281, -0.81246219, -0.75039742, -0.86427063,\n",
              "        -0.7692826 , -0.85777879, -0.77655135, -0.80448258, -0.72937088,\n",
              "        -0.80227248, -0.74101717, -0.8334841 , -0.83353264, -0.80427671,\n",
              "        -0.77131233, -0.81971997, -0.85359669, -0.70481667, -0.78904833,\n",
              "        -0.74112407, -0.75265555, -0.70165041, -0.79106017, -0.85836403,\n",
              "        -0.78045818, -0.73361646, -0.65279136, -0.8035234 , -0.85780445,\n",
              "        -0.76212886, -0.75256471, -0.89678691, -0.83953843, -0.79546401,\n",
              "        -0.83743248, -0.9240732 , -0.72782614, -0.86261873, -0.73532757,\n",
              "        -0.82111922, -0.76444097, -0.86192204, -0.78848046, -0.73117863,\n",
              "        -0.77311965, -0.72784301, -0.74977034, -0.79045075, -0.9017795 ,\n",
              "        -0.76444402, -0.78204116, -0.73144429, -0.8017375 , -0.81543982,\n",
              "        -0.87274105, -0.72808482, -0.86860827, -0.88614858, -0.81241039,\n",
              "        -0.81119802, -0.85852943, -0.88016919, -0.81795707, -0.70132486,\n",
              "        -0.75805685, -0.71620481, -0.79334752, -0.7680815 , -0.84671721,\n",
              "        -0.76704185, -0.81292955, -0.82542387, -0.89188381, -0.76662872,\n",
              "        -0.82157186, -0.65709251, -0.74740287, -0.72174043, -0.72792282,\n",
              "        -0.77224269, -0.75522922, -0.68929448, -0.77319   , -0.81434961,\n",
              "        -0.85133812, -0.70843171, -0.69493125, -0.80283375, -0.7425945 ,\n",
              "        -0.79341483, -0.7656792 , -0.73331146, -0.6966122 , -0.8067118 ]),\n",
              " 'std_deviation': 0.052204462702121895,\n",
              " 'quartiles': array([-0.82444394, -0.78913149, -0.7524544 ]),\n",
              " 'max_value': -0.6196231789676053,\n",
              " 'min_value': -0.9588609588380539}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IpJF8Iqq3MAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QvQKAbrREQ-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62-UMuTfERSN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPH0pW45HBYVcCVtWfZGCjU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}