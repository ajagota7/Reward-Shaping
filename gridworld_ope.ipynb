{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajagota7/Reward-Shaping/blob/main/gridworld_ope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt3QR_86NGdz"
      },
      "source": [
        "# Creating Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "-Ww-v9JI2Hhj"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, height, width, start, end, bad_regions, good_regions):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.bad_regions = bad_regions\n",
        "        self.good_regions = good_regions\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = self.start\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_position\n",
        "\n",
        "        if action == \"up\" and y < self.height - 1:\n",
        "            y += 1\n",
        "        elif action == \"down\" and y > 0:\n",
        "            y -= 1\n",
        "        elif action == \"left\" and x > 0:\n",
        "            x -= 1\n",
        "        elif action == \"right\" and x < self.width - 1:\n",
        "            x += 1\n",
        "\n",
        "        self.agent_position = (x, y)\n",
        "\n",
        "        if self.agent_position == self.end:\n",
        "            reward = 3\n",
        "            done = True\n",
        "        elif self.agent_position in self.bad_regions:\n",
        "            reward = -1\n",
        "            done = False\n",
        "        elif self.agent_position in self.good_regions:\n",
        "            reward = 0.5\n",
        "            done = False\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return (x, y), reward, done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "HQACkJ1xWBoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, epsilon=0.0):\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def select_action(self, policy_func):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            # Choose a random action\n",
        "            action = np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "        else:\n",
        "            # Use the provided policy function to get the best action\n",
        "            action = policy_func()\n",
        "        return action\n",
        "\n",
        "# Define different policy functions outside the class\n",
        "\n",
        "def random_policy():\n",
        "    # Choose a random action\n",
        "    return np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "\n",
        "def behavior_policy():\n",
        "    action_probs = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def evaluation_policy():\n",
        "    action_probs = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def manhattan_distance(pos1, pos2):\n",
        "    # Compute the Manhattan distance between two positions\n",
        "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GipigR-ZNTo6"
      },
      "source": [
        "# Generating Policy data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gridworld environment\n",
        "height = 5\n",
        "width  = 5\n",
        "start = (0,0)\n",
        "end = (4,4)"
      ],
      "metadata": {
        "id": "5ActZ0YInJCE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IgfscYWMW0XS"
      },
      "outputs": [],
      "source": [
        "\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)])\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 200\n",
        "\n",
        "def create_policy_set(env, policy, num_episodes):\n",
        "  # Create a list to store policies as trajectories\n",
        "  policies = []\n",
        "\n",
        "  # Run multiple episodes\n",
        "  for episode in range(num_episodes):\n",
        "      # Create a new Agent for each episode to generate a different policy\n",
        "      agent = Agent(epsilon=0.0)\n",
        "\n",
        "      # Run an episode\n",
        "      env.reset()\n",
        "      done = False\n",
        "      trajectory = []  # Store the trajectory for the current episode\n",
        "      cumulative_reward = 0.0  # Initialize cumulative reward\n",
        "      while not done:\n",
        "          state = env.agent_position  # Get the current state\n",
        "          action = agent.select_action(policy)\n",
        "          next_state, reward, done = env.step(action)\n",
        "\n",
        "          # Compute cumulative reward\n",
        "          cumulative_reward += reward\n",
        "\n",
        "          # Compute feature function values (manhattan distances)\n",
        "          good_region_distances = [manhattan_distance(state, gr) for gr in env.good_regions]\n",
        "          bad_region_distances = [manhattan_distance(state, br) for br in env.bad_regions]\n",
        "\n",
        "          # Store the (state, action, reward, next_state) tuple in the trajectory\n",
        "          trajectory.append((state, action, reward, next_state, cumulative_reward, good_region_distances, bad_region_distances))\n",
        "\n",
        "      # Append the trajectory to the policies list\n",
        "      policies.append(trajectory)\n",
        "\n",
        "  good_regions_str = \"_\".join([f\"gr_{pos[0]}_{pos[1]}\" for pos in env.good_regions])\n",
        "  bad_regions_str = \"_\".join([f\"br_{pos[0]}_{pos[1]}\" for pos in env.bad_regions])\n",
        "  filename = f\"{env.__class__.__name__}_policy_{policy.__name__}_{good_regions_str}_{bad_regions_str}.txt\"\n",
        "\n",
        "  return policies, filename\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_V_pi_e(evaluation_policies):\n",
        "\n",
        "  total_cumulative_reward = 0.0\n",
        "\n",
        "  for episode_trajectory in evaluation_policies:\n",
        "      total_cumulative_reward += episode_trajectory[-1][4]\n",
        "\n",
        "  return total_cumulative_reward/len(evaluation_policies)"
      ],
      "metadata": {
        "id": "v0rRjUcUf4Tv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_policies, a = create_policy_set(env, behavior_policy, 200)"
      ],
      "metadata": {
        "id": "_ZzGc9yegxaW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V05T0HkwSaZA",
        "outputId": "1507da16-1e5f-44b6-abd9-7d960be4f802"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GridWorld_policy_behavior_policy_gr_3_3_br_1_1_br_2_2.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_policies = create_policy_set(env, evaluation_policy, 1000)\n",
        "V_pi_e = calc_V_pi_e(evaluation_policies)"
      ],
      "metadata": {
        "id": "ZRFZ42OyMht9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and Loading Data"
      ],
      "metadata": {
        "id": "3UdUjHYmPsZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def save_policies_to_file(policies, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(policies, file)\n",
        "\n",
        "def load_policies_from_file(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        policies = pickle.load(file)\n",
        "    return policies\n"
      ],
      "metadata": {
        "id": "CfI_8J51Pwer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkhpULigM6wI"
      },
      "source": [
        "# Training Reward Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyIPQCnIBAO"
      },
      "source": [
        "## State -> Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOWRWObYB7R"
      },
      "source": [
        "Training model to predict rewards based on state only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcB-vIWj2V8r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    rewards = [state_action_reward[2] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_rewards.extend(rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_rewards = np.array(all_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPnb1zoYE3N3"
      },
      "outputs": [],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictorStates(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorStates, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_states = RewardPredictorStates(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_states.fit(\n",
        "    all_next_states, all_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KaLPE-xb2e0",
        "outputId": "230b8ee6-0a6f-46a1-adcd-45f52ac7becf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Current State: -0.03829813\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Next State: -0.006269932\n"
          ]
        }
      ],
      "source": [
        "current_state = behavior_policies[0][5][0]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])\n",
        "\n",
        "new_state = behavior_policies[0][5][3]  # Replace ... with the new state for which you want to predict the reward\n",
        "new_state = np.array(new_state, dtype=np.float32)\n",
        "predicted_reward_next = reward_predictor_states.predict(np.expand_dims(new_state, axis=0))\n",
        "print(\"Predicted Reward Next State:\", predicted_reward_next[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7rtgAXVINap"
      },
      "source": [
        "## State -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f79jQoF-tQXR"
      },
      "source": [
        "Reward model based on State -> Cumulative Rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfr9e-LRtUEj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_cum_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    cum_rewards = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_cum_rewards.extend(cum_rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_cum_rewards = np.array(all_cum_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBE4eg3xtpYa"
      },
      "outputs": [],
      "source": [
        "# Step 2: Design the neural network\n",
        "# import tensorflow as tf\n",
        "\n",
        "class RewardPredictorCumulative(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorCumulative, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense3 = tf.keras.layers.Dense(1, kernel_initializer='he_normal')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_cumulative = RewardPredictorCumulative(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_cumulative.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_cumulative.fit(\n",
        "    all_next_states, all_cum_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdFBS44nIUn_"
      },
      "source": [
        "## State -> Rewards over past 3 timesteps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIkOPgG07ffz"
      },
      "source": [
        "Cumulative rewards over 3 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlBb0H0NSYDz"
      },
      "outputs": [],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_behavior_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in behavior_policies\n",
        "for trajectory in behavior_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox, bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_behavior_policies list\n",
        "    augmented_behavior_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06vVUBLO23-Y"
      },
      "outputs": [],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_evaluation_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in evaluation_policies\n",
        "for trajectory in evaluation_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox. bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_evaluation_policies list\n",
        "    augmented_evaluation_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pngt7A8eLoOl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "def preprocess_nstep_data(policy_data):\n",
        "  # Initialize lists to store all 'next_state' and 'reward' values\n",
        "  all_next_states = []\n",
        "  all_past3_rewards = []\n",
        "\n",
        "  # Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "  for trajectory in policy_data:\n",
        "      # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "      next_states = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "      rewards = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "\n",
        "      # Append the values to the corresponding lists\n",
        "      all_next_states.extend(next_states)\n",
        "      all_past3_rewards.extend(rewards)\n",
        "\n",
        "  # Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "  all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "  all_past3_rewards = np.array(all_past3_rewards, dtype=np.float32)\n",
        "  return all_next_states, all_past3_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCAp1fg2LUfU"
      },
      "outputs": [],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictor3States(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictor3States, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frcPgL555yen",
        "outputId": "86b058ae-b14a-478b-f0c1-2ac4d03875b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "541/541 [==============================] - 2s 2ms/step - loss: 0.1163 - val_loss: 0.0807\n",
            "Epoch 2/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0498 - val_loss: 0.0302\n",
            "Epoch 3/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0184 - val_loss: 0.0124\n",
            "Epoch 4/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0085 - val_loss: 0.0050\n",
            "Epoch 5/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0036 - val_loss: 0.0026\n",
            "Epoch 6/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 7/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 8.8721e-04 - val_loss: 6.9921e-04\n",
            "Epoch 8/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.3470e-04 - val_loss: 2.7174e-04\n",
            "Epoch 9/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.5016e-04 - val_loss: 4.5576e-04\n",
            "Epoch 10/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8726e-04 - val_loss: 3.1336e-04\n",
            "Epoch 11/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.3167e-04 - val_loss: 2.8632e-04\n",
            "Epoch 12/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.0322e-04 - val_loss: 1.9239e-04\n",
            "Epoch 13/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.2741e-04 - val_loss: 1.2284e-04\n",
            "Epoch 14/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.3022e-04 - val_loss: 2.0752e-04\n",
            "Epoch 15/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.6412e-04 - val_loss: 2.0257e-05\n",
            "Epoch 16/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.0789e-06 - val_loss: 6.2865e-07\n",
            "Epoch 17/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.2175e-05 - val_loss: 6.4133e-05\n",
            "Epoch 18/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.5053e-04 - val_loss: 6.7341e-05\n",
            "Epoch 19/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.5021e-04 - val_loss: 4.7176e-05\n",
            "Epoch 20/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8386e-04 - val_loss: 9.6860e-05\n",
            "Epoch 21/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.8277e-04 - val_loss: 1.4178e-04\n",
            "Epoch 22/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.4954e-04 - val_loss: 4.4968e-05\n",
            "Epoch 23/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.9865e-05 - val_loss: 2.5360e-04\n",
            "Epoch 24/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.9564e-04 - val_loss: 1.2803e-04\n",
            "Epoch 25/1000\n",
            "541/541 [==============================] - 1s 3ms/step - loss: 6.9617e-05 - val_loss: 2.7309e-06\n",
            "Epoch 26/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 4.5949e-04 - val_loss: 5.9128e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bc0d40ae080>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Behavior policies training\n",
        "all_next_states_behav, all_past3_rewards_behav = preprocess_nstep_data(augmented_behavior_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_behav.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states.fit(\n",
        "    all_next_states_behav, all_past3_rewards_behav,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeNydEaSIz6v",
        "outputId": "848f8b86-c1f0-4100-d492-4fd8fe3e64b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted Reward Current State: 0.0006133178\n"
          ]
        }
      ],
      "source": [
        "current_state = [2,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr0W6G3l6btP"
      },
      "outputs": [],
      "source": [
        "# Evaluation policies training\n",
        "all_next_states_eval, all_past3_rewards_eval = preprocess_nstep_data(augmented_evaluation_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_eval.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states_eval = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states_eval.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states_eval.fit(\n",
        "    all_next_states_eval, all_past3_rewards_eval,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeEQ3Isp7KxB"
      },
      "outputs": [],
      "source": [
        "current_state = [4,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states_eval.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyxbNoxzs_ah"
      },
      "source": [
        "State, action, Next State -> Reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm0AQ1adtC3-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# # Initialize lists to store all 'current_state', 'action', 'next_state', and 'reward' values\n",
        "# all_current_states = []\n",
        "# all_actions = []\n",
        "# all_next_states = []\n",
        "# all_rewards = []\n",
        "\n",
        "# # Extract the 'current_state', 'action', 'next_state', and 'reward' from the 'behavior_policies' list\n",
        "# for trajectory in behavior_policies:\n",
        "#     for state, action, reward, next_state in trajectory:\n",
        "#         all_current_states.append(state)\n",
        "#         all_actions.append(action)\n",
        "#         all_next_states.append(next_state)\n",
        "#         all_rewards.append(reward)\n",
        "\n",
        "# # Convert 'current_states', 'actions', 'next_states', and 'rewards' into appropriate formats for training\n",
        "# all_current_states = np.array(all_current_states, dtype=np.float32)\n",
        "# all_actions = np.array(all_actions, dtype=np.float32)\n",
        "# all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "# all_rewards = np.array(all_rewards, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx2TXRLpJdVV"
      },
      "source": [
        "## State + proximity to good/bad regions -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPE Calculations"
      ],
      "metadata": {
        "id": "6nNbcCLTOMxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance Weights"
      ],
      "metadata": {
        "id": "608qtLdqhbOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3ykvUcj6YAJM"
      },
      "outputs": [],
      "source": [
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "def calculate_importance_weights(eval_policy, behav_policy, behavior_policies):\n",
        "  all_weights = []\n",
        "  for trajectory in behavior_policies:\n",
        "    cum_ratio = 1\n",
        "    cumul_weights = []\n",
        "    for step in trajectory:\n",
        "        ratio = eval_policy[step[1]]/behav_policy[step[1]]\n",
        "        # print(\"Ratio:\",ratio)\n",
        "        cum_ratio *= ratio\n",
        "        cumul_weights.append(cum_ratio)\n",
        "        # print(\"Cumul:\",cum_ratio)\n",
        "    all_weights.append(cumul_weights)\n",
        "\n",
        "  return all_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IS"
      ],
      "metadata": {
        "id": "VgadQNBwlT2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "0yCj9q-qGxvf"
      },
      "outputs": [],
      "source": [
        "def per_step_IS(behavior_policies, phi_trajectories, num_boostraps):\n",
        "  all_timesteps = []\n",
        "  gamma = 0.9\n",
        "  policy_for_scope,_ = subset_policies(behavior_policies, phi_trajectories)\n",
        "  scope_weights = calculate_importance_weights(eval_policy, behav_policy, policy_for_scope)\n",
        "  for j in range(len(scope_weights)):\n",
        "    Timestep_values = []\n",
        "    for i in range(len(scope_weights[j])-1):\n",
        "      timestep = gamma**(i)*scope_weights[j][i]*policy_for_scope[j][i][2]\n",
        "      # print(\"Timestep: \",timestep)\n",
        "      Timestep_values.append(timestep)\n",
        "\n",
        "    all_timesteps.append(Timestep_values)\n",
        "\n",
        "  V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "  seed_value = 42\n",
        "  np.random.seed(seed_value)\n",
        "\n",
        "  num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "  bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "                        for _ in range(num_boostraps)]\n",
        "\n",
        "  V_per_sample = [sum(sample)/len(policy_for_scope) for sample in bootstrap_samples]\n",
        "  V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "  std_deviation = np.std(V_per_sample)\n",
        "  quartiles = np.percentile(V_per_sample, [25, 50, 75])\n",
        "  max_value = np.max(V_per_sample)\n",
        "  min_value = np.min(V_per_sample)\n",
        "\n",
        "  return all_timesteps, V_per_sample, std_deviation, quartiles, max_value, min_value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SCOPE"
      ],
      "metadata": {
        "id": "DzDCmT7zm4IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def SCOPE(behavior_policies, beta, phi_trajectories, num_bootstraps):\n",
        "    all_timesteps = []\n",
        "    gamma = 0.9\n",
        "    policy_for_scope,_ = subset_policies(behavior_policies, phi_trajectories)\n",
        "    scope_weights = calculate_importance_weights(eval_policy, behav_policy, policy_for_scope)\n",
        "    for j in range(len(scope_weights)):\n",
        "        Timestep_values = []\n",
        "        for i in range(len(scope_weights[j]) - 1):\n",
        "            features = policy_for_scope[j][i][5] + policy_for_scope[j][i][6]\n",
        "            features_next = policy_for_scope[j][i + 1][5] + policy_for_scope[j][i + 1][6]\n",
        "            timestep = gamma ** (i) * scope_weights[j][i] * (policy_for_scope[j][i][2] + gamma * phi(features_next, beta) - phi(features, beta))\n",
        "            Timestep_values.append(timestep)\n",
        "\n",
        "        all_timesteps.append(Timestep_values)\n",
        "\n",
        "    V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "\n",
        "\n",
        "    seed_value = 42\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "    bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(num_bootstraps)]\n",
        "\n",
        "    V_per_sample = [sum(sample)/len(policy_for_scope) for sample in bootstrap_samples]\n",
        "    V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "    std_deviation = np.std(V_per_sample)\n",
        "    quartiles = np.percentile(V_per_sample, [25, 50, 75])\n",
        "    max_value = np.max(V_per_sample)\n",
        "    min_value = np.min(V_per_sample)\n",
        "\n",
        "    return V_per_sample, std_deviation, quartiles, max_value, min_value\n"
      ],
      "metadata": {
        "id": "BgqnBM4abWL9"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance Preparation and Calculation"
      ],
      "metadata": {
        "id": "WL9BcVXloGpd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CfXiEdgqDE4D"
      },
      "outputs": [],
      "source": [
        "def phi(features, beta):\n",
        "  features = np.array(features)\n",
        "  beta = np.array(beta)\n",
        "  phi_s = np.dot(beta,features)\n",
        "  return phi_s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9NdwpKSkhxZR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# gamma = 0.9\n",
        "# beta = [random.random() for _ in range(3)]\n",
        "def variance_terms(policy_set,gamma, beta):\n",
        "  all_weights = calculate_importance_weights(eval_policy, behav_policy, policy_set)\n",
        "  y_w_r_all = 0\n",
        "  r_all = 0\n",
        "  f_a = 0\n",
        "  for j in range(len(policy_set)):\n",
        "    y_w_r = 0\n",
        "    r = 0\n",
        "    for i in range(len(policy_set[j])):\n",
        "      features = policy_set[j][i][5]+policy_set[j][i][6]\n",
        "      y_w_r += gamma**(i)*all_weights[j][i]*policy_set[j][i][2]\n",
        "      if i>0 & i<len(policy_set):\n",
        "        r += phi(features, beta)*(all_weights[j][i-1]-all_weights[j][i])\n",
        "    y_w_r_all += y_w_r\n",
        "    f_a +=  gamma**(len(policy_set[j]))*all_weights[j][-1]*phi(features,beta) - phi(features, beta) # fix the features part\n",
        "    r_all += r\n",
        "\n",
        "  IS = y_w_r_all/len(policy_set)\n",
        "  R = r_all/len(policy_set)\n",
        "  F = f_a/len(policy_set)\n",
        "  return IS, R, F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6BpxUSqy7K86"
      },
      "outputs": [],
      "source": [
        "\n",
        "def subset_policies(policy, percent_to_estimate_phi):\n",
        "    seed_value = 42\n",
        "    np.random.seed(seed_value)\n",
        "    num_policies = len(policy)\n",
        "    num_policies_to_estimate_phi = int(num_policies * percent_to_estimate_phi)\n",
        "\n",
        "    policy_for_scope = policy[num_policies_to_estimate_phi:]\n",
        "    policy_for_phi = policy[:num_policies_to_estimate_phi]\n",
        "\n",
        "    return policy_for_scope, policy_for_phi\n",
        "\n",
        "def calc_variance(behavior_policies, gamma, beta, num_bootstrap_samples = 100,phi_trajectories = 0.3):\n",
        "  # Set the seed value (you can use any integer value)\n",
        "  seed_value = 42\n",
        "  np.random.seed(seed_value)\n",
        "  num_trajectories_to_sample = max(1, int(len(behavior_policies) * phi_trajectories))\n",
        "\n",
        "  policy_for_scope, policy_for_phi = subset_policies(behavior_policies, percent_to_estimate_phi=phi_trajectories)\n",
        "  num_trajectories_to_sample = max(1, len(policy_for_phi))\n",
        "\n",
        "  bootstrap_samples = [np.random.choice(policy_for_phi, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(num_bootstrap_samples)]\n",
        "  IS_all = []\n",
        "  R_all = []\n",
        "  F_all = []\n",
        "\n",
        "  for pol in bootstrap_samples:\n",
        "    IS, R, F = variance_terms(pol,0.9,beta)\n",
        "    IS_all.append(IS)\n",
        "    R_all.append(R)\n",
        "    F_all.append(F)\n",
        "  IS_sq = np.mean([num**2 for num in IS_all])\n",
        "  IS_R_F = 2*np.mean([IS_all[i]*(R_all[i]+F_all[i]) for i in range(len(IS_all))])\n",
        "  R_sq = np.mean([num**2 for num in R_all])\n",
        "  IS_sq_all = (np.mean(IS_all))**2\n",
        "  IS_r_t_f = 2*np.mean(IS_all)*np.mean([R_all[i]+F_all[i] for i in range(len(R_all))])\n",
        "  R_sq_all = (np.mean(R_all))**2\n",
        "\n",
        "  variance_scope = IS_sq + IS_R_F + R_sq - IS_sq_all - IS_r_t_f - R_sq_all\n",
        "  variance_is = IS_sq - IS_sq_all\n",
        "  return variance_scope, variance_is"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of an initial guess of phi can be seen below, as you can see the SCOPE variance is not ideal."
      ],
      "metadata": {
        "id": "36vQxkjcnrEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scope_set, phi_set = subset_policies(behavior_policies, 0.3)\n",
        "variance_scope, variance_is = calc_variance(phi_set,0.9,[-0.1,.1,.1], 100, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74yBmg6_edJv",
        "outputId": "92e0acaf-5fc1-4131-81e9-273bba973508"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.7292656998003821\n",
            "Var IS:  0.3796743263659492\n",
            "Percent change in variance:  92.07664283770379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UefNOC1GC8Wm"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we aim to optimize beta to minimize SCOPE variance."
      ],
      "metadata": {
        "id": "RWx9IbJon2IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function to minimize variance_scope\n",
        "def objective_function(beta, behavior_policies, phi_trajectories):\n",
        "    scope_set, phi_set = subset_policies(behavior_policies, phi_trajectories)\n",
        "    variance_scope, variance_is = calc_variance(phi_set, 0.9, beta, 100, 0.3)\n",
        "    return variance_scope\n",
        "\n",
        "# Set the initial values of beta\n",
        "initial_beta = np.array([ 0.2610704,   0.30396575, -0.43850237])\n",
        "\n",
        "\n",
        "def optimize_variance_scope(initial_beta, behavior_policies, phi_trajectories):\n",
        "    # Lists to store beta and variance_scope values at each iteration\n",
        "    all_betas = []\n",
        "    all_variance_scopes = []\n",
        "\n",
        "    # Callback function to record beta and variance_scope values at each iteration\n",
        "    def callback_function(beta):\n",
        "        all_betas.append(beta.copy())\n",
        "        variance_scope = objective_function(beta, behavior_policies, phi_trajectories)\n",
        "        all_variance_scopes.append(variance_scope)\n",
        "        print(\"Iteration:\", len(all_betas))\n",
        "        print(\"Beta:\", beta)\n",
        "        print(\"Variance Scope:\", variance_scope)\n",
        "        print(\"----------\")\n",
        "\n",
        "    # Run the optimization with the callback\n",
        "    result = minimize(\n",
        "        objective_function,\n",
        "        initial_beta,\n",
        "        args=(behavior_policies, phi_trajectories),\n",
        "        method='L-BFGS-B',\n",
        "        callback=callback_function\n",
        "    )\n",
        "\n",
        "    # Extract the optimal beta values\n",
        "    optimal_beta = result.x\n",
        "\n",
        "    return optimal_beta\n"
      ],
      "metadata": {
        "id": "Ws00tRw1KZnN"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_beta = optimize_variance_scope(initial_beta, behavior_policies, 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZFyTciRkhpd",
        "outputId": "661864ee-246d-4fbf-af01-58cde24a3543"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [ 0.16821737  0.35271354 -0.42156545]\n",
            "Variance Scope: 0.11317799047435423\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.15853249  0.34137881 -0.42710298]\n",
            "Variance Scope: 0.11040088606596254\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.15073224  0.32968859 -0.42471605]\n",
            "Variance Scope: 0.1093933569252897\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.14615259  0.3202478  -0.4164246 ]\n",
            "Variance Scope: 0.10883472336459371\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.14331703  0.31081431 -0.40169853]\n",
            "Variance Scope: 0.108436847289928\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.14352606  0.31073858 -0.40072254]\n",
            "Variance Scope: 0.10843321214844745\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.14356653  0.31079481 -0.40072313]\n",
            "Variance Scope: 0.10843319408861363\n",
            "----------\n",
            "Iteration: 8\n",
            "Beta: [ 0.14356994  0.31080162 -0.40072872]\n",
            "Variance Scope: 0.10843319398401882\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value estimates of IS and SCOPE estimators"
      ],
      "metadata": {
        "id": "LUVgi1GMloz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all_weights = calculate_importance_weights(eval_policy, behav_policy, behavior_policies)\n",
        "beta =  [ 0.2609209,   0.47456879, -0.52815694]\n",
        "# beta = [0,0,0]\n",
        "V_per_sample, scope_std, scope_quartiles, scope_max_value, scope_min_value = SCOPE(behavior_policies,optimal_beta,0.3,300)\n",
        "print(\"SCOPE Std Dev: \", scope_std)\n",
        "print(\"SCOPE quartiles: \",scope_quartiles)\n",
        "print(\"SCOPE max: \",scope_max_value)\n",
        "print(\"SCOPE min\",scope_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHQ-x-S-hLnV",
        "outputId": "332add1b-f692-49f4-f837-d456cb61a0a9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCOPE Std Dev:  0.10569617855732055\n",
            "SCOPE quartiles:  [-0.59477096 -0.52791441 -0.44771446]\n",
            "SCOPE max:  -0.2758543575915438\n",
            "SCOPE min -0.8459066085675662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_per_traj, is_std, is_quartiles, is_max_value, is_min_value = per_step_IS(behavior_policies,0.3,300)\n",
        "print(\"IS std dev: \",is_std)\n",
        "print(\"IS quartiles: \",is_quartiles)\n",
        "print(\"IS max: \",is_max_value)\n",
        "print(\"IS min: \", is_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBVmYwOIhuTo",
        "outputId": "89ef9958-1310-4e73-d831-aaf399f2cbd4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IS std dev:  0.10933610817754\n",
            "IS quartiles:  [-0.84036769 -0.77909955 -0.69050267]\n",
            "IS max:  -0.5479659789843923\n",
            "IS min:  -1.0898425977186181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xTexgYskreS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assimilation"
      ],
      "metadata": {
        "id": "PsY0dioZrpEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)])\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 1000\n"
      ],
      "metadata": {
        "id": "IflYRufTrqQe"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_policies, filename = create_policy_set(env, behavior_policy, num_episodes)"
      ],
      "metadata": {
        "id": "J-xiQHlgrrJh"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scope_set, phi_set = subset_policies(behavior_policies, 0.3)\n",
        "variance_scope, variance_is = calc_variance(phi_set,0.9,[-0.1,.1,.1], 100, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COh19GO3sKYb",
        "outputId": "d3d8e010-7337-476a-b655-911723bacd2b"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.013946532561433217\n",
            "Var IS:  0.022615461805532644\n",
            "Percent change in variance:  -38.331869225764216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_beta = [-0.1,.1,.1]\n",
        "optimal_beta = optimize_variance_scope(initial_beta, behavior_policies, 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRS-QsNHsWgw",
        "outputId": "bfc60bea-4755-4d32-c79e-08014135640d"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [-0.08392227  0.05913637  0.07349569]\n",
            "Variance Scope: 0.011642645740983157\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [-0.08203403  0.07167259  0.04989847]\n",
            "Variance Scope: 0.01141263319047077\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [-0.00940688  0.20899382 -0.17114916]\n",
            "Variance Scope: 0.010697596271026302\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [-0.04254343  0.18694986 -0.15114324]\n",
            "Variance Scope: 0.01062320694836956\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [-0.03471752  0.17893528 -0.12954731]\n",
            "Variance Scope: 0.010528931141857503\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [-0.0348804   0.17844556 -0.12868972]\n",
            "Variance Scope: 0.010528909950091495\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_beta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlw5s7NP6z0_",
        "outputId": "2f767a63-40e0-4b55-e8b7-2a6ae1dc8397"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.0348804 ,  0.17844556, -0.12868972])"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "variance_scope, variance_is = calc_variance(phi_set,0.9,optimal_beta, 100, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2OPoXh9wdQ",
        "outputId": "b83d6b2c-75d4-4d87-8b19-6066be25572f"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.010528909950091495\n",
            "Var IS:  0.022615461805532644\n",
            "Percent change in variance:  -53.44375436315122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V_per_traj, scope_std, scope_quartiles, scope_max_value, scope_min_value = SCOPE(behavior_policies,optimal_beta,0.3,300)\n",
        "print(\"SCOPE Std Dev: \", scope_std)\n",
        "print(\"SCOPE quartiles: \",scope_quartiles)\n",
        "print(\"SCOPE max: \",scope_max_value)\n",
        "print(\"SCOPE min\",scope_min_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAIgTfMKskLi",
        "outputId": "03497178-7605-415f-ecf2-8106b5f424ee"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCOPE Std Dev:  0.051621052587860865\n",
            "SCOPE quartiles:  [-0.34820468 -0.30815884 -0.27829188]\n",
            "SCOPE max:  -0.16441272079000718\n",
            "SCOPE min -0.44919755110095066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "at, IS_per_traj, is_std, is_quartiles, is_max_value, is_min_value = per_step_IS(behavior_policies,0.3,300)\n",
        "print(\"IS std dev: \",is_std)\n",
        "print(\"IS quartiles: \",is_quartiles)\n",
        "print(\"IS max: \",is_max_value)\n",
        "print(\"IS min: \", is_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXrb1n237q0R",
        "outputId": "21bf1802-38f6-4a36-e121-74f91e5cf7c7"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IS std dev:  0.05829691620819242\n",
            "IS quartiles:  [-0.83981247 -0.7984766  -0.76046578]\n",
            "IS max:  -0.6233971345116637\n",
            "IS min:  -0.9572766237675874\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "MkhpULigM6wI"
      ],
      "authorship_tag": "ABX9TyP0Y01n89NH8JngY0Iz7RHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}