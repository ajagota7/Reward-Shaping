{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajagota7/Reward-Shaping/blob/main/gridworld_ope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt3QR_86NGdz"
      },
      "source": [
        "# Creating Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "-Ww-v9JI2Hhj"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, height, width, start, end, bad_regions, good_regions):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.bad_regions = bad_regions\n",
        "        self.good_regions = good_regions\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = self.start\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_position\n",
        "\n",
        "        if action == \"up\" and y < self.height - 1:\n",
        "            y += 1\n",
        "        elif action == \"down\" and y > 0:\n",
        "            y -= 1\n",
        "        elif action == \"left\" and x > 0:\n",
        "            x -= 1\n",
        "        elif action == \"right\" and x < self.width - 1:\n",
        "            x += 1\n",
        "\n",
        "        self.agent_position = (x, y)\n",
        "\n",
        "        if self.agent_position == self.end:\n",
        "            reward = 3\n",
        "            done = True\n",
        "        elif self.agent_position in self.bad_regions:\n",
        "            reward = -1\n",
        "            done = False\n",
        "        elif self.agent_position in self.good_regions:\n",
        "            reward = 0.5\n",
        "            done = False\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return (x, y), reward, done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HQACkJ1xWBoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, epsilon=0.0):\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def select_action(self, policy_func):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            # Choose a random action\n",
        "            action = np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "        else:\n",
        "            # Use the provided policy function to get the best action\n",
        "            action = policy_func()\n",
        "        return action\n",
        "\n",
        "# Define different policy functions outside the class\n",
        "\n",
        "def random_policy():\n",
        "    # Choose a random action\n",
        "    return np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "\n",
        "def behavior_policy():\n",
        "    action_probs = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def evaluation_policy():\n",
        "    action_probs = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def manhattan_distance(pos1, pos2):\n",
        "    # Compute the Manhattan distance between two positions\n",
        "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mIpC_Kpfice"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GipigR-ZNTo6"
      },
      "source": [
        "# Generating Policy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgfscYWMW0XS",
        "outputId": "a7aef5a2-0696-457b-9714-997666bab07a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 995\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 995\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 995\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 995\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 995\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 995\n",
            "State: (0, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 995\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 995\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (1, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 995\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -15.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 995\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -15.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -15.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -16.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 995\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -16.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 995\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (0, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 995\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 995\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 995\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 995\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -17.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 995\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 995\n",
            "State: (2, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (2, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 995\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (3, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 995\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (3, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 995\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 995\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 995\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 995\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 995\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 995\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 995\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (1, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 995\n",
            "State: (1, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (0, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 995\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 995\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 995\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 995\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 995\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 995\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 995\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 995\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 995\n",
            "State: (4, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 995\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -20.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 995\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -20.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 995\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -20.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 995\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: -17.5\n",
            "Done: True\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 996\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 996\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 996\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 996\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 996\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 996\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 996\n",
            "State: (4, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 996\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 996\n",
            "State: (4, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 996\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 996\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 996\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 996\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 996\n",
            "State: (4, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (3, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 996\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (3, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 996\n",
            "State: (2, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 996\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 996\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 996\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (3, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (3, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (3, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 996\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (3, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (3, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 996\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 996\n",
            "State: (3, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 996\n",
            "State: (3, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 996\n",
            "State: (3, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 996\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 996\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 996\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 996\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 996\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 996\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 996\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: True\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 997\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 997\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 997\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 997\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 997\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 997\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 997\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 997\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 997\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 997\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 997\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 997\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 997\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 997\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 997\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 997\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 997\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: True\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 998\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 998\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 998\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (4, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (4, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (3, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (3, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (3, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (3, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (2, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 998\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 998\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 998\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 998\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 998\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (0, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (2, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (2, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (2, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 998\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 1.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 998\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 998\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (2, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (1, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 998\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 998\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 998\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (3, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (3, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 998\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 1.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (3, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 1.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 998\n",
            "State: (2, 3)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 998\n",
            "State: (2, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 998\n",
            "State: (1, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (1, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (0, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (0, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 998\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 998\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (0, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (0, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 998\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 998\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 998\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 998\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.5\n",
            "Done: True\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (3, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (3, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 999\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -4.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -5.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -5.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -6.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -6.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -7.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -7.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -7.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (0, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -7.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -7.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -7.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -8.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -8.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -8.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -8.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (3, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -8.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -8.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -8.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -8.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -9.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -9.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -10.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -10.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -10.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -10.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -10.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -10.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -11.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -11.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -12.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -12.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (0, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 999\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (4, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (3, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (3, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (4, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -12.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 999\n",
            "State: (0, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -13.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -14.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (3, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (3, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (3, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [4, 4]\n",
            "Episode: 999\n",
            "State: (4, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (3, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (3, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -13.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 999\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -14.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -15.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -15.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -16.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -16.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 999\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -16.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -16.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -17.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -17.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -17.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -17.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -17.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -17.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (1, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -17.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -17.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -18.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -18.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -19.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -19.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -20.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 0]\n",
            "Episode: 999\n",
            "State: (2, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -20.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 999\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (0, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [1, 1]\n",
            "Episode: 999\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 999\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -21.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 999\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -21.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 999\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -20.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -20.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -20.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -19.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -19.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 999\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -19.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 999\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -18.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -18.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -18.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 999\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -18.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 999\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 999\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -18.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 999\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: -15.0\n",
            "Done: True\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 1000\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 1000\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 1000\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 1000\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 1000\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 1000\n",
            "State: (0, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 1000\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 1000\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 1000\n",
            "State: (0, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [0, 2]\n",
            "Episode: 1000\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 1000\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 1000\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [6]\n",
            "Bad Region Distances:  [2, 4]\n",
            "Episode: 1000\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [5]\n",
            "Bad Region Distances:  [1, 3]\n",
            "Episode: 1000\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [4]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 1000\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 1000\n",
            "State: (0, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 1000\n",
            "State: (0, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 1000\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [2, 2]\n",
            "Episode: 1000\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [3]\n",
            "Bad Region Distances:  [3, 3]\n",
            "Episode: 1000\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 1000\n",
            "State: (2, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 1000\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 1000\n",
            "State: (3, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [3, 1]\n",
            "Episode: 1000\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 1000\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 1000\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [2]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 1000\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 1000\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 1000\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 1000\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [0]\n",
            "Bad Region Distances:  [4, 2]\n",
            "Episode: 1000\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Good Region Distances [1]\n",
            "Bad Region Distances:  [5, 3]\n",
            "Episode: 1000\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# Create the GridWorld environment\n",
        "height = 5\n",
        "width = 5\n",
        "start = (0, 0)\n",
        "end = (4, 4)\n",
        "# bad_regions = [(1, 1), (2, 2)]\n",
        "# good_regions = [(3, 3)]\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)])\n",
        "\n",
        "# Create a list to store behavior policies as trajectories\n",
        "behavior_policies = []\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 1000\n",
        "\n",
        "# Run multiple episodes\n",
        "for episode in range(num_episodes):\n",
        "    # Create a new Agent for each episode to generate a different behavior policy\n",
        "    agent = Agent(epsilon=0.0)\n",
        "\n",
        "    # Run an episode\n",
        "    env.reset()\n",
        "    done = False\n",
        "    trajectory = []  # Store the trajectory for the current episode\n",
        "    cumulative_reward = 0.0  # Initialize cumulative reward\n",
        "    while not done:\n",
        "        state = env.agent_position  # Get the current state\n",
        "        action = agent.select_action(behavior_policy)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Compute cumulative reward\n",
        "        cumulative_reward += reward\n",
        "\n",
        "        # Compute feature function values (manhattan distances)\n",
        "        good_region_distances = [manhattan_distance(state, gr) for gr in good_regions]\n",
        "        bad_region_distances = [manhattan_distance(state, br) for br in bad_regions]\n",
        "\n",
        "        print(\"Good Region Distances\",good_region_distances)\n",
        "        print(\"Bad Region Distances: \",bad_region_distances)\n",
        "\n",
        "        # Store the (state, action, reward, next_state) tuple in the trajectory\n",
        "        trajectory.append((state, action, reward, next_state, cumulative_reward, good_region_distances, bad_region_distances))\n",
        "\n",
        "        # Print the episode information\n",
        "        print(\"Episode:\", episode + 1)\n",
        "        print(\"State:\", state)\n",
        "        print(\"Action:\", action)\n",
        "        print(\"Reward:\", reward)\n",
        "        print(\"Next State:\", next_state)\n",
        "        print(\"Cumulative Reward:\", cumulative_reward)\n",
        "        print(\"Done:\", done)\n",
        "        print(\"-----\")\n",
        "\n",
        "    # Append the trajectory to the behavior policies list\n",
        "    behavior_policies.append(trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2grHZQpX9Byl",
        "outputId": "b97f81fe-6159-4ac7-e957-8a591dcbc0e9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a grid to store cumulative rewards for each state\n",
        "height = 5\n",
        "width = 5\n",
        "cumulative_rewards_grid = np.zeros((height, width))\n",
        "\n",
        "# Loop through all trajectories and accumulate cumulative rewards for each state\n",
        "for trajectory in behavior_policies:\n",
        "    for _, _, reward, next_state, cumulative_reward in trajectory:\n",
        "        x, y = next_state\n",
        "        cumulative_rewards_grid[y, x] += cumulative_reward\n",
        "\n",
        "# # Create the heatmap\n",
        "plt.imshow(cumulative_rewards_grid, cmap='viridis', origin='lower')\n",
        "\n",
        "# Add colorbar for better visualization\n",
        "plt.colorbar()\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Heatmap of Cumulative Rewards')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Calculate the differences between adjacent states\n",
        "diff_x = np.abs(np.diff(cumulative_rewards_grid, axis=1))\n",
        "diff_y = np.abs(np.diff(cumulative_rewards_grid, axis=0))\n",
        "\n",
        "# Create the heatmap for differences in x-direction\n",
        "plt.imshow(diff_x, cmap='viridis', origin='lower')\n",
        "\n",
        "# Add colorbar for better visualization\n",
        "plt.colorbar()\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Heatmap of Differences between Adjacent States (in X-direction)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Create the heatmap for differences in y-direction\n",
        "plt.imshow(diff_y, cmap='viridis', origin='lower')\n",
        "\n",
        "# Add colorbar for better visualization\n",
        "plt.colorbar()\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Heatmap of Differences between Adjacent States (in Y-direction)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFWfa7CxBxZj",
        "outputId": "f33cadf7-b1d5-49f9-a3fc-762ba176e2d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  39.5,  422.5,  183. ,  866. ],\n",
              "       [ 651. ,  993.5,  726. , 1096. ],\n",
              "       [1324.5,  664.5, 1781.5, 1085. ],\n",
              "       [  42.5,  569. , 1663. ,  835. ],\n",
              "       [ 215.5,  684.5, 1889. , 2226. ]])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diff_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dGquMSCFat",
        "outputId": "be00be77-9e44-45ca-c037-723a8abe173b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 415. , 1026.5,  455.5,   87.5,  317.5],\n",
              "       [2006. , 1332.5,  325.5,  730. ,  719. ],\n",
              "       [   5. , 1372. , 2605.5, 2487. , 2237. ],\n",
              "       [ 147. ,  320. ,  435.5,  661.5, 2052.5]])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diff_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9DgLcqD9pj5",
        "outputId": "3b5641b7-d882-46b5-cfa5-4f7915877cc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-7840. , -7879.5, -7457. , -7274. , -6408. ],\n",
              "       [-8255. , -8906. , -7912.5, -7186.5, -6090.5],\n",
              "       [-6249. , -7573.5, -8238. , -6456.5, -5371.5],\n",
              "       [-6244. , -6201.5, -5632.5, -3969.5, -3134.5],\n",
              "       [-6097. , -5881.5, -5197. , -3308. , -1082. ]])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cumulative_rewards_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZudK5NVl_vsm",
        "outputId": "0f0018f0-bc1f-4406-c492-d8b83f1b2fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Episode: 163\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 163\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 163\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 163\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 163\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 163\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 163\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 163\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 163\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 164\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 164\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 165\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 165\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 166\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (2, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 166\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 167\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (2, 1)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 167\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 168\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 169\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 169\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 170\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 170\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 171\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 172\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 172\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 173\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (2, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (2, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 173\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 5.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 174\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 175\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 175\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 176\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 176\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 177\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 177\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 178\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 178\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 179\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 180\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 180\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 181\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 182\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (0, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 182\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 183\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 183\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 184\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 184\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 185\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 185\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 186\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 186\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 187\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (2, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 187\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 188\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (3, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 188\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 189\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (2, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (4, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 189\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 190\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 4)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (0, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (0, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 190\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 191\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (3, 2)\n",
            "Action: left\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 191\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 0.5\n",
            "Done: True\n",
            "-----\n",
            "Episode: 192\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (1, 2)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (2, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (2, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 192\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 2.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (0, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (1, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (3, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 193\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 4.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 194\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (0, 1)\n",
            "Action: right\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (3, 4)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (3, 4)\n",
            "Action: down\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (3, 3)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 3)\n",
            "Cumulative Reward: -2.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (2, 3)\n",
            "Action: right\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 194\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 195\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (3, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 195\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 196\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (3, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (3, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 196\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 197\n",
            "State: (0, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (2, 0)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 197\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 3.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 198\n",
            "State: (0, 0)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (0, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (1, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (2, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (4, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (4, 3)\n",
            "Action: left\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 3)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: 0.5\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 2)\n",
            "Action: up\n",
            "Reward: 0.5\n",
            "Next State: (3, 3)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 198\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 4.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 199\n",
            "State: (0, 0)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 0)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (1, 0)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (3, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (2, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (3, 1)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (2, 2)\n",
            "Action: left\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (1, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 3)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (1, 3)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (1, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (1, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (2, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (2, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (2, 4)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (3, 4)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (3, 4)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 199\n",
            "State: (3, 4)\n",
            "Action: right\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 1.0\n",
            "Done: True\n",
            "-----\n",
            "Episode: 200\n",
            "State: (0, 0)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 1)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (0, 1)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (0, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (0, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (1, 2)\n",
            "Cumulative Reward: 0.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (1, 2)\n",
            "Action: down\n",
            "Reward: -1\n",
            "Next State: (1, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (1, 1)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -1.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (2, 2)\n",
            "Action: down\n",
            "Reward: 0\n",
            "Next State: (2, 1)\n",
            "Cumulative Reward: -2.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (2, 1)\n",
            "Action: up\n",
            "Reward: -1\n",
            "Next State: (2, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (2, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (3, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (3, 2)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 2)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (4, 2)\n",
            "Action: up\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (4, 3)\n",
            "Action: right\n",
            "Reward: 0\n",
            "Next State: (4, 3)\n",
            "Cumulative Reward: -3.0\n",
            "Done: False\n",
            "-----\n",
            "Episode: 200\n",
            "State: (4, 3)\n",
            "Action: up\n",
            "Reward: 3\n",
            "Next State: (4, 4)\n",
            "Cumulative Reward: 0.0\n",
            "Done: True\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# Gridworld for evaluation policy\n",
        "height = 5\n",
        "width = 5\n",
        "start = (0, 0)\n",
        "end = (4, 4)\n",
        "bad_regions = [(1, 1), (2, 2)]\n",
        "good_regions = [(3, 3)]\n",
        "\n",
        "env = GridWorld(height, width, start, end, bad_regions, good_regions)\n",
        "\n",
        "# Create a list to store evaluation policies as trajectories\n",
        "evaluation_policies = []\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 200\n",
        "\n",
        "# Run multiple episodes\n",
        "for episode in range(num_episodes):\n",
        "    # Create a new Agent for each episode to generate a different behavior policy\n",
        "    agent = Agent(epsilon=0.0)\n",
        "\n",
        "    # Run an episode\n",
        "    env.reset()\n",
        "    done = False\n",
        "    trajectory = []  # Store the trajectory for the current episode\n",
        "    cumulative_reward = 0.0  # Initialize cumulative reward\n",
        "    while not done:\n",
        "        state = env.agent_position  # Get the current state\n",
        "        action = agent.select_action(evaluation_policy)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Compute cumulative reward\n",
        "        cumulative_reward += reward\n",
        "\n",
        "        # Store the (state, action, reward, next_state) tuple in the trajectory\n",
        "        trajectory.append((state, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "        # Print the episode information\n",
        "        print(\"Episode:\", episode + 1)\n",
        "        print(\"State:\", state)\n",
        "        print(\"Action:\", action)\n",
        "        print(\"Reward:\", reward)\n",
        "        print(\"Next State:\", next_state)\n",
        "        print(\"Cumulative Reward:\", cumulative_reward)\n",
        "        print(\"Done:\", done)\n",
        "        print(\"-----\")\n",
        "\n",
        "    # Append the trajectory to the behavior policies list\n",
        "    evaluation_policies.append(trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a variable to store the sum of all cumulative rewards\n",
        "total_cumulative_reward = 0.0\n",
        "\n",
        "# Iterate through the evaluation_policies list to sum up the rewards\n",
        "for episode_trajectory in evaluation_policies:\n",
        "    # Get the last tuple (state, action, reward, next_state, cumulative_reward)\n",
        "    # from the trajectory to get the cumulative reward of the episode\n",
        "    cumulative_reward_episode = episode_trajectory[-1][-1]\n",
        "\n",
        "    # Add the episode's cumulative reward to the total cumulative reward\n",
        "    total_cumulative_reward += cumulative_reward_episode\n",
        "\n",
        "# Print the total cumulative reward\n",
        "print(\"Mean Cumulative Reward of 200 evaluation policies:\", total_cumulative_reward/len(evaluation_policies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0rRjUcUf4Tv",
        "outputId": "24c4fda6-aab0-4d66-b705-923af30c0818"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Cumulative Reward of 200 evaluation policies: 2.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_policies[178][-1][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ac_1NgMhaEc",
        "outputId": "95a88cf4-66d5-4d17-b957-604d57dbeab1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_reward_episode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-NhNQNfhcbD",
        "outputId": "02d998a0-4dde-4864-f595-1de9ec8d16bf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awj49skRMy4z"
      },
      "source": [
        "# OPE Calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Bqt6MgB_GD7A"
      },
      "outputs": [],
      "source": [
        "# all_weights = calculate_importance_weights(eval_policy, behav_policy,behavior_policies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnXf-tMGWREX",
        "outputId": "229887af-02a2-49a1-b35d-9a276e11f69f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-6ba03e290771>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-step IS estimates (mean): -0.1245277129215959\n",
            "Per-step IS variances: 0.0004130924942039972\n"
          ]
        }
      ],
      "source": [
        "# # Step 1: Calculate Importance Weights\n",
        "# eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "# behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "\n",
        "# # Calculate the importance weights for each trajectory in behavior_policies\n",
        "# all_weights = calculate_importance_weights(eval_policy, behav_policy, behavior_policies)\n",
        "\n",
        "# # Step 2: Bootstrap Resampling\n",
        "# num_bootstrap_samples = 100  # Number of bootstrap samples to generate\n",
        "# bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n",
        "#                      for _ in range(num_bootstrap_samples)]\n",
        "\n",
        "# # Step 3: Per-Step Importance Sampling (IS) Estimates\n",
        "# all_timesteps_list = []\n",
        "# V_all = []\n",
        "# for bootstrap_sample in bootstrap_samples:\n",
        "#     # Calculate importance weights for this bootstrap sample\n",
        "#     bootstrap_weights = calculate_importance_weights(eval_policy, behav_policy, bootstrap_sample)\n",
        "\n",
        "#     # Calculate per-step IS estimates for this bootstrap sample\n",
        "#     all_timesteps, V = per_step_IS(bootstrap_weights, bootstrap_sample)\n",
        "\n",
        "#     all_timesteps_list.append(all_timesteps)\n",
        "#     V_all.append(V)\n",
        "\n",
        "# # Step 4: Variance Estimation\n",
        "# # Calculate the variance of the per-step IS estimates for each time step\n",
        "# variances = np.var(V_all, axis=0)\n",
        "\n",
        "# # Calculate the mean of the per-step IS estimates for each time step (optional, for visualization)\n",
        "# mean_IS_estimates = np.mean(V_all, axis=0)\n",
        "\n",
        "# # Print or visualize the variances and mean IS estimates\n",
        "# print(\"Per-step IS estimates (mean):\", mean_IS_estimates)\n",
        "# print(\"Per-step IS variances:\", variances)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkhpULigM6wI"
      },
      "source": [
        "# Training Reward Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyIPQCnIBAO"
      },
      "source": [
        "## State -> Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOWRWObYB7R"
      },
      "source": [
        "Training model to predict rewards based on state only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcB-vIWj2V8r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    rewards = [state_action_reward[2] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_rewards.extend(rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_rewards = np.array(all_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42rExRZ9uReQ",
        "outputId": "c2d6f96c-d803-4cfd-dc60-ba8297af8fa2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21499"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "ZPnb1zoYE3N3",
        "outputId": "ad44bcc7-37cb-4707-b6fa-28794ab13df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d4a356955079>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m ]\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m reward_predictor.fit(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mall_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m       _, _, filtered_flat_args = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictorStates(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorStates, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_states = RewardPredictorStates(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_states.fit(\n",
        "    all_next_states, all_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KaLPE-xb2e0",
        "outputId": "230b8ee6-0a6f-46a1-adcd-45f52ac7becf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Current State: -0.03829813\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Next State: -0.006269932\n"
          ]
        }
      ],
      "source": [
        "current_state = behavior_policies[0][5][0]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])\n",
        "\n",
        "new_state = behavior_policies[0][5][3]  # Replace ... with the new state for which you want to predict the reward\n",
        "new_state = np.array(new_state, dtype=np.float32)\n",
        "predicted_reward_next = reward_predictor_states.predict(np.expand_dims(new_state, axis=0))\n",
        "print(\"Predicted Reward Next State:\", predicted_reward_next[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7rtgAXVINap"
      },
      "source": [
        "## State -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f79jQoF-tQXR"
      },
      "source": [
        "Reward model based on State -> Cumulative Rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfr9e-LRtUEj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_cum_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    cum_rewards = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_cum_rewards.extend(cum_rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_cum_rewards = np.array(all_cum_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bioPIacuJIG",
        "outputId": "b20476bb-2697-4a4d-d516-b111e0becde0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21499"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_cum_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBE4eg3xtpYa",
        "outputId": "1c27e545-c834-4869-afc3-f37535a1e35e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "2691/2691 [==============================] - 14s 4ms/step - loss: 14287.8623 - val_loss: 9193.1582\n",
            "Epoch 2/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 12563.5850 - val_loss: 7138.1450\n",
            "Epoch 3/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 10212.1973 - val_loss: 5365.0083\n",
            "Epoch 4/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 8540.7607 - val_loss: 4638.1211\n",
            "Epoch 5/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7985.4175 - val_loss: 4635.2275\n",
            "Epoch 6/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7930.9414 - val_loss: 4674.1807\n",
            "Epoch 7/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7929.3970 - val_loss: 4667.2554\n",
            "Epoch 8/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7929.0581 - val_loss: 4669.0732\n",
            "Epoch 9/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7927.9824 - val_loss: 4675.1816\n",
            "Epoch 10/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7928.5508 - val_loss: 4663.8682\n",
            "Epoch 11/1000\n",
            "2691/2691 [==============================] - 12s 5ms/step - loss: 7928.4463 - val_loss: 4675.1260\n",
            "Epoch 12/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7926.7422 - val_loss: 4675.1904\n",
            "Epoch 13/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7926.7793 - val_loss: 4658.1753\n",
            "Epoch 14/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7927.3550 - val_loss: 4671.8379\n",
            "Epoch 15/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7927.1836 - val_loss: 4697.7061\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e28c5be80a0>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 2: Design the neural network\n",
        "# import tensorflow as tf\n",
        "\n",
        "class RewardPredictorCumulative(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorCumulative, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense3 = tf.keras.layers.Dense(1, kernel_initializer='he_normal')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_cumulative = RewardPredictorCumulative(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_cumulative.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_cumulative.fit(\n",
        "    all_next_states, all_cum_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdFBS44nIUn_"
      },
      "source": [
        "## State -> Rewards over past 3 timesteps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIkOPgG07ffz"
      },
      "source": [
        "Cumulative rewards over 3 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlBb0H0NSYDz"
      },
      "outputs": [],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_behavior_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in behavior_policies\n",
        "for trajectory in behavior_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox, bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_behavior_policies list\n",
        "    augmented_behavior_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "06vVUBLO23-Y",
        "outputId": "6f39260b-3788-45b0-e890-123da0f72cb1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-85b1615e7bfe>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Iterate through each trajectory in evaluation_policies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrajectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluation_policies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnew_trajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluation_policies' is not defined"
          ]
        }
      ],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_evaluation_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in evaluation_policies\n",
        "for trajectory in evaluation_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox. bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_evaluation_policies list\n",
        "    augmented_evaluation_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pngt7A8eLoOl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "def preprocess_nstep_data(policy_data):\n",
        "  # Initialize lists to store all 'next_state' and 'reward' values\n",
        "  all_next_states = []\n",
        "  all_past3_rewards = []\n",
        "\n",
        "  # Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "  for trajectory in policy_data:\n",
        "      # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "      next_states = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "      rewards = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "\n",
        "      # Append the values to the corresponding lists\n",
        "      all_next_states.extend(next_states)\n",
        "      all_past3_rewards.extend(rewards)\n",
        "\n",
        "  # Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "  all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "  all_past3_rewards = np.array(all_past3_rewards, dtype=np.float32)\n",
        "  return all_next_states, all_past3_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCAp1fg2LUfU"
      },
      "outputs": [],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictor3States(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictor3States, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frcPgL555yen",
        "outputId": "86b058ae-b14a-478b-f0c1-2ac4d03875b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "541/541 [==============================] - 2s 2ms/step - loss: 0.1163 - val_loss: 0.0807\n",
            "Epoch 2/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0498 - val_loss: 0.0302\n",
            "Epoch 3/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0184 - val_loss: 0.0124\n",
            "Epoch 4/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0085 - val_loss: 0.0050\n",
            "Epoch 5/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0036 - val_loss: 0.0026\n",
            "Epoch 6/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 7/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 8.8721e-04 - val_loss: 6.9921e-04\n",
            "Epoch 8/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.3470e-04 - val_loss: 2.7174e-04\n",
            "Epoch 9/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.5016e-04 - val_loss: 4.5576e-04\n",
            "Epoch 10/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8726e-04 - val_loss: 3.1336e-04\n",
            "Epoch 11/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.3167e-04 - val_loss: 2.8632e-04\n",
            "Epoch 12/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.0322e-04 - val_loss: 1.9239e-04\n",
            "Epoch 13/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.2741e-04 - val_loss: 1.2284e-04\n",
            "Epoch 14/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.3022e-04 - val_loss: 2.0752e-04\n",
            "Epoch 15/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.6412e-04 - val_loss: 2.0257e-05\n",
            "Epoch 16/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.0789e-06 - val_loss: 6.2865e-07\n",
            "Epoch 17/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.2175e-05 - val_loss: 6.4133e-05\n",
            "Epoch 18/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.5053e-04 - val_loss: 6.7341e-05\n",
            "Epoch 19/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.5021e-04 - val_loss: 4.7176e-05\n",
            "Epoch 20/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8386e-04 - val_loss: 9.6860e-05\n",
            "Epoch 21/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.8277e-04 - val_loss: 1.4178e-04\n",
            "Epoch 22/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.4954e-04 - val_loss: 4.4968e-05\n",
            "Epoch 23/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.9865e-05 - val_loss: 2.5360e-04\n",
            "Epoch 24/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.9564e-04 - val_loss: 1.2803e-04\n",
            "Epoch 25/1000\n",
            "541/541 [==============================] - 1s 3ms/step - loss: 6.9617e-05 - val_loss: 2.7309e-06\n",
            "Epoch 26/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 4.5949e-04 - val_loss: 5.9128e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bc0d40ae080>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Behavior policies training\n",
        "all_next_states_behav, all_past3_rewards_behav = preprocess_nstep_data(augmented_behavior_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_behav.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states.fit(\n",
        "    all_next_states_behav, all_past3_rewards_behav,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeNydEaSIz6v",
        "outputId": "848f8b86-c1f0-4100-d492-4fd8fe3e64b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted Reward Current State: 0.0006133178\n"
          ]
        }
      ],
      "source": [
        "current_state = [2,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr0W6G3l6btP",
        "outputId": "4b7fabd9-37e5-4e01-c4c0-aab4f4476b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "83/83 [==============================] - 1s 4ms/step - loss: 0.4849 - val_loss: 0.4438\n",
            "Epoch 2/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.4098 - val_loss: 0.4183\n",
            "Epoch 3/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.3815 - val_loss: 0.4099\n",
            "Epoch 4/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.3551 - val_loss: 0.3524\n",
            "Epoch 5/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.3321 - val_loss: 0.3293\n",
            "Epoch 6/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.3087 - val_loss: 0.3227\n",
            "Epoch 7/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.2799 - val_loss: 0.2717\n",
            "Epoch 8/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.2471 - val_loss: 0.2828\n",
            "Epoch 9/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.2288 - val_loss: 0.2699\n",
            "Epoch 10/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.2065 - val_loss: 0.1874\n",
            "Epoch 11/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 0.1470\n",
            "Epoch 12/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 0.1439\n",
            "Epoch 13/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1149 - val_loss: 0.1049\n",
            "Epoch 14/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1029 - val_loss: 0.1270\n",
            "Epoch 15/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0817 - val_loss: 0.0718\n",
            "Epoch 16/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0656 - val_loss: 0.0533\n",
            "Epoch 17/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.0458\n",
            "Epoch 18/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.0345\n",
            "Epoch 19/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0252\n",
            "Epoch 20/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0217\n",
            "Epoch 21/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0163\n",
            "Epoch 22/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0117\n",
            "Epoch 23/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0116\n",
            "Epoch 24/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0071\n",
            "Epoch 25/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0065 - val_loss: 0.0076\n",
            "Epoch 26/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0048\n",
            "Epoch 27/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0034\n",
            "Epoch 28/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 29/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 30/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0029\n",
            "Epoch 31/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0027\n",
            "Epoch 32/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0019\n",
            "Epoch 33/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 34/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 35/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 36/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 37/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 38/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 9.5918e-04 - val_loss: 7.8082e-04\n",
            "Epoch 39/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 8.0509e-04 - val_loss: 6.2314e-04\n",
            "Epoch 40/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 7.8073e-04 - val_loss: 9.3644e-04\n",
            "Epoch 41/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.2917e-04 - val_loss: 7.1878e-04\n",
            "Epoch 42/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.1702e-04 - val_loss: 4.6636e-04\n",
            "Epoch 43/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 4.5697e-04 - val_loss: 3.6084e-04\n",
            "Epoch 44/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.0917e-04 - val_loss: 3.4176e-04\n",
            "Epoch 45/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.7846e-04 - val_loss: 2.6852e-04\n",
            "Epoch 46/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 2.6775e-04 - val_loss: 2.5256e-04\n",
            "Epoch 47/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 2.3555e-04 - val_loss: 2.5202e-04\n",
            "Epoch 48/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.5380e-04 - val_loss: 4.1054e-04\n",
            "Epoch 49/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 1.7520e-04\n",
            "Epoch 50/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 2.5071e-04 - val_loss: 1.2413e-04\n",
            "Epoch 51/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.2073e-04 - val_loss: 1.2467e-04\n",
            "Epoch 52/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.5401e-04 - val_loss: 1.4558e-04\n",
            "Epoch 53/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.9663e-04 - val_loss: 7.4738e-05\n",
            "Epoch 54/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.3939e-05 - val_loss: 6.0070e-05\n",
            "Epoch 55/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.0002e-04 - val_loss: 1.3382e-04\n",
            "Epoch 56/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.8231e-05 - val_loss: 4.1035e-05\n",
            "Epoch 57/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.4122e-05 - val_loss: 3.3879e-05\n",
            "Epoch 58/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 6.4845e-05 - val_loss: 3.5132e-04\n",
            "Epoch 59/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.8452e-04 - val_loss: 5.2070e-05\n",
            "Epoch 60/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.5917e-05 - val_loss: 1.6958e-04\n",
            "Epoch 61/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.8848e-04 - val_loss: 3.6782e-05\n",
            "Epoch 62/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 4.3945e-05 - val_loss: 4.2240e-05\n",
            "Epoch 63/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.6298e-04 - val_loss: 1.0759e-04\n",
            "Epoch 64/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.3274e-05 - val_loss: 1.0252e-05\n",
            "Epoch 65/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 1.9539e-05 - val_loss: 6.6084e-06\n",
            "Epoch 66/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 8.9223e-06 - val_loss: 4.6469e-06\n",
            "Epoch 67/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.2781e-06 - val_loss: 3.7274e-06\n",
            "Epoch 68/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 4.9305e-06 - val_loss: 1.5393e-06\n",
            "Epoch 69/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 4.0442e-06 - val_loss: 2.5122e-06\n",
            "Epoch 70/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.8186e-04 - val_loss: 2.5483e-04\n",
            "Epoch 71/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 9.9513e-04 - val_loss: 3.9814e-04\n",
            "Epoch 72/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 4.8291e-04 - val_loss: 3.7384e-04\n",
            "Epoch 73/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 74/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 5.3833e-04\n",
            "Epoch 75/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 1.1340e-04 - val_loss: 2.2208e-05\n",
            "Epoch 76/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.1680e-05 - val_loss: 3.0175e-06\n",
            "Epoch 77/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 2.8516e-06 - val_loss: 1.7239e-06\n",
            "Epoch 78/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.7179e-06 - val_loss: 2.9370e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bc0cd77ffa0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation policies training\n",
        "all_next_states_eval, all_past3_rewards_eval = preprocess_nstep_data(augmented_evaluation_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_eval.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states_eval = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states_eval.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states_eval.fit(\n",
        "    all_next_states_eval, all_past3_rewards_eval,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeEQ3Isp7KxB",
        "outputId": "2404e9dd-3673-47db-8793-eea611bce14b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "Predicted Reward Current State: 0.00042444468\n"
          ]
        }
      ],
      "source": [
        "current_state = [4,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states_eval.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyxbNoxzs_ah"
      },
      "source": [
        "State, action, Next State -> Reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm0AQ1adtC3-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# # Initialize lists to store all 'current_state', 'action', 'next_state', and 'reward' values\n",
        "# all_current_states = []\n",
        "# all_actions = []\n",
        "# all_next_states = []\n",
        "# all_rewards = []\n",
        "\n",
        "# # Extract the 'current_state', 'action', 'next_state', and 'reward' from the 'behavior_policies' list\n",
        "# for trajectory in behavior_policies:\n",
        "#     for state, action, reward, next_state in trajectory:\n",
        "#         all_current_states.append(state)\n",
        "#         all_actions.append(action)\n",
        "#         all_next_states.append(next_state)\n",
        "#         all_rewards.append(reward)\n",
        "\n",
        "# # Convert 'current_states', 'actions', 'next_states', and 'rewards' into appropriate formats for training\n",
        "# all_current_states = np.array(all_current_states, dtype=np.float32)\n",
        "# all_actions = np.array(all_actions, dtype=np.float32)\n",
        "# all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "# all_rewards = np.array(all_rewards, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx2TXRLpJdVV"
      },
      "source": [
        "## State + proximity to good/bad regions -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhffPOSBJpJR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oszAKQ0PJpM-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPE Calculations"
      ],
      "metadata": {
        "id": "6nNbcCLTOMxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "3ykvUcj6YAJM"
      },
      "outputs": [],
      "source": [
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "def calculate_importance_weights(eval_policy, behav_policy, behavior_policies):\n",
        "  all_weights = []\n",
        "  for trajectory in behavior_policies:\n",
        "    cum_ratio = 1\n",
        "    cumul_weights = []\n",
        "    for step in trajectory:\n",
        "        ratio = eval_policy[step[1]]/behav_policy[step[1]]\n",
        "        # print(\"Ratio:\",ratio)\n",
        "        cum_ratio *= ratio\n",
        "        cumul_weights.append(cum_ratio)\n",
        "        # print(\"Cumul:\",cum_ratio)\n",
        "    all_weights.append(cumul_weights)\n",
        "\n",
        "  return all_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "0yCj9q-qGxvf"
      },
      "outputs": [],
      "source": [
        "def per_step_IS(all_weights, behavior_policies):\n",
        "  all_timesteps = []\n",
        "  gamma = 0.9\n",
        "  for j in range(len(all_weights)):\n",
        "    Timestep_values = []\n",
        "    for i in range(len(all_weights[j])-1):\n",
        "      timestep = gamma**(i)*all_weights[j][i]*behavior_policies[j][i][2]\n",
        "      # print(\"Timestep: \",timestep)\n",
        "      Timestep_values.append(timestep)\n",
        "\n",
        "    all_timesteps.append(Timestep_values)\n",
        "  V = sum([sum(sublist) for sublist in all_timesteps])/len(all_weights)\n",
        "  return V"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SCOPE"
      ],
      "metadata": {
        "id": "DzDCmT7zm4IV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7FkplO4oFUG-"
      },
      "outputs": [],
      "source": [
        "# beta =  [ 0.24443418,  0.30068975, -0.40465262]\n",
        "def SCOPE(all_weights, behavior_policies,beta):\n",
        "  all_timesteps = []\n",
        "  gamma = 0.9\n",
        "  for j in range(len(all_weights)):\n",
        "    Timestep_values = []\n",
        "    for i in range(len(all_weights[j])-1):\n",
        "      features = behavior_policies[j][i][5] + behavior_policies[j][i][6]\n",
        "      features_next = behavior_policies[j][i+1][5] + behavior_policies[j][i+1][6]\n",
        "      timestep = gamma**(i)*all_weights[j][i]*(behavior_policies[j][i][2] + gamma*phi(features_next,beta) - phi(features,beta))\n",
        "      Timestep_values.append(timestep)\n",
        "\n",
        "    all_timesteps.append(Timestep_values)\n",
        "  V = sum([sum(sublist) for sublist in all_timesteps])/len(all_weights)\n",
        "  return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVVVkqR1hTfE"
      },
      "outputs": [],
      "source": [
        "# Variance calculations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance Terms Preparation and Calculation"
      ],
      "metadata": {
        "id": "cRLxCxKUScbp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "CfXiEdgqDE4D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def phi(features, beta):\n",
        "  features = np.array(features)\n",
        "  beta = np.array(beta)\n",
        "  phi_s = np.dot(beta,features)\n",
        "  return phi_s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "9NdwpKSkhxZR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# gamma = 0.9\n",
        "# beta = [random.random() for _ in range(3)]\n",
        "def variance_terms(policy_set,gamma, beta):\n",
        "  all_weights = calculate_importance_weights(eval_policy, behav_policy, policy_set)\n",
        "  y_w_r_all = 0\n",
        "  r_all = 0\n",
        "  f_a = 0\n",
        "  for j in range(len(policy_set)):\n",
        "    y_w_r = 0\n",
        "    r = 0\n",
        "    for i in range(len(policy_set[j])):\n",
        "      features = policy_set[j][i][5]+policy_set[j][i][6]\n",
        "      y_w_r += gamma**(i)*all_weights[j][i]*policy_set[j][i][2]\n",
        "      if i>0 & i<len(policy_set):\n",
        "        r += phi(features, beta)*(all_weights[j][i-1]-all_weights[j][i])\n",
        "    y_w_r_all += y_w_r\n",
        "    f_a +=  gamma**(len(policy_set[j]))*all_weights[j][-1]*phi(features,beta) - phi(features, beta) # fix the features part\n",
        "    r_all += r\n",
        "\n",
        "  IS = y_w_r_all/len(policy_set)\n",
        "  R = r_all/len(policy_set)\n",
        "  F = f_a/len(policy_set)\n",
        "  return IS, R, F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "6BpxUSqy7K86"
      },
      "outputs": [],
      "source": [
        "def calc_variance(behavior_policies, gamma, beta):\n",
        "  # Set the seed value (you can use any integer value)\n",
        "  seed_value = 42\n",
        "  np.random.seed(seed_value)\n",
        "  num_bootstrap_samples = 100  # Number of bootstrap samples to generate\n",
        "  bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n",
        "                     for _ in range(num_bootstrap_samples)]\n",
        "  IS_all = []\n",
        "  R_all = []\n",
        "  F_all = []\n",
        "\n",
        "  for pol in bootstrap_samples:\n",
        "    IS, R, F = variance_terms(pol,0.9,beta)\n",
        "    IS_all.append(IS)\n",
        "    R_all.append(R)\n",
        "    F_all.append(F)\n",
        "  IS_sq = np.mean([num**2 for num in IS_all])\n",
        "  IS_R_F = 2*np.mean([IS_all[i]*(R_all[i]+F_all[i]) for i in range(len(IS_all))])\n",
        "  R_sq = np.mean([num**2 for num in R_all])\n",
        "  IS_sq_all = (np.mean(IS_all))**2\n",
        "  IS_r_t_f = 2*np.mean(IS_all)*np.mean([R_all[i]+F_all[i] for i in range(len(R_all))])\n",
        "  R_sq_all = (np.mean(R_all))**2\n",
        "\n",
        "  variance_scope = IS_sq + IS_R_F + R_sq - IS_sq_all - IS_r_t_f - R_sq_all\n",
        "  variance_is = IS_sq - IS_sq_all\n",
        "  return IS_all, R_all, F_all, variance_scope, variance_is\n",
        "  # return IS_sq, IS_R_F, R_sq, IS_sq_all, IS_r_t_f, R_sq_all"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_all, R_all, F_all, variance_scope, variance_is = calc_variance(behavior_policies,0.9,[-0.1,.1,.1])\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74yBmg6_edJv",
        "outputId": "309d3077-f4bd-4a5d-8de5-da8b698a43d6"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-22418df8b347>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.025360229675781828\n",
            "Var IS:  0.029827248473569323\n",
            "Percent change in variance:  14.976301960088051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpBk8OZDtYHD",
        "outputId": "ad07107a-7ccd-4e62-bdb3-397233635a0f"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percent change in variance:  -14.976301960088051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_all, R_all, F_all, variance_scope, variance_is = calc_variance(behavior_policies,0.9,[ 0.2610704,   0.30396575, -0.43850237])\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdiGVoz2ZaFp",
        "outputId": "8cde622d-45c6-44fd-bd44-911d332f3f7d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-22418df8b347>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBEakxZIetSY",
        "outputId": "33de422d-a2c8-41c6-8649-c9b757a8a08e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.015075898161348182\n",
            "Var IS:  0.029827248473569323\n",
            "improvement:  49.45595409275745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UefNOC1GC8Wm"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function to minimize variance_scope\n",
        "def objective_function(beta):\n",
        "    IS_all, R_all, F_all, variance_scope, variance_is = calc_variance(behavior_policies, 0.9, beta)\n",
        "    return variance_scope\n",
        "\n",
        "# Set the initial values of beta\n",
        "initial_beta = np.array([ 0.2610704,   0.30396575, -0.43850237])\n",
        "\n",
        "# Lists to store beta and variance_scope values at each iteration\n",
        "all_betas = []\n",
        "all_variance_scopes = []\n",
        "\n",
        "# Callback function to record beta and variance_scope values at each iteration\n",
        "def callback_function(beta):\n",
        "    all_betas.append(beta.copy())\n",
        "    variance_scope = objective_function(beta)\n",
        "    all_variance_scopes.append(variance_scope)\n",
        "    print(\"Iteration:\", len(all_betas))\n",
        "    print(\"Beta:\", beta)\n",
        "    print(\"Variance Scope:\", variance_scope)\n",
        "    print(\"----------\")\n",
        "\n",
        "# Run the optimization with the callback\n",
        "result = minimize(objective_function, initial_beta, method='L-BFGS-B', callback=callback_function)\n",
        "\n",
        "# Extract the optimal beta values\n",
        "optimal_beta = result.x\n",
        "\n",
        "# Print the result\n",
        "print(\"Optimal Beta Values:\", optimal_beta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws00tRw1KZnN",
        "outputId": "702564c1-9348-4439-dd35-416276711c6a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-22418df8b347>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [ 0.26288465  0.31249517 -0.43308314]\n",
            "Variance Scope: 0.03890192180671534\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.25760065  0.3139119  -0.4324235 ]\n",
            "Variance Scope: 0.038879962562817755\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.25099805  0.31446644 -0.42970297]\n",
            "Variance Scope: 0.03886889032715903\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.24972093  0.31338761 -0.42732695]\n",
            "Variance Scope: 0.038866587917230196\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.24414824  0.30446678 -0.41038807]\n",
            "Variance Scope: 0.0388566913198793\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.24427272  0.30086003 -0.40483056]\n",
            "Variance Scope: 0.038855107068841224\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.24443418  0.30068975 -0.40465262]\n",
            "Variance Scope: 0.03885509432490412\n",
            "----------\n",
            "Optimal Beta Values: [ 0.24443418  0.30068975 -0.40465262]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function to minimize variance_scope\n",
        "def objective_function(beta):\n",
        "    IS_all, R_all, F_all, variance_scope, variance_is = calc_variance(behavior_policies, 0.9, beta)\n",
        "    return variance_scope\n",
        "\n",
        "# Set the initial values of beta\n",
        "initial_beta = np.array([1, -0.1, -0.1])\n",
        "\n",
        "# Lists to store beta and variance_scope values at each iteration\n",
        "all_betas = []\n",
        "all_variance_scopes = []\n",
        "\n",
        "# Callback function to record beta and variance_scope values at each iteration\n",
        "def callback_function(beta):\n",
        "    all_betas.append(beta.copy())\n",
        "    variance_scope = objective_function(beta)\n",
        "    all_variance_scopes.append(variance_scope)\n",
        "    print(\"Iteration:\", len(all_betas))\n",
        "    print(\"Beta:\", beta)\n",
        "    print(\"Variance Scope:\", variance_scope)\n",
        "    print(\"----------\")\n",
        "\n",
        "# Run the optimization with the callback\n",
        "result = minimize(objective_function, initial_beta, method='L-BFGS-B', callback=callback_function)\n",
        "\n",
        "# Extract the optimal beta values\n",
        "optimal_beta = result.x\n",
        "\n",
        "# Print the result\n",
        "print(\"Optimal Beta Values:\", optimal_beta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u23aNe_r1yat",
        "outputId": "a9887235-6587-421e-bb4d-4280a298bdab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-b436dd9eb8ff>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [ 0.74558738 -0.0727051  -0.13429517]\n",
            "Variance Scope: 0.17205980293372852\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.51422013  0.01444041 -0.13118779]\n",
            "Variance Scope: 0.10132221718839504\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.21974179  0.12674824 -0.12945906]\n",
            "Variance Scope: 0.06890789685172624\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.22100604  0.15532669 -0.17596816]\n",
            "Variance Scope: 0.06790719356206737\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.2578635   0.30333719 -0.43552043]\n",
            "Variance Scope: 0.06528770052678368\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.26104719  0.30397559 -0.43850381]\n",
            "Variance Scope: 0.0652844236998165\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.2610704   0.30396575 -0.43850237]\n",
            "Variance Scope: 0.06528442349549637\n",
            "----------\n",
            "Optimal Beta Values: [ 0.2610704   0.30396575 -0.43850237]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function to minimize variance_scope\n",
        "def objective_function(beta):\n",
        "    IS_all, R_all, F_all, variance_scope, variance_is = calc_variance(behavior_policies, 0.9, beta)\n",
        "    return variance_scope\n",
        "\n",
        "# Set the initial values of beta\n",
        "initial_beta = np.array([1, -0.1, -0.1])\n",
        "\n",
        "# Lists to store beta and variance_scope values at each iteration\n",
        "all_betas = []\n",
        "all_variance_scopes = []\n",
        "\n",
        "# Callback function to record beta and variance_scope values at each iteration\n",
        "def callback_function(beta):\n",
        "    all_betas.append(beta.copy())\n",
        "    variance_scope = objective_function(beta)\n",
        "    all_variance_scopes.append(variance_scope)\n",
        "    print(\"Iteration:\", len(all_betas))\n",
        "    print(\"Beta:\", beta)\n",
        "    print(\"Variance Scope:\", variance_scope)\n",
        "    print(\"----------\")\n",
        "\n",
        "# Run the optimization with the callback\n",
        "result = minimize(objective_function, initial_beta, method='L-BFGS-B', callback=callback_function)\n",
        "\n",
        "# Extract the optimal beta values\n",
        "optimal_beta = result.x\n",
        "\n",
        "# Print the result\n",
        "print(\"Optimal Beta Values:\", optimal_beta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUHzR5PDLOcS",
        "outputId": "33bf3173-2ae3-413c-e084-6795bbf4b4ba"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-80-b436dd9eb8ff>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(behavior_policies, size=len(behavior_policies), replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [ 0.95461424 -0.18038518 -0.14714184]\n",
            "Variance Scope: 0.11818026023735667\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.9016154  -0.156789   -0.15068515]\n",
            "Variance Scope: 0.10796090614265882\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.46725172  0.10210465 -0.29978589]\n",
            "Variance Scope: 0.06275082447768895\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.44485279  0.171732   -0.410621  ]\n",
            "Variance Scope: 0.05877067174584359\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.46031673  0.3296736  -0.71168708]\n",
            "Variance Scope: 0.05390185593164576\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.46135818  0.32966832 -0.7124576 ]\n",
            "Variance Scope: 0.05390157377166176\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.46138779  0.32965123 -0.71244844]\n",
            "Variance Scope: 0.05390157358436144\n",
            "----------\n",
            "Optimal Beta Values: [ 0.46138779  0.32965123 -0.71244844]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bb6VtUv4loLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value estimates of IS and SCOPE estimators"
      ],
      "metadata": {
        "id": "LUVgi1GMloz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_weights = calculate_importance_weights(eval_policy, behav_policy, behavior_policies)"
      ],
      "metadata": {
        "id": "1XT65vAXLtaZ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta = [-1,1,1]\n",
        "# beta =  [ 0.24443418,  0.30068975, -0.40465262]\n",
        "V_SCOPE = SCOPE(all_weights, behavior_policies,beta)\n",
        "print(\"SCOPE values estimate: %f and variance: %f\" % (V_SCOPE,variance_scope))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Eje3uw7L1wk",
        "outputId": "b86685e2-e955-4ad2-94c7-235490306164"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCOPE values estimate: 1.024355 and variance: 0.015076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V = per_step_IS(all_weights, behavior_policies)\n",
        "print(\"IS values estimate: %f and variance: %f\" % (V,variance_is))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKHQjsGNMI8C",
        "outputId": "08798130-3bc6-4d2a-c9a7-417165258c24"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IS values estimate: -0.735629 and variance: 0.029827\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOMQofdMSKzaZjVAv7JOf34",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}