{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajagota7/Reward-Shaping/blob/main/gridworld_ope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt3QR_86NGdz"
      },
      "source": [
        "# Creating Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-Ww-v9JI2Hhj"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, height, width, start, end, bad_regions, good_regions):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.bad_regions = bad_regions\n",
        "        self.good_regions = good_regions\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = self.start\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_position\n",
        "\n",
        "        if action == \"up\" and y < self.height - 1:\n",
        "            y += 1\n",
        "        elif action == \"down\" and y > 0:\n",
        "            y -= 1\n",
        "        elif action == \"left\" and x > 0:\n",
        "            x -= 1\n",
        "        elif action == \"right\" and x < self.width - 1:\n",
        "            x += 1\n",
        "\n",
        "        self.agent_position = (x, y)\n",
        "\n",
        "        if self.agent_position == self.end:\n",
        "            reward = 3\n",
        "            done = True\n",
        "        elif self.agent_position in self.bad_regions:\n",
        "            reward = -1\n",
        "            done = False\n",
        "        elif self.agent_position in self.good_regions:\n",
        "            reward = 0.5\n",
        "            done = False\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return (x, y), reward, done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HQACkJ1xWBoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, epsilon=0.0):\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def select_action(self, policy_func):\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            # Choose a random action\n",
        "            action = np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "        else:\n",
        "            # Use the provided policy function to get the best action\n",
        "            action = policy_func()\n",
        "        return action\n",
        "\n",
        "# Define different policy functions outside the class\n",
        "\n",
        "def random_policy():\n",
        "    # Choose a random action\n",
        "    return np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
        "\n",
        "def behavior_policy():\n",
        "    action_probs = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def evaluation_policy():\n",
        "    action_probs = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
        "\n",
        "def manhattan_distance(pos1, pos2):\n",
        "    # Compute the Manhattan distance between two positions\n",
        "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GipigR-ZNTo6"
      },
      "source": [
        "# Generating Policy data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gridworld environment\n",
        "height = 5\n",
        "width  = 5\n",
        "start = (0,0)\n",
        "end = (4,4)"
      ],
      "metadata": {
        "id": "5ActZ0YInJCE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IgfscYWMW0XS"
      },
      "outputs": [],
      "source": [
        "\n",
        "env = GridWorld(height, width, start, end, [(1, 1), (2, 2)], [(3,3)])\n",
        "\n",
        "# Number of episodes\n",
        "num_episodes = 200\n",
        "\n",
        "def create_policy_set(env, policy, num_episodes):\n",
        "  # Create a list to store policies as trajectories\n",
        "  policies = []\n",
        "\n",
        "  # Run multiple episodes\n",
        "  for episode in range(num_episodes):\n",
        "      # Create a new Agent for each episode to generate a different policy\n",
        "      agent = Agent(epsilon=0.0)\n",
        "\n",
        "      # Run an episode\n",
        "      env.reset()\n",
        "      done = False\n",
        "      trajectory = []  # Store the trajectory for the current episode\n",
        "      cumulative_reward = 0.0  # Initialize cumulative reward\n",
        "      while not done:\n",
        "          state = env.agent_position  # Get the current state\n",
        "          action = agent.select_action(policy)\n",
        "          next_state, reward, done = env.step(action)\n",
        "\n",
        "          # Compute cumulative reward\n",
        "          cumulative_reward += reward\n",
        "\n",
        "          # Compute feature function values (manhattan distances)\n",
        "          good_region_distances = [manhattan_distance(state, gr) for gr in env.good_regions]\n",
        "          bad_region_distances = [manhattan_distance(state, br) for br in env.bad_regions]\n",
        "\n",
        "          # Store the (state, action, reward, next_state) tuple in the trajectory\n",
        "          trajectory.append((state, action, reward, next_state, cumulative_reward, good_region_distances, bad_region_distances))\n",
        "\n",
        "      # Append the trajectory to the policies list\n",
        "      policies.append(trajectory)\n",
        "\n",
        "  good_regions_str = \"_\".join([f\"gr_{pos[0]}_{pos[1]}\" for pos in env.good_regions])\n",
        "  bad_regions_str = \"_\".join([f\"br_{pos[0]}_{pos[1]}\" for pos in env.bad_regions])\n",
        "  filename = f\"{env.__class__.__name__}_policy_{policy.__name__}_{good_regions_str}_{bad_regions_str}.txt\"\n",
        "\n",
        "  return policies, filename\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_V_pi_e(evaluation_policies):\n",
        "\n",
        "  total_cumulative_reward = 0.0\n",
        "\n",
        "  for episode_trajectory in evaluation_policies:\n",
        "      total_cumulative_reward += episode_trajectory[-1][4]\n",
        "\n",
        "  return total_cumulative_reward/len(evaluation_policies)"
      ],
      "metadata": {
        "id": "v0rRjUcUf4Tv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_policies, a = create_policy_set(env, behavior_policy, 200)"
      ],
      "metadata": {
        "id": "_ZzGc9yegxaW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V05T0HkwSaZA",
        "outputId": "1507da16-1e5f-44b6-abd9-7d960be4f802"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GridWorld_policy_behavior_policy_gr_3_3_br_1_1_br_2_2.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_policies = create_policy_set(env, evaluation_policy, 1000)\n",
        "V_pi_e = calc_V_pi_e(evaluation_policies)"
      ],
      "metadata": {
        "id": "ZRFZ42OyMht9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and Loading Data"
      ],
      "metadata": {
        "id": "3UdUjHYmPsZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def save_policies_to_file(policies, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(policies, file)\n",
        "\n",
        "def load_policies_from_file(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        policies = pickle.load(file)\n",
        "    return policies\n"
      ],
      "metadata": {
        "id": "CfI_8J51Pwer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkhpULigM6wI"
      },
      "source": [
        "# Training Reward Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyIPQCnIBAO"
      },
      "source": [
        "## State -> Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOWRWObYB7R"
      },
      "source": [
        "Training model to predict rewards based on state only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcB-vIWj2V8r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    rewards = [state_action_reward[2] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_rewards.extend(rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_rewards = np.array(all_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42rExRZ9uReQ",
        "outputId": "c2d6f96c-d803-4cfd-dc60-ba8297af8fa2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21499"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "ZPnb1zoYE3N3",
        "outputId": "ad44bcc7-37cb-4707-b6fa-28794ab13df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d4a356955079>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m ]\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m reward_predictor.fit(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mall_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m       _, _, filtered_flat_args = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictorStates(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorStates, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_states = RewardPredictorStates(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_states.fit(\n",
        "    all_next_states, all_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KaLPE-xb2e0",
        "outputId": "230b8ee6-0a6f-46a1-adcd-45f52ac7becf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Current State: -0.03829813\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted Reward Next State: -0.006269932\n"
          ]
        }
      ],
      "source": [
        "current_state = behavior_policies[0][5][0]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])\n",
        "\n",
        "new_state = behavior_policies[0][5][3]  # Replace ... with the new state for which you want to predict the reward\n",
        "new_state = np.array(new_state, dtype=np.float32)\n",
        "predicted_reward_next = reward_predictor_states.predict(np.expand_dims(new_state, axis=0))\n",
        "print(\"Predicted Reward Next State:\", predicted_reward_next[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7rtgAXVINap"
      },
      "source": [
        "## State -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f79jQoF-tQXR"
      },
      "source": [
        "Reward model based on State -> Cumulative Rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfr9e-LRtUEj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# Initialize lists to store all 'next_state' and 'reward' values\n",
        "all_next_states = []\n",
        "all_cum_rewards = []\n",
        "\n",
        "# Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "for trajectory in behavior_policies:\n",
        "    # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "    next_states = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "    cum_rewards = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "\n",
        "    # Append the values to the corresponding lists\n",
        "    all_next_states.extend(next_states)\n",
        "    all_cum_rewards.extend(cum_rewards)\n",
        "\n",
        "# Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "all_cum_rewards = np.array(all_cum_rewards, dtype=np.float32)\n",
        "\n",
        "# Now, 'all_next_states' contains all the 'next_state' values, and 'all_rewards' contains all the corresponding rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bioPIacuJIG",
        "outputId": "b20476bb-2697-4a4d-d516-b111e0becde0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21499"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_cum_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBE4eg3xtpYa",
        "outputId": "1c27e545-c834-4869-afc3-f37535a1e35e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "2691/2691 [==============================] - 14s 4ms/step - loss: 14287.8623 - val_loss: 9193.1582\n",
            "Epoch 2/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 12563.5850 - val_loss: 7138.1450\n",
            "Epoch 3/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 10212.1973 - val_loss: 5365.0083\n",
            "Epoch 4/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 8540.7607 - val_loss: 4638.1211\n",
            "Epoch 5/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7985.4175 - val_loss: 4635.2275\n",
            "Epoch 6/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7930.9414 - val_loss: 4674.1807\n",
            "Epoch 7/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7929.3970 - val_loss: 4667.2554\n",
            "Epoch 8/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7929.0581 - val_loss: 4669.0732\n",
            "Epoch 9/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7927.9824 - val_loss: 4675.1816\n",
            "Epoch 10/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7928.5508 - val_loss: 4663.8682\n",
            "Epoch 11/1000\n",
            "2691/2691 [==============================] - 12s 5ms/step - loss: 7928.4463 - val_loss: 4675.1260\n",
            "Epoch 12/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7926.7422 - val_loss: 4675.1904\n",
            "Epoch 13/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7926.7793 - val_loss: 4658.1753\n",
            "Epoch 14/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7927.3550 - val_loss: 4671.8379\n",
            "Epoch 15/1000\n",
            "2691/2691 [==============================] - 12s 4ms/step - loss: 7927.1836 - val_loss: 4697.7061\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e28c5be80a0>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 2: Design the neural network\n",
        "# import tensorflow as tf\n",
        "\n",
        "class RewardPredictorCumulative(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictorCumulative, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense3 = tf.keras.layers.Dense(1, kernel_initializer='he_normal')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_cumulative = RewardPredictorCumulative(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_cumulative.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_cumulative.fit(\n",
        "    all_next_states, all_cum_rewards,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdFBS44nIUn_"
      },
      "source": [
        "## State -> Rewards over past 3 timesteps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIkOPgG07ffz"
      },
      "source": [
        "Cumulative rewards over 3 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlBb0H0NSYDz"
      },
      "outputs": [],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_behavior_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in behavior_policies\n",
        "for trajectory in behavior_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox, bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_behavior_policies list\n",
        "    augmented_behavior_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "06vVUBLO23-Y",
        "outputId": "6f39260b-3788-45b0-e890-123da0f72cb1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-85b1615e7bfe>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Iterate through each trajectory in evaluation_policies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrajectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevaluation_policies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnew_trajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluation_policies' is not defined"
          ]
        }
      ],
      "source": [
        "# Discounted sum\n",
        "# Create a new list to store trajectories with the new data\n",
        "augmented_evaluation_policies = []\n",
        "\n",
        "# Set the discount factor (gamma)\n",
        "discount_factor = 0.9  # You can adjust this value as needed (usually between 0 and 1)\n",
        "\n",
        "# Iterate through each trajectory in evaluation_policies\n",
        "for trajectory in evaluation_policies:\n",
        "    num_timesteps = len(trajectory)\n",
        "    new_trajectory = []\n",
        "\n",
        "    # Iterate through each timestep in the trajectory\n",
        "    for t in range(num_timesteps):\n",
        "        # Calculate the discounted sum of the past 3 rewards for the past 3 timesteps\n",
        "        discounted_sum = 0.0\n",
        "        for i in range(1, min(4, t + 1)):\n",
        "            discounted_sum += (discount_factor ** (i - 1)) * trajectory[t - i][2]\n",
        "\n",
        "        # Update the trajectory to include only the discounted sum of the past 3 rewards\n",
        "        state, action, reward, next_state, cumulative_reward, good_prox. bad_prox = trajectory[t]\n",
        "        new_trajectory.append((state, discounted_sum, action, reward, next_state, cumulative_reward))\n",
        "\n",
        "    # Append the modified trajectory to the augmented_evaluation_policies list\n",
        "    augmented_evaluation_policies.append(new_trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pngt7A8eLoOl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "def preprocess_nstep_data(policy_data):\n",
        "  # Initialize lists to store all 'next_state' and 'reward' values\n",
        "  all_next_states = []\n",
        "  all_past3_rewards = []\n",
        "\n",
        "  # Extract the 'next_state' and 'reward' from the 'behavior_policies' list\n",
        "  for trajectory in policy_data:\n",
        "      # For each trajectory, extract all 'next_state' and 'reward' values\n",
        "      next_states = [state_action_reward[4] for state_action_reward in trajectory]\n",
        "      rewards = [state_action_reward[3] for state_action_reward in trajectory]\n",
        "\n",
        "      # Append the values to the corresponding lists\n",
        "      all_next_states.extend(next_states)\n",
        "      all_past3_rewards.extend(rewards)\n",
        "\n",
        "  # Convert 'next_states' and 'rewards' into appropriate formats for training\n",
        "  all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "  all_past3_rewards = np.array(all_past3_rewards, dtype=np.float32)\n",
        "  return all_next_states, all_past3_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCAp1fg2LUfU"
      },
      "outputs": [],
      "source": [
        "# Step 2: Design the neural network\n",
        "class RewardPredictor3States(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super(RewardPredictor3States, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frcPgL555yen",
        "outputId": "86b058ae-b14a-478b-f0c1-2ac4d03875b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "541/541 [==============================] - 2s 2ms/step - loss: 0.1163 - val_loss: 0.0807\n",
            "Epoch 2/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0498 - val_loss: 0.0302\n",
            "Epoch 3/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0184 - val_loss: 0.0124\n",
            "Epoch 4/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0085 - val_loss: 0.0050\n",
            "Epoch 5/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0036 - val_loss: 0.0026\n",
            "Epoch 6/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 7/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 8.8721e-04 - val_loss: 6.9921e-04\n",
            "Epoch 8/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.3470e-04 - val_loss: 2.7174e-04\n",
            "Epoch 9/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.5016e-04 - val_loss: 4.5576e-04\n",
            "Epoch 10/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8726e-04 - val_loss: 3.1336e-04\n",
            "Epoch 11/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.3167e-04 - val_loss: 2.8632e-04\n",
            "Epoch 12/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.0322e-04 - val_loss: 1.9239e-04\n",
            "Epoch 13/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.2741e-04 - val_loss: 1.2284e-04\n",
            "Epoch 14/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.3022e-04 - val_loss: 2.0752e-04\n",
            "Epoch 15/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.6412e-04 - val_loss: 2.0257e-05\n",
            "Epoch 16/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 5.0789e-06 - val_loss: 6.2865e-07\n",
            "Epoch 17/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 7.2175e-05 - val_loss: 6.4133e-05\n",
            "Epoch 18/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.5053e-04 - val_loss: 6.7341e-05\n",
            "Epoch 19/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.5021e-04 - val_loss: 4.7176e-05\n",
            "Epoch 20/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.8386e-04 - val_loss: 9.6860e-05\n",
            "Epoch 21/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 1.8277e-04 - val_loss: 1.4178e-04\n",
            "Epoch 22/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 3.4954e-04 - val_loss: 4.4968e-05\n",
            "Epoch 23/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 6.9865e-05 - val_loss: 2.5360e-04\n",
            "Epoch 24/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 2.9564e-04 - val_loss: 1.2803e-04\n",
            "Epoch 25/1000\n",
            "541/541 [==============================] - 1s 3ms/step - loss: 6.9617e-05 - val_loss: 2.7309e-06\n",
            "Epoch 26/1000\n",
            "541/541 [==============================] - 1s 2ms/step - loss: 4.5949e-04 - val_loss: 5.9128e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bc0d40ae080>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Behavior policies training\n",
        "all_next_states_behav, all_past3_rewards_behav = preprocess_nstep_data(augmented_behavior_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_behav.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states.fit(\n",
        "    all_next_states_behav, all_past3_rewards_behav,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeNydEaSIz6v",
        "outputId": "848f8b86-c1f0-4100-d492-4fd8fe3e64b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted Reward Current State: 0.0006133178\n"
          ]
        }
      ],
      "source": [
        "current_state = [2,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr0W6G3l6btP",
        "outputId": "4b7fabd9-37e5-4e01-c4c0-aab4f4476b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "83/83 [==============================] - 1s 4ms/step - loss: 0.4849 - val_loss: 0.4438\n",
            "Epoch 2/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.4098 - val_loss: 0.4183\n",
            "Epoch 3/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.3815 - val_loss: 0.4099\n",
            "Epoch 4/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.3551 - val_loss: 0.3524\n",
            "Epoch 5/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.3321 - val_loss: 0.3293\n",
            "Epoch 6/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.3087 - val_loss: 0.3227\n",
            "Epoch 7/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.2799 - val_loss: 0.2717\n",
            "Epoch 8/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.2471 - val_loss: 0.2828\n",
            "Epoch 9/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.2288 - val_loss: 0.2699\n",
            "Epoch 10/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.2065 - val_loss: 0.1874\n",
            "Epoch 11/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 0.1470\n",
            "Epoch 12/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1344 - val_loss: 0.1439\n",
            "Epoch 13/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1149 - val_loss: 0.1049\n",
            "Epoch 14/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.1029 - val_loss: 0.1270\n",
            "Epoch 15/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0817 - val_loss: 0.0718\n",
            "Epoch 16/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0656 - val_loss: 0.0533\n",
            "Epoch 17/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.0458\n",
            "Epoch 18/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.0345\n",
            "Epoch 19/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0252\n",
            "Epoch 20/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0217\n",
            "Epoch 21/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0202 - val_loss: 0.0163\n",
            "Epoch 22/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0117\n",
            "Epoch 23/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0116\n",
            "Epoch 24/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0071\n",
            "Epoch 25/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0065 - val_loss: 0.0076\n",
            "Epoch 26/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0048\n",
            "Epoch 27/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0034\n",
            "Epoch 28/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0030\n",
            "Epoch 29/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 30/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.0029\n",
            "Epoch 31/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0027\n",
            "Epoch 32/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0019\n",
            "Epoch 33/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 34/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 35/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 36/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 37/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 38/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 9.5918e-04 - val_loss: 7.8082e-04\n",
            "Epoch 39/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 8.0509e-04 - val_loss: 6.2314e-04\n",
            "Epoch 40/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 7.8073e-04 - val_loss: 9.3644e-04\n",
            "Epoch 41/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.2917e-04 - val_loss: 7.1878e-04\n",
            "Epoch 42/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.1702e-04 - val_loss: 4.6636e-04\n",
            "Epoch 43/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 4.5697e-04 - val_loss: 3.6084e-04\n",
            "Epoch 44/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.0917e-04 - val_loss: 3.4176e-04\n",
            "Epoch 45/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.7846e-04 - val_loss: 2.6852e-04\n",
            "Epoch 46/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 2.6775e-04 - val_loss: 2.5256e-04\n",
            "Epoch 47/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 2.3555e-04 - val_loss: 2.5202e-04\n",
            "Epoch 48/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.5380e-04 - val_loss: 4.1054e-04\n",
            "Epoch 49/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 1.7520e-04\n",
            "Epoch 50/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 2.5071e-04 - val_loss: 1.2413e-04\n",
            "Epoch 51/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.2073e-04 - val_loss: 1.2467e-04\n",
            "Epoch 52/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.5401e-04 - val_loss: 1.4558e-04\n",
            "Epoch 53/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.9663e-04 - val_loss: 7.4738e-05\n",
            "Epoch 54/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.3939e-05 - val_loss: 6.0070e-05\n",
            "Epoch 55/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.0002e-04 - val_loss: 1.3382e-04\n",
            "Epoch 56/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.8231e-05 - val_loss: 4.1035e-05\n",
            "Epoch 57/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.4122e-05 - val_loss: 3.3879e-05\n",
            "Epoch 58/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 6.4845e-05 - val_loss: 3.5132e-04\n",
            "Epoch 59/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.8452e-04 - val_loss: 5.2070e-05\n",
            "Epoch 60/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 7.5917e-05 - val_loss: 1.6958e-04\n",
            "Epoch 61/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.8848e-04 - val_loss: 3.6782e-05\n",
            "Epoch 62/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 4.3945e-05 - val_loss: 4.2240e-05\n",
            "Epoch 63/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.6298e-04 - val_loss: 1.0759e-04\n",
            "Epoch 64/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.3274e-05 - val_loss: 1.0252e-05\n",
            "Epoch 65/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 1.9539e-05 - val_loss: 6.6084e-06\n",
            "Epoch 66/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 8.9223e-06 - val_loss: 4.6469e-06\n",
            "Epoch 67/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 5.2781e-06 - val_loss: 3.7274e-06\n",
            "Epoch 68/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 4.9305e-06 - val_loss: 1.5393e-06\n",
            "Epoch 69/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 4.0442e-06 - val_loss: 2.5122e-06\n",
            "Epoch 70/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.8186e-04 - val_loss: 2.5483e-04\n",
            "Epoch 71/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 9.9513e-04 - val_loss: 3.9814e-04\n",
            "Epoch 72/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 4.8291e-04 - val_loss: 3.7384e-04\n",
            "Epoch 73/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.0026\n",
            "Epoch 74/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 5.3833e-04\n",
            "Epoch 75/1000\n",
            "83/83 [==============================] - 0s 3ms/step - loss: 1.1340e-04 - val_loss: 2.2208e-05\n",
            "Epoch 76/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 1.1680e-05 - val_loss: 3.0175e-06\n",
            "Epoch 77/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 2.8516e-06 - val_loss: 1.7239e-06\n",
            "Epoch 78/1000\n",
            "83/83 [==============================] - 0s 2ms/step - loss: 3.7179e-06 - val_loss: 2.9370e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bc0cd77ffa0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation policies training\n",
        "all_next_states_eval, all_past3_rewards_eval = preprocess_nstep_data(augmented_evaluation_policies)\n",
        "\n",
        "# Define hyperparameters for the neural network\n",
        "input_shape = all_next_states_eval.shape[1:]  # Shape of the input state (excluding batch size)\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "# Create the neural network\n",
        "reward_predictor_3states_eval = RewardPredictor3States(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "reward_predictor_3states_eval.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                         loss='mean_squared_error')\n",
        "\n",
        "# Step 3: Train the neural network with early stopping\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "]\n",
        "\n",
        "reward_predictor_3states_eval.fit(\n",
        "    all_next_states_eval, all_past3_rewards_eval,\n",
        "    batch_size=batch_size, epochs=num_epochs,\n",
        "    callbacks=callbacks, validation_split=0.2, verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeEQ3Isp7KxB",
        "outputId": "2404e9dd-3673-47db-8793-eea611bce14b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "Predicted Reward Current State: 0.00042444468\n"
          ]
        }
      ],
      "source": [
        "current_state = [4,3]\n",
        "current_state = np.array(current_state, dtype=np.float32)\n",
        "predicted_reward_current = reward_predictor_3states_eval.predict(np.expand_dims(current_state, axis=0))\n",
        "print(\"Predicted Reward Current State:\", predicted_reward_current[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyxbNoxzs_ah"
      },
      "source": [
        "State, action, Next State -> Reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm0AQ1adtC3-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Prepare the data\n",
        "# behavior_policies = [...]  # Replace [...] with your actual behavior_policies list\n",
        "\n",
        "# # Initialize lists to store all 'current_state', 'action', 'next_state', and 'reward' values\n",
        "# all_current_states = []\n",
        "# all_actions = []\n",
        "# all_next_states = []\n",
        "# all_rewards = []\n",
        "\n",
        "# # Extract the 'current_state', 'action', 'next_state', and 'reward' from the 'behavior_policies' list\n",
        "# for trajectory in behavior_policies:\n",
        "#     for state, action, reward, next_state in trajectory:\n",
        "#         all_current_states.append(state)\n",
        "#         all_actions.append(action)\n",
        "#         all_next_states.append(next_state)\n",
        "#         all_rewards.append(reward)\n",
        "\n",
        "# # Convert 'current_states', 'actions', 'next_states', and 'rewards' into appropriate formats for training\n",
        "# all_current_states = np.array(all_current_states, dtype=np.float32)\n",
        "# all_actions = np.array(all_actions, dtype=np.float32)\n",
        "# all_next_states = np.array(all_next_states, dtype=np.float32)\n",
        "# all_rewards = np.array(all_rewards, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx2TXRLpJdVV"
      },
      "source": [
        "## State + proximity to good/bad regions -> Cumulative Reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhffPOSBJpJR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oszAKQ0PJpM-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPE Calculations"
      ],
      "metadata": {
        "id": "6nNbcCLTOMxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance Weights"
      ],
      "metadata": {
        "id": "608qtLdqhbOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3ykvUcj6YAJM"
      },
      "outputs": [],
      "source": [
        "eval_policy = {\"up\": 0.4, \"down\": 0.1, \"left\": 0.1, \"right\": 0.4}\n",
        "behav_policy = {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25}\n",
        "def calculate_importance_weights(eval_policy, behav_policy, behavior_policies):\n",
        "  all_weights = []\n",
        "  for trajectory in behavior_policies:\n",
        "    cum_ratio = 1\n",
        "    cumul_weights = []\n",
        "    for step in trajectory:\n",
        "        ratio = eval_policy[step[1]]/behav_policy[step[1]]\n",
        "        # print(\"Ratio:\",ratio)\n",
        "        cum_ratio *= ratio\n",
        "        cumul_weights.append(cum_ratio)\n",
        "        # print(\"Cumul:\",cum_ratio)\n",
        "    all_weights.append(cumul_weights)\n",
        "\n",
        "  return all_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IS"
      ],
      "metadata": {
        "id": "VgadQNBwlT2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "0yCj9q-qGxvf"
      },
      "outputs": [],
      "source": [
        "def per_step_IS(behavior_policies, phi_trajectories = 0.3):\n",
        "  all_timesteps = []\n",
        "  gamma = 0.9\n",
        "  policy_for_scope,_ = subset_policies(behavior_policies, phi_trajectories)\n",
        "  scope_weights = calculate_importance_weights(eval_policy, behav_policy, policy_for_scope)\n",
        "  for j in range(len(scope_weights)):\n",
        "    Timestep_values = []\n",
        "    for i in range(len(scope_weights[j])-1):\n",
        "      timestep = gamma**(i)*scope_weights[j][i]*policy_for_scope[j][i][2]\n",
        "      # print(\"Timestep: \",timestep)\n",
        "      Timestep_values.append(timestep)\n",
        "\n",
        "    all_timesteps.append(Timestep_values)\n",
        "\n",
        "  V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "  seed_value = 42\n",
        "  np.random.seed(seed_value)\n",
        "\n",
        "  num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "  bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "                        for _ in range(100)]\n",
        "\n",
        "  V_per_sample = [sum(sample)/len(bootstrap_samples) for sample in bootstrap_samples]\n",
        "  V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "  std_deviation = np.std(V_per_sample)\n",
        "  quartiles = np.percentile(V_per_sample, [25, 50, 75])\n",
        "  max_value = np.max(V_per_sample)\n",
        "  min_value = np.min(V_per_sample)\n",
        "\n",
        "  return V_per_sample, std_deviation, quartiles, max_value, min_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IS_per_traj, is_std, is_quartiles, is_max_value, is_min_value = per_step_IS(behavior_policies,0.3)\n",
        "# print(is_std)\n",
        "# print(is_quartiles)\n",
        "# print(is_max_value)\n",
        "# print(is_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQM-eboLqbcY",
        "outputId": "f6529e25-fa4a-4218-ef0a-03f586db00f8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10933610817754\n",
            "[-0.84036769 -0.77909955 -0.69050267]\n",
            "-0.5479659789843923\n",
            "-1.0898425977186181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SCOPE"
      ],
      "metadata": {
        "id": "DzDCmT7zm4IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def SCOPE(behavior_policies, beta, phi_trajectories = 0.3):\n",
        "    all_timesteps = []\n",
        "    gamma = 0.9\n",
        "    policy_for_scope,_ = subset_policies(behavior_policies, phi_trajectories)\n",
        "    scope_weights = calculate_importance_weights(eval_policy, behav_policy, policy_for_scope)\n",
        "    for j in range(len(scope_weights)):\n",
        "        Timestep_values = []\n",
        "        for i in range(len(scope_weights[j]) - 1):\n",
        "            features = policy_for_scope[j][i][5] + policy_for_scope[j][i][6]\n",
        "            features_next = policy_for_scope[j][i + 1][5] + policy_for_scope[j][i + 1][6]\n",
        "            timestep = gamma ** (i) * scope_weights[j][i] * (policy_for_scope[j][i][2] + gamma * phi(features_next, beta) - phi(features, beta))\n",
        "            Timestep_values.append(timestep)\n",
        "\n",
        "        all_timesteps.append(Timestep_values)\n",
        "\n",
        "    V_per_traj = [sum(sublist) for sublist in all_timesteps]\n",
        "\n",
        "\n",
        "\n",
        "    seed_value = 42\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    num_trajectories_to_sample = max(1, len(V_per_traj))\n",
        "\n",
        "    bootstrap_samples = [np.random.choice(V_per_traj, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(100)]\n",
        "\n",
        "    V_per_sample = [sum(sample)/len(bootstrap_samples) for sample in bootstrap_samples]\n",
        "    V_per_sample = np.array(V_per_sample)\n",
        "\n",
        "    std_deviation = np.std(V_per_sample)\n",
        "    quartiles = np.percentile(V_per_sample, [25, 50, 75])\n",
        "    max_value = np.max(V_per_sample)\n",
        "    min_value = np.min(V_per_sample)\n",
        "\n",
        "    return V_per_sample, std_deviation, quartiles, max_value, min_value\n",
        "    # return bootstrap_samples\n"
      ],
      "metadata": {
        "id": "BgqnBM4abWL9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = SCOPE(behavior_policies, beta, 0.3)"
      ],
      "metadata": {
        "id": "SaTaVi4Q4eGV"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(samples[99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TaEsJ5_4lcX",
        "outputId": "e14f79e7-d9f2-4dfc-99dc-fc89949fa2b1"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-147.58609119391252"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # all_weights = calculate_importance_weights(eval_policy, behav_policy, behavior_policies)\n",
        "# beta =  [ 0.2609209,   0.47456879, -0.52815694]\n",
        "# # beta = [0,0,0]\n",
        "# V_per_sample, scope_std, scope_quartiles, scope_max_value, scope_min_value = SCOPE(behavior_policies,beta,0.3)\n",
        "# print(scope_std)\n",
        "# print(scope_quartiles)\n",
        "# print(scope_max_value)\n",
        "# print(scope_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEk4vgdcbarZ",
        "outputId": "c9afbb06-493e-4950-dd02-96ec8f3c817b"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.15306293298484386\n",
            "[-1.2602556  -1.17031182 -1.07600677]\n",
            "-0.8297692802152171\n",
            "-1.662222105105361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance Preparation and Calculation"
      ],
      "metadata": {
        "id": "WL9BcVXloGpd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CfXiEdgqDE4D"
      },
      "outputs": [],
      "source": [
        "def phi(features, beta):\n",
        "  features = np.array(features)\n",
        "  beta = np.array(beta)\n",
        "  phi_s = np.dot(beta,features)\n",
        "  return phi_s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9NdwpKSkhxZR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# gamma = 0.9\n",
        "# beta = [random.random() for _ in range(3)]\n",
        "def variance_terms(policy_set,gamma, beta):\n",
        "  all_weights = calculate_importance_weights(eval_policy, behav_policy, policy_set)\n",
        "  y_w_r_all = 0\n",
        "  r_all = 0\n",
        "  f_a = 0\n",
        "  for j in range(len(policy_set)):\n",
        "    y_w_r = 0\n",
        "    r = 0\n",
        "    for i in range(len(policy_set[j])):\n",
        "      features = policy_set[j][i][5]+policy_set[j][i][6]\n",
        "      y_w_r += gamma**(i)*all_weights[j][i]*policy_set[j][i][2]\n",
        "      if i>0 & i<len(policy_set):\n",
        "        r += phi(features, beta)*(all_weights[j][i-1]-all_weights[j][i])\n",
        "    y_w_r_all += y_w_r\n",
        "    f_a +=  gamma**(len(policy_set[j]))*all_weights[j][-1]*phi(features,beta) - phi(features, beta) # fix the features part\n",
        "    r_all += r\n",
        "\n",
        "  IS = y_w_r_all/len(policy_set)\n",
        "  R = r_all/len(policy_set)\n",
        "  F = f_a/len(policy_set)\n",
        "  return IS, R, F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6BpxUSqy7K86"
      },
      "outputs": [],
      "source": [
        "def subset_policies(policy, percent_to_estimate_phi):\n",
        "    seed_value = 42\n",
        "    np.random.seed(seed_value)\n",
        "    num_policies = len(policy)\n",
        "    num_policies_to_estimate_phi = int(num_policies * percent_to_estimate_phi)\n",
        "\n",
        "    policy_for_scope = policy[num_policies_to_estimate_phi:]\n",
        "    policy_for_phi = policy[:num_policies_to_estimate_phi]\n",
        "\n",
        "    return policy_for_scope, policy_for_phi\n",
        "\n",
        "def calc_variance(behavior_policies, gamma, beta, num_bootstrap_samples = 100,phi_trajectories = 0.3):\n",
        "  # Set the seed value (you can use any integer value)\n",
        "  seed_value = 42\n",
        "  np.random.seed(seed_value)\n",
        "  num_trajectories_to_sample = max(1, int(len(behavior_policies) * phi_trajectories))\n",
        "\n",
        "  policy_for_scope, policy_for_phi = subset_policies(behavior_policies, percent_to_estimate_phi=phi_trajectories)\n",
        "  num_trajectories_to_sample = max(1, len(policy_for_phi))\n",
        "\n",
        "  bootstrap_samples = [np.random.choice(policy_for_phi, size=num_trajectories_to_sample, replace=True)\n",
        "                         for _ in range(num_bootstrap_samples)]\n",
        "  IS_all = []\n",
        "  R_all = []\n",
        "  F_all = []\n",
        "\n",
        "  for pol in bootstrap_samples:\n",
        "    IS, R, F = variance_terms(pol,0.9,beta)\n",
        "    IS_all.append(IS)\n",
        "    R_all.append(R)\n",
        "    F_all.append(F)\n",
        "  IS_sq = np.mean([num**2 for num in IS_all])\n",
        "  IS_R_F = 2*np.mean([IS_all[i]*(R_all[i]+F_all[i]) for i in range(len(IS_all))])\n",
        "  R_sq = np.mean([num**2 for num in R_all])\n",
        "  IS_sq_all = (np.mean(IS_all))**2\n",
        "  IS_r_t_f = 2*np.mean(IS_all)*np.mean([R_all[i]+F_all[i] for i in range(len(R_all))])\n",
        "  R_sq_all = (np.mean(R_all))**2\n",
        "\n",
        "  variance_scope = IS_sq + IS_R_F + R_sq - IS_sq_all - IS_r_t_f - R_sq_all\n",
        "  variance_is = IS_sq - IS_sq_all\n",
        "  return variance_scope, variance_is"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of an initial guess of phi can be seen below, as you can see the SCOPE variance is not ideal."
      ],
      "metadata": {
        "id": "36vQxkjcnrEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scope_set, phi_set = subset_policies(behavior_policies, 0.3)\n",
        "variance_scope, variance_is = calc_variance(phi_set,0.9,[-0.1,.1,.1], 100, 0.3)\n",
        "print(\"Var SCOPE: \",variance_scope)\n",
        "print(\"Var IS: \",variance_is)\n",
        "print(\"Percent change in variance: \",((variance_scope-variance_is)/variance_is)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74yBmg6_edJv",
        "outputId": "8a8add94-d472-48e1-925a-805773d9fd04"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-f5a21e0b065d>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(policy_for_phi, size=num_trajectories_to_sample, replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var SCOPE:  0.7292656998003821\n",
            "Var IS:  0.3796743263659492\n",
            "Percent change in variance:  92.07664283770379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UefNOC1GC8Wm"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we aim to optimize beta to minimize SCOPE variance."
      ],
      "metadata": {
        "id": "RWx9IbJon2IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function to minimize variance_scope\n",
        "def objective_function(beta, behavior_policies, phi_trajectories):\n",
        "    scope_set, phi_set = subset_policies(behavior_policies, phi_trajectories)\n",
        "    variance_scope, variance_is = calc_variance(phi_set, 0.9, beta, 100, 0.3)\n",
        "    return variance_scope\n",
        "\n",
        "# Set the initial values of beta\n",
        "initial_beta = np.array([ 0.2610704,   0.30396575, -0.43850237])\n",
        "\n",
        "\n",
        "def optimize_variance_scope(initial_beta, behavior_policies, phi_trajectories):\n",
        "    # Lists to store beta and variance_scope values at each iteration\n",
        "    all_betas = []\n",
        "    all_variance_scopes = []\n",
        "\n",
        "    # Callback function to record beta and variance_scope values at each iteration\n",
        "    def callback_function(beta):\n",
        "        all_betas.append(beta.copy())\n",
        "        variance_scope = objective_function(beta, behavior_policies, phi_trajectories)\n",
        "        all_variance_scopes.append(variance_scope)\n",
        "        print(\"Iteration:\", len(all_betas))\n",
        "        print(\"Beta:\", beta)\n",
        "        print(\"Variance Scope:\", variance_scope)\n",
        "        print(\"----------\")\n",
        "\n",
        "    # Run the optimization with the callback\n",
        "    result = minimize(\n",
        "        objective_function,\n",
        "        initial_beta,\n",
        "        args=(behavior_policies, phi_trajectories),\n",
        "        method='L-BFGS-B',\n",
        "        callback=callback_function\n",
        "    )\n",
        "\n",
        "    # Extract the optimal beta values\n",
        "    optimal_beta = result.x\n",
        "\n",
        "    return optimal_beta, all_betas, all_variance_scopes\n"
      ],
      "metadata": {
        "id": "Ws00tRw1KZnN"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_beta, _, _ = optimize_variance_scope(initial_beta, behavior_policies, 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZFyTciRkhpd",
        "outputId": "12b187b7-c347-42fd-ccd9-ef42716feb79"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-f5a21e0b065d>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  bootstrap_samples = [np.random.choice(policy_for_phi, size=num_trajectories_to_sample, replace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Beta: [ 0.16821737  0.35271354 -0.42156545]\n",
            "Variance Scope: 0.11317799047435423\n",
            "----------\n",
            "Iteration: 2\n",
            "Beta: [ 0.15853249  0.34137881 -0.42710298]\n",
            "Variance Scope: 0.11040088606596254\n",
            "----------\n",
            "Iteration: 3\n",
            "Beta: [ 0.15073224  0.32968859 -0.42471605]\n",
            "Variance Scope: 0.1093933569252897\n",
            "----------\n",
            "Iteration: 4\n",
            "Beta: [ 0.14615259  0.3202478  -0.4164246 ]\n",
            "Variance Scope: 0.10883472336459371\n",
            "----------\n",
            "Iteration: 5\n",
            "Beta: [ 0.14331703  0.31081431 -0.40169853]\n",
            "Variance Scope: 0.108436847289928\n",
            "----------\n",
            "Iteration: 6\n",
            "Beta: [ 0.14352606  0.31073858 -0.40072254]\n",
            "Variance Scope: 0.10843321214844745\n",
            "----------\n",
            "Iteration: 7\n",
            "Beta: [ 0.14356653  0.31079481 -0.40072313]\n",
            "Variance Scope: 0.10843319408861363\n",
            "----------\n",
            "Iteration: 8\n",
            "Beta: [ 0.14356994  0.31080162 -0.40072872]\n",
            "Variance Scope: 0.10843319398401882\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value estimates of IS and SCOPE estimators"
      ],
      "metadata": {
        "id": "LUVgi1GMloz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all_weights = calculate_importance_weights(eval_policy, behav_policy, behavior_policies)\n",
        "beta =  [ 0.2609209,   0.47456879, -0.52815694]\n",
        "# beta = [0,0,0]\n",
        "V_per_sample, scope_std, scope_quartiles, scope_max_value, scope_min_value = SCOPE(behavior_policies,optimal_beta,0.3)\n",
        "print(\"SCOPE Std Dev: \", scope_std)\n",
        "print(\"SCOPE quartiles: \",scope_quartiles)\n",
        "print(\"SCOPE max: \",scope_max_value)\n",
        "print(\"SCOPE min\",scope_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHQ-x-S-hLnV",
        "outputId": "332add1b-f692-49f4-f837-d456cb61a0a9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCOPE Std Dev:  0.10569617855732055\n",
            "SCOPE quartiles:  [-0.59477096 -0.52791441 -0.44771446]\n",
            "SCOPE max:  -0.2758543575915438\n",
            "SCOPE min -0.8459066085675662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_per_traj, is_std, is_quartiles, is_max_value, is_min_value = per_step_IS(behavior_policies,0.3)\n",
        "print(\"IS std dev: \",is_std)\n",
        "print(\"IS quartiles: \",is_quartiles)\n",
        "print(\"IS max: \",is_max_value)\n",
        "print(\"IS min: \", is_min_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBVmYwOIhuTo",
        "outputId": "89ef9958-1310-4e73-d831-aaf399f2cbd4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IS std dev:  0.10933610817754\n",
            "IS quartiles:  [-0.84036769 -0.77909955 -0.69050267]\n",
            "IS max:  -0.5479659789843923\n",
            "IS min:  -1.0898425977186181\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "MkhpULigM6wI"
      ],
      "authorship_tag": "ABX9TyOzCMb0FN5SlXHBcEEKhPR6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}